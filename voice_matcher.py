#!/usr/bin/env python3
"""
äººå£°åŒ¹é…å™¨ - åŸºäºå‚è€ƒéŸ³é¢‘é€‰æ‹©æœ€åŒ¹é…çš„åˆ†ç¦»äººå£°
Voice Matcher - Select the best matching separated voice based on reference audio
"""

import numpy as np
import librosa
import soundfile as sf
from pathlib import Path
import matplotlib.pyplot as plt
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr
from sklearn.metrics.pairwise import cosine_similarity
import subprocess
import os

class VoiceMatcher:
    """äººå£°åŒ¹é…å™¨"""
    
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.hop_length = 256
        self.n_fft = 1024
        self.n_mfcc = 13
        self.n_mels = 80
        
    def load_audio(self, audio_path):
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            print(f"åŠ è½½éŸ³é¢‘: {audio_path}")
            print(f"  æ—¶é•¿: {len(audio)/self.sample_rate:.2f}ç§’")
            return audio
        except Exception as e:
            print(f"éŸ³é¢‘åŠ è½½å¤±è´¥ {audio_path}: {e}")
            return None
    
    def convert_audio_format(self, input_path, output_path):
        """ä½¿ç”¨transwav.pyè½¬æ¢éŸ³é¢‘æ ¼å¼ï¼ˆ64ä½è½¬16ä½ï¼‰"""
        try:
            # è¯»å–éŸ³é¢‘æ•°æ®
            data, samplerate = sf.read(input_path)
            
            # æŸ¥çœ‹æ³¢å½¢çš„æœ€å¤§ç»å¯¹å€¼
            max_val = np.max(np.abs(data))
            print(f"  æœ€å¤§ç»å¯¹å€¼: {max_val}")
            
            # å¦‚æœè¶…å‡ºæ­£å¸¸èŒƒå›´ï¼Œå…ˆå½’ä¸€åŒ–
            if max_val > 1:
                data = data / max_val
            
            # ç¨³å¥çš„å½’ä¸€åŒ–
            data = np.clip(data, -1.0, 1.0) * 0.98
            
            # è½¬æ¢ä¸º int16 ç±»å‹
            data_int16 = (data * 32767).astype(np.int16)
            
            # ä¿å­˜éŸ³é¢‘
            sf.write(output_path, data_int16, samplerate, subtype='PCM_16')
            print(f"  âœ… æ ¼å¼è½¬æ¢å®Œæˆ: {output_path}")
            return True
            
        except Exception as e:
            print(f"  âŒ æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            return False
    
    def extract_comprehensive_features(self, audio):
        """æå–ç»¼åˆç‰¹å¾"""
        try:
            features = {}
            
            # 1. MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(
                y=audio, 
                sr=self.sample_rate,
                n_mfcc=self.n_mfcc,
                hop_length=self.hop_length,
                n_fft=self.n_fft
            )
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            features['mfcc_delta'] = np.mean(librosa.feature.delta(mfcc), axis=1)
            
            # 2. Melé¢‘è°±ç‰¹å¾
            mel_spec = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                hop_length=self.hop_length,
                n_fft=self.n_fft
            )
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            features['mel_mean'] = np.mean(mel_spec_db, axis=1)
            features['mel_std'] = np.std(mel_spec_db, axis=1)
            
            # 3. è‰²åº¦ç‰¹å¾
            chroma = librosa.feature.chroma_stft(
                y=audio,
                sr=self.sample_rate,
                hop_length=self.hop_length,
                n_fft=self.n_fft
            )
            features['chroma_mean'] = np.mean(chroma, axis=1)
            features['chroma_std'] = np.std(chroma, axis=1)
            
            # 4. é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]
            spectral_bandwidth = librosa.feature.spectral_bandwidth(
                y=audio, sr=self.sample_rate, hop_length=self.hop_length
            )[0]
            
            features['spectral_centroid_mean'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)
            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)
            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
            
            # 5. é›¶äº¤å‰ç‡
            zcr = librosa.feature.zero_crossing_rate(
                y=audio, hop_length=self.hop_length
            )[0]
            features['zcr_mean'] = np.mean(zcr)
            features['zcr_std'] = np.std(zcr)
            
            # 6. åŸºé¢‘ç‰¹å¾
            pitches, magnitudes = librosa.piptrack(
                y=audio,
                sr=self.sample_rate,
                hop_length=self.hop_length,
                fmin=80,
                fmax=400
            )
            pitch_values = pitches[pitches > 0]
            if len(pitch_values) > 0:
                features['pitch_mean'] = np.mean(pitch_values)
                features['pitch_std'] = np.std(pitch_values)
                features['pitch_median'] = np.median(pitch_values)
            else:
                features['pitch_mean'] = 0
                features['pitch_std'] = 0
                features['pitch_median'] = 0
            
            # 7. RMSèƒ½é‡
            rms = librosa.feature.rms(y=audio, hop_length=self.hop_length)[0]
            features['rms_mean'] = np.mean(rms)
            features['rms_std'] = np.std(rms)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾ä¸ºå‘é‡
            feature_vector = np.concatenate([
                features['mfcc_mean'],
                features['mfcc_std'], 
                features['mfcc_delta'],
                features['mel_mean'][:20],  # åªå–å‰20ä¸ªmelé¢‘å¸¦
                features['mel_std'][:20],
                features['chroma_mean'],
                features['chroma_std'],
                [features['spectral_centroid_mean']],
                [features['spectral_centroid_std']],
                [features['spectral_rolloff_mean']],
                [features['spectral_bandwidth_mean']],
                [features['zcr_mean']],
                [features['zcr_std']],
                [features['pitch_mean']],
                [features['pitch_std']],
                [features['pitch_median']],
                [features['rms_mean']],
                [features['rms_std']]
            ])
            
            return feature_vector, features
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None, None
    
    def calculate_similarity_scores(self, ref_features, target_features):
        """è®¡ç®—å¤šç»´ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            scores = {}
            
            # ç¡®ä¿ç‰¹å¾å‘é‡é•¿åº¦ä¸€è‡´
            min_len = min(len(ref_features), len(target_features))
            ref_vec = ref_features[:min_len]
            target_vec = target_features[:min_len]
            
            # 1. ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = 1 - cosine(ref_vec, target_vec)
            scores['cosine'] = max(0, cos_sim)
            
            # 2. çš®å°”é€Šç›¸å…³ç³»æ•°
            try:
                pearson_corr, _ = pearsonr(ref_vec, target_vec)
                if np.isnan(pearson_corr):
                    pearson_corr = 0
                scores['pearson'] = max(0, pearson_corr)
            except:
                scores['pearson'] = 0
            
            # 3. æ¬§æ°è·ç¦»ç›¸ä¼¼åº¦
            euclidean_dist = np.linalg.norm(ref_vec - target_vec)
            max_possible_dist = np.linalg.norm(ref_vec) + np.linalg.norm(target_vec)
            if max_possible_dist > 0:
                euclidean_sim = 1 - (euclidean_dist / max_possible_dist)
            else:
                euclidean_sim = 0
            scores['euclidean'] = max(0, euclidean_sim)
            
            # 4. æ›¼å“ˆé¡¿è·ç¦»ç›¸ä¼¼åº¦
            manhattan_dist = np.sum(np.abs(ref_vec - target_vec))
            max_manhattan = np.sum(np.abs(ref_vec)) + np.sum(np.abs(target_vec))
            if max_manhattan > 0:
                manhattan_sim = 1 - (manhattan_dist / max_manhattan)
            else:
                manhattan_sim = 0
            scores['manhattan'] = max(0, manhattan_sim)
            
            # 5. ç»¼åˆåŠ æƒåˆ†æ•°
            weights = {
                'cosine': 0.4,
                'pearson': 0.3,
                'euclidean': 0.2,
                'manhattan': 0.1
            }
            
            composite_score = sum(scores[metric] * weight for metric, weight in weights.items())
            scores['composite'] = composite_score
            
            return scores
            
        except Exception as e:
            print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return {'cosine': 0, 'pearson': 0, 'euclidean': 0, 'manhattan': 0, 'composite': 0}
    
    def match_voices(self, reference_audio_path, separated_voices_dir, output_dir="output"):
        """åŒ¹é…äººå£°"""
        try:
            print("ğŸ¯ å¼€å§‹äººå£°åŒ¹é…...")
            print("=" * 60)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            Path(output_dir).mkdir(exist_ok=True)
            
            # 1. åŠ è½½å‚è€ƒéŸ³é¢‘
            print("\nğŸ“ åŠ è½½å‚è€ƒéŸ³é¢‘...")
            reference_audio = self.load_audio(reference_audio_path)
            if reference_audio is None:
                return None
            
            # 2. æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("ğŸ§  æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            ref_features, ref_details = self.extract_comprehensive_features(reference_audio)
            if ref_features is None:
                print("âŒ å‚è€ƒéŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return None
            
            print(f"å‚è€ƒç‰¹å¾ç»´åº¦: {ref_features.shape}")
            
            # 3. æŸ¥æ‰¾åˆ†ç¦»çš„äººå£°æ–‡ä»¶
            separated_dir = Path(separated_voices_dir)
            voice_files = []
            
            # æŸ¥æ‰¾å„ç§å¯èƒ½çš„æ–‡ä»¶æ ¼å¼
            for pattern in ["*.wav", "*.mp3", "*.flac"]:
                voice_files.extend(separated_dir.glob(pattern))
            
            # ä¹ŸæŸ¥æ‰¾SPEAKER_å¼€å¤´çš„æ–‡ä»¶
            for pattern in ["SPEAKER_*.wav", "speaker_*.wav"]:
                voice_files.extend(separated_dir.glob(pattern))
            
            if not voice_files:
                print(f"âŒ åœ¨ {separated_voices_dir} ä¸­æœªæ‰¾åˆ°äººå£°æ–‡ä»¶")
                return None
            
            print(f"\nğŸ” æ‰¾åˆ° {len(voice_files)} ä¸ªäººå£°æ–‡ä»¶:")
            for vf in voice_files:
                print(f"  - {vf.name}")
            
            # 4. è½¬æ¢éŸ³é¢‘æ ¼å¼å¹¶æ¯”è¾ƒ
            print(f"\nğŸ”„ è½¬æ¢éŸ³é¢‘æ ¼å¼å¹¶æå–ç‰¹å¾...")
            results = []
            
            for i, voice_file in enumerate(voice_files):
                print(f"\nå¤„ç†æ–‡ä»¶ {i+1}/{len(voice_files)}: {voice_file.name}")
                
                # è½¬æ¢æ ¼å¼
                converted_file = Path(output_dir) / f"converted_{voice_file.stem}.wav"
                if self.convert_audio_format(str(voice_file), str(converted_file)):
                    
                    # åŠ è½½è½¬æ¢åçš„éŸ³é¢‘
                    voice_audio = self.load_audio(str(converted_file))
                    if voice_audio is not None:
                        
                        # æå–ç‰¹å¾
                        voice_features, voice_details = self.extract_comprehensive_features(voice_audio)
                        if voice_features is not None:
                            
                            # è®¡ç®—ç›¸ä¼¼åº¦
                            similarity_scores = self.calculate_similarity_scores(ref_features, voice_features)
                            
                            result = {
                                'file_path': str(voice_file),
                                'converted_path': str(converted_file),
                                'similarity_scores': similarity_scores,
                                'features': voice_features,
                                'feature_details': voice_details
                            }
                            results.append(result)
                            
                            print(f"  ç›¸ä¼¼åº¦åˆ†æ•°:")
                            print(f"    ä½™å¼¦ç›¸ä¼¼åº¦: {similarity_scores['cosine']:.3f}")
                            print(f"    çš®å°”é€Šç›¸å…³: {similarity_scores['pearson']:.3f}")
                            print(f"    æ¬§æ°è·ç¦»: {similarity_scores['euclidean']:.3f}")
                            print(f"    ç»¼åˆåˆ†æ•°: {similarity_scores['composite']:.3f}")
            
            if not results:
                print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„äººå£°æ–‡ä»¶")
                return None
            
            # 5. é€‰æ‹©æœ€ä½³åŒ¹é…
            print(f"\nğŸ† é€‰æ‹©æœ€ä½³åŒ¹é…...")
            best_match = max(results, key=lambda x: x['similarity_scores']['composite'])
            
            print(f"æœ€ä½³åŒ¹é…: {Path(best_match['file_path']).name}")
            print(f"ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•°: {best_match['similarity_scores']['composite']:.3f}")
            
            # 6. ä¿å­˜æœ€ä½³åŒ¹é…ç»“æœ
            best_output_path = Path(output_dir) / "best_matched_voice.wav"
            
            # å¤åˆ¶æœ€ä½³åŒ¹é…çš„è½¬æ¢åæ–‡ä»¶
            import shutil
            shutil.copy2(best_match['converted_path'], best_output_path)
            
            print(f"âœ… æœ€ä½³åŒ¹é…ç»“æœä¿å­˜åˆ°: {best_output_path}")
            
            # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            self.generate_matching_report(reference_audio_path, results, best_match, output_dir)
            
            return {
                'best_match': best_match,
                'all_results': results,
                'output_path': str(best_output_path)
            }
            
        except Exception as e:
            print(f"âŒ äººå£°åŒ¹é…å¤±è´¥: {e}")
            return None
    
    def generate_matching_report(self, reference_path, results, best_match, output_dir):
        """ç”ŸæˆåŒ¹é…åˆ†ææŠ¥å‘Š"""
        try:
            print("\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            
            # 1. æ–‡æœ¬æŠ¥å‘Š
            report_path = Path(output_dir) / "voice_matching_report.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("äººå£°åŒ¹é…åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                
                f.write(f"å‚è€ƒéŸ³é¢‘: {reference_path}\n")
                f.write(f"å¤„ç†æ–‡ä»¶æ•°: {len(results)}\n\n")
                
                f.write("æ‰€æœ‰å€™é€‰äººå£°ç›¸ä¼¼åº¦åˆ†æ•°:\n")
                f.write("-" * 40 + "\n")
                
                # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
                sorted_results = sorted(results, key=lambda x: x['similarity_scores']['composite'], reverse=True)
                
                for i, result in enumerate(sorted_results):
                    file_name = Path(result['file_path']).name
                    scores = result['similarity_scores']
                    f.write(f"\n{i+1}. {file_name}\n")
                    f.write(f"   ç»¼åˆåˆ†æ•°: {scores['composite']:.3f}\n")
                    f.write(f"   ä½™å¼¦ç›¸ä¼¼åº¦: {scores['cosine']:.3f}\n")
                    f.write(f"   çš®å°”é€Šç›¸å…³: {scores['pearson']:.3f}\n")
                    f.write(f"   æ¬§æ°è·ç¦»: {scores['euclidean']:.3f}\n")
                    f.write(f"   æ›¼å“ˆé¡¿è·ç¦»: {scores['manhattan']:.3f}\n")
                    
                    if result == best_match:
                        f.write("   *** æœ€ä½³åŒ¹é… ***\n")
                
                f.write(f"\næœ€ç»ˆé€‰æ‹©: {Path(best_match['file_path']).name}\n")
                f.write(f"æœ€é«˜ç»¼åˆåˆ†æ•°: {best_match['similarity_scores']['composite']:.3f}\n")
            
            print(f"  ğŸ“„ æ–‡æœ¬æŠ¥å‘Š: {report_path}")
            
            # 2. å¯è§†åŒ–æŠ¥å‘Š
            plt.figure(figsize=(12, 8))
            
            # ç›¸ä¼¼åº¦å¯¹æ¯”å›¾
            file_names = [Path(result['file_path']).stem for result in results]
            composite_scores = [result['similarity_scores']['composite'] for result in results]
            cosine_scores = [result['similarity_scores']['cosine'] for result in results]
            pearson_scores = [result['similarity_scores']['pearson'] for result in results]
            
            x = np.arange(len(file_names))
            width = 0.25
            
            plt.bar(x - width, composite_scores, width, label='ç»¼åˆåˆ†æ•°', alpha=0.8)
            plt.bar(x, cosine_scores, width, label='ä½™å¼¦ç›¸ä¼¼åº¦', alpha=0.8)
            plt.bar(x + width, pearson_scores, width, label='çš®å°”é€Šç›¸å…³', alpha=0.8)
            
            plt.xlabel('å€™é€‰äººå£°æ–‡ä»¶')
            plt.ylabel('ç›¸ä¼¼åº¦åˆ†æ•°')
            plt.title('äººå£°åŒ¹é…ç›¸ä¼¼åº¦å¯¹æ¯”')
            plt.xticks(x, file_names, rotation=45, ha='right')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            viz_path = Path(output_dir) / "voice_matching_analysis.png"
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"  ğŸ“ˆ å¯è§†åŒ–æŠ¥å‘Š: {viz_path}")
            
        except Exception as e:
            print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    reference_audio_path = "reference/lttgd_ref.wav"  # å‚è€ƒéŸ³é¢‘
    separated_voices_dir = "sepvoice"  # åˆ†ç¦»äººå£°æ–‡ä»¶ç›®å½•ï¼ˆå½“å‰ç›®å½•ï¼ŒåŒ…å«SPEAKER_XX.wavï¼‰
    output_dir = "output"
    
    print("ğŸ¤ äººå£°åŒ¹é…å™¨ - åŸºäºå‚è€ƒéŸ³é¢‘é€‰æ‹©æœ€ä½³åŒ¹é…äººå£°")
    print("=" * 60)
    
    # åˆ›å»ºåŒ¹é…å™¨
    matcher = VoiceMatcher(sample_rate=16000)
    
    # æ‰§è¡ŒåŒ¹é…
    result = matcher.match_voices(
        reference_audio_path=reference_audio_path,
        separated_voices_dir=separated_voices_dir,
        output_dir=output_dir
    )
    
    if result:
        print(f"\nğŸ‰ äººå£°åŒ¹é…å®Œæˆ!")
        print(f"æœ€ä½³åŒ¹é…æ–‡ä»¶: {result['output_path']}")
        print(f"ç»¼åˆç›¸ä¼¼åº¦: {result['best_match']['similarity_scores']['composite']:.3f}")
    else:
        print("\nâŒ äººå£°åŒ¹é…å¤±è´¥")

if __name__ == "__main__":
    main()
