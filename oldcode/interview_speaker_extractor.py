#!/usr/bin/env python3
"""
åŸºäºè¯­éŸ³æ®µåˆ†ç¦»çš„è¯´è¯äººæå–å™¨
Interview Speaker Extractor

ä¸“é—¨ç”¨äºé‡‡è®¿ç±»éŸ³é¢‘ï¼Œåˆ©ç”¨è‡ªç„¶é™éŸ³é—´éš™åˆ†æ®µï¼Œç„¶åè¿›è¡Œè¯´è¯äººè¯†åˆ«
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine
from pathlib import Path

class InterviewSpeakerExtractor:
    """
    é‡‡è®¿è¯´è¯äººæå–å™¨
    
    æ ¸å¿ƒæ€è·¯ï¼š
    1. åˆ©ç”¨é™éŸ³é—´éš™æ£€æµ‹è¯­éŸ³æ®µ
    2. å¯¹æ¯ä¸ªè¯­éŸ³æ®µæå–è¯´è¯äººç‰¹å¾
    3. ä¸å‚è€ƒéŸ³é¢‘æ¯”è¾ƒï¼Œå†³å®šä¿ç•™æˆ–é™éŸ³
    4. é‡æ„å®Œæ•´éŸ³é¢‘
    """
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13
        
        # è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°
        self.vad_threshold = 0.01    # VADèƒ½é‡é˜ˆå€¼
        self.min_speech_duration = 0.5  # æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼ˆç§’ï¼‰
        self.min_silence_duration = 0.3  # æœ€å°é™éŸ³é—´éš”ï¼ˆç§’ï¼‰
        
        # è¯´è¯äººè¯†åˆ«å‚æ•°
        self.similarity_threshold = 0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def detect_speech_segments(self, audio):
        """
        æ£€æµ‹è¯­éŸ³æ®µï¼ŒåŸºäºèƒ½é‡å’Œé™éŸ³é—´éš”
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            segments: [(start_time, end_time), ...] è¯­éŸ³æ®µåˆ—è¡¨
        """
        print("ğŸ” æ£€æµ‹è¯­éŸ³æ®µ...")
        
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_length = int(0.025 * self.sample_rate)  # 25ms
        hop_length = int(0.01 * self.sample_rate)     # 10ms
        
        frames = []
        for i in range(0, len(audio) - frame_length + 1, hop_length):
            frame = audio[i:i + frame_length]
            energy = np.sum(frame ** 2)
            frames.append(energy)
        
        frames = np.array(frames)
        
        # è‡ªé€‚åº”é˜ˆå€¼
        mean_energy = np.mean(frames)
        std_energy = np.std(frames)
        threshold = max(self.vad_threshold, mean_energy - 0.5 * std_energy)
        
        print(f"   èƒ½é‡é˜ˆå€¼: {threshold:.6f}")
        print(f"   å¹³å‡èƒ½é‡: {mean_energy:.6f}")
        
        # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
        speech_frames = frames > threshold
        
        # è½¬æ¢ä¸ºæ—¶é—´
        frame_times = np.arange(len(speech_frames)) * hop_length / self.sample_rate
        
        # æŸ¥æ‰¾è¯­éŸ³æ®µ
        segments = []
        in_speech = False
        start_time = 0
        
        for i, is_speech in enumerate(speech_frames):
            current_time = frame_times[i]
            
            if is_speech and not in_speech:
                # è¯­éŸ³å¼€å§‹
                start_time = current_time
                in_speech = True
            elif not is_speech and in_speech:
                # è¯­éŸ³ç»“æŸ
                end_time = current_time
                duration = end_time - start_time
                
                if duration >= self.min_speech_duration:
                    segments.append((start_time, end_time))
                    print(f"   ğŸ“ è¯­éŸ³æ®µ: {start_time:.2f}s - {end_time:.2f}s (æ—¶é•¿: {duration:.2f}s)")
                
                in_speech = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µ
        if in_speech:
            end_time = len(audio) / self.sample_rate
            duration = end_time - start_time
            if duration >= self.min_speech_duration:
                segments.append((start_time, end_time))
                print(f"   ğŸ“ è¯­éŸ³æ®µ: {start_time:.2f}s - {end_time:.2f}s (æ—¶é•¿: {duration:.2f}s)")
        
        print(f"âœ… æ£€æµ‹åˆ° {len(segments)} ä¸ªè¯­éŸ³æ®µ")
        return segments
    
    def extract_speaker_features(self, audio_segment):
        """
        æå–è¯´è¯äººç‰¹å¾
        
        Args:
            audio_segment: éŸ³é¢‘æ®µ
            
        Returns:
            features: ç‰¹å¾å­—å…¸
        """
        if len(audio_segment) == 0:
            return None
        
        try:
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(
                y=audio_segment, sr=self.sample_rate, n_mfcc=self.n_mfcc,
                n_fft=self.frame_length, hop_length=self.hop_length
            )
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            
            # åŸºé¢‘ç‰¹å¾
            try:
                f0 = librosa.yin(audio_segment, fmin=60, fmax=500, sr=self.sample_rate)
                f0_clean = f0[f0 > 0]
                if len(f0_clean) > 10:  # éœ€è¦è¶³å¤Ÿçš„åŸºé¢‘ç‚¹
                    f0_mean = np.mean(f0_clean)
                    f0_std = np.std(f0_clean)
                    f0_median = np.median(f0_clean)
                else:
                    f0_mean = f0_std = f0_median = 0
            except:
                f0_mean = f0_std = f0_median = 0
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(
                y=audio_segment, sr=self.sample_rate
            ))
            
            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(
                y=audio_segment, sr=self.sample_rate
            ))
            
            # èƒ½é‡ç‰¹å¾
            rms = np.sqrt(np.mean(audio_segment ** 2))
            
            return {
                'mfcc_mean': mfcc_mean,
                'mfcc_std': mfcc_std,
                'f0_mean': f0_mean,
                'f0_std': f0_std,
                'f0_median': f0_median,
                'spectral_centroid': spectral_centroid,
                'spectral_bandwidth': spectral_bandwidth,
                'rms': rms
            }
            
        except Exception as e:
            print(f"   âš ï¸ ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def calculate_speaker_similarity(self, features1, features2):
        """
        è®¡ç®—è¯´è¯äººç›¸ä¼¼åº¦
        
        Args:
            features1, features2: ç‰¹å¾å­—å…¸
            
        Returns:
            similarity: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if features1 is None or features2 is None:
            return 0.0
        
        try:
            similarities = []
            
            # MFCCç›¸ä¼¼åº¦ (æœ€é‡è¦)
            if len(features1['mfcc_mean']) == len(features2['mfcc_mean']):
                mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
                similarities.append(('MFCC', mfcc_sim, 0.5))
            
            # åŸºé¢‘ç›¸ä¼¼åº¦
            if features1['f0_mean'] > 0 and features2['f0_mean'] > 0:
                f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
                f0_sim = 1 / (1 + f0_diff / 30)  # åŸºé¢‘å·®å¼‚å®¹å¿åº¦30Hz
                similarities.append(('åŸºé¢‘', f0_sim, 0.3))
            
            # é¢‘è°±è´¨å¿ƒç›¸ä¼¼åº¦
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 500)
            similarities.append(('é¢‘è°±è´¨å¿ƒ', centroid_sim, 0.2))
            
            # åŠ æƒå¹³å‡
            if similarities:
                weighted_sim = sum(sim * weight for _, sim, weight in similarities)
                total_weight = sum(weight for _, sim, weight in similarities)
                final_similarity = weighted_sim / total_weight if total_weight > 0 else 0
            else:
                final_similarity = 0
            
            return max(0, min(1, final_similarity))
            
        except Exception as e:
            print(f"   âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def extract_target_speaker(self, mixed_audio_path, reference_audio_path, output_path):
        """
        æå–ç›®æ ‡è¯´è¯äºº
        
        Args:
            mixed_audio_path: æ··åˆéŸ³é¢‘è·¯å¾„
            reference_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„  
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            success: æ˜¯å¦æˆåŠŸ
            result_info: ç»“æœä¿¡æ¯
        """
        try:
            print("ğŸ¯ åŸºäºè¯­éŸ³æ®µåˆ†ç¦»çš„è¯´è¯äººæå–")
            print("=" * 60)
            
            # åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            mixed_audio, _ = librosa.load(mixed_audio_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_audio_path, sr=self.sample_rate)
            
            print(f"   æ··åˆéŸ³é¢‘: {len(mixed_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("\nğŸ” åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            ref_features = self.extract_speaker_features(reference_audio)
            if ref_features is None:
                print("âŒ æ— æ³•æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾")
                return False, {"error": "Reference feature extraction failed"}
            
            print(f"   å‚è€ƒåŸºé¢‘: {ref_features['f0_mean']:.1f} Hz")
            print(f"   å‚è€ƒé¢‘è°±è´¨å¿ƒ: {ref_features['spectral_centroid']:.1f} Hz")
            
            # æ£€æµ‹è¯­éŸ³æ®µ
            speech_segments = self.detect_speech_segments(mixed_audio)
            
            if not speech_segments:
                print("âŒ æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µ")
                return False, {"error": "No speech segments detected"}
            
            # åˆå§‹åŒ–è¾“å‡ºéŸ³é¢‘
            output_audio = np.zeros_like(mixed_audio)
            
            # é€æ®µåˆ†æå’Œå†³ç­–
            print(f"\nğŸ”„ é€æ®µåˆ†æ {len(speech_segments)} ä¸ªè¯­éŸ³æ®µ...")
            print("-" * 60)
            
            kept_segments = 0
            total_kept_duration = 0
            
            for i, (start_time, end_time) in enumerate(speech_segments):
                # æå–éŸ³é¢‘æ®µ
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                start_sample = max(0, start_sample)
                end_sample = min(len(mixed_audio), end_sample)
                
                if end_sample <= start_sample:
                    continue
                
                segment_audio = mixed_audio[start_sample:end_sample]
                segment_duration = end_time - start_time
                
                # æå–æ®µç‰¹å¾
                segment_features = self.extract_speaker_features(segment_audio)
                
                if segment_features is None:
                    print(f"   æ®µ {i+1:2d}: {start_time:6.2f}s-{end_time:6.2f}s | âŒ ç‰¹å¾æå–å¤±è´¥ -> ä¸¢å¼ƒ")
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_speaker_similarity(ref_features, segment_features)
                
                # å†³ç­–
                if similarity >= self.similarity_threshold:
                    # ä¿ç•™æ­¤æ®µ
                    output_audio[start_sample:end_sample] = segment_audio
                    kept_segments += 1
                    total_kept_duration += segment_duration
                    decision = "âœ… ä¿ç•™"
                    
                    print(f"   æ®µ {i+1:2d}: {start_time:6.2f}s-{end_time:6.2f}s | "
                          f"ç›¸ä¼¼åº¦: {similarity:.3f} | åŸºé¢‘: {segment_features['f0_mean']:5.1f}Hz | {decision}")
                else:
                    # ä¸¢å¼ƒæ­¤æ®µï¼ˆä¿æŒé™éŸ³ï¼‰
                    decision = "âŒ ä¸¢å¼ƒ"
                    
                    print(f"   æ®µ {i+1:2d}: {start_time:6.2f}s-{end_time:6.2f}s | "
                          f"ç›¸ä¼¼åº¦: {similarity:.3f} | åŸºé¢‘: {segment_features['f0_mean']:5.1f}Hz | {decision}")
            
            print("-" * 60)
            print(f"ğŸ“Š åˆ†æå®Œæˆ:")
            print(f"   æ€»æ®µæ•°: {len(speech_segments)}")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(speech_segments)*100:.1f}%")
            print(f"   ä¿ç•™æ—¶é•¿: {total_kept_duration:.2f}s / {len(mixed_audio)/self.sample_rate:.2f}s")
            
            # åå¤„ç†
            output_audio = self.post_process_audio(output_audio)
            
            # ä¿å­˜ç»“æœ
            if output_path:
                sf.write(output_path, output_audio, self.sample_rate)
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
            
            # ç»“æœä¿¡æ¯
            result_info = {
                'success': True,
                'total_segments': len(speech_segments),
                'kept_segments': kept_segments,
                'keep_ratio': kept_segments / len(speech_segments),
                'original_duration': len(mixed_audio) / self.sample_rate,
                'output_duration': len(output_audio) / self.sample_rate,
                'kept_speech_duration': total_kept_duration,
                'avg_similarity_threshold': self.similarity_threshold
            }
            
            return True, result_info
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}
    
    def post_process_audio(self, audio):
        """éŸ³é¢‘åå¤„ç†"""
        if len(audio) == 0:
            return audio
        
        # å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # è½»å¾®çš„å™ªå£°é—¨é™
        threshold = np.max(np.abs(audio)) * 0.01
        mask = np.abs(audio) < threshold
        audio[mask] = 0
        
        # å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    extractor = InterviewSpeakerExtractor(sample_rate=22050)
    
    # æ–‡ä»¶è·¯å¾„
    mixed_path = "temp/demucs_output/htdemucs/answer/vocals.wav"
    reference_path = "reference/refne2.wav"
    output_path = "output/gender_separated_speaker.wav"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(mixed_path).exists():
        print(f"âŒ æ··åˆéŸ³é¢‘ä¸å­˜åœ¨: {mixed_path}")
        return
    
    if not Path(reference_path).exists():
        print(f"âŒ å‚è€ƒéŸ³é¢‘ä¸å­˜åœ¨: {reference_path}")
        return
    
    # æ‰§è¡Œæå–
    success, result_info = extractor.extract_target_speaker(
        mixed_path, reference_path, output_path
    )
    
    if success:
        print(f"\nğŸ‰ è¯´è¯äººæå–æˆåŠŸ!")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   ä¿ç•™æ®µæ•°: {result_info['kept_segments']}/{result_info['total_segments']}")
        print(f"   ä¿ç•™ç‡: {result_info['keep_ratio']*100:.1f}%")
        print(f"   æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {result_info['kept_speech_duration']:.2f}s")
        
        if result_info['keep_ratio'] > 0.6:
            print("âœ… æå–æ•ˆæœ: ä¼˜ç§€")
        elif result_info['keep_ratio'] > 0.3:
            print("ğŸ‘ æå–æ•ˆæœ: è‰¯å¥½")
        else:
            print("âš ï¸ æå–æ•ˆæœ: ä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é˜ˆå€¼")
    else:
        print(f"\nâŒ è¯´è¯äººæå–å¤±è´¥: {result_info.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()
