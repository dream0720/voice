import numpy as np
import librosa
from scipy.spatial.distance import cosine
from .enhanced_speaker_extraction import EnhancedSpeakerExtractor

class SpeakerMatcher:
    """
    è¯´è¯äººåŒ¹é…å™¨ (å‡çº§ç‰ˆ)
    ç”¨äºè¯†åˆ«å’ŒåŒ¹é…ç›®æ ‡è¯´è¯äººçš„å£°éŸ³
    
    æ•´åˆäº†å¢å¼ºçš„è¯´è¯äººæå–ç®—æ³•
    """
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        # åˆå§‹åŒ–å¢å¼ºæå–å™¨
        self.enhanced_extractor = EnhancedSpeakerExtractor(sample_rate)
        
    def extract_mfcc_features(self, audio, n_mfcc=13, n_fft=2048, hop_length=512):
        """
        æå–MFCCç‰¹å¾
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘ä¿¡å·
            n_mfcc: MFCCç³»æ•°æ•°é‡
            n_fft: FFTçª—å£å¤§å°
            hop_length: è·³è·ƒé•¿åº¦
            
        Returns:
            mfcc_features: MFCCç‰¹å¾çŸ©é˜µ
        """
        try:
            # ç¡®ä¿éŸ³é¢‘ä¸ä¸ºç©º
            if len(audio) == 0:
                return np.zeros((n_mfcc, 1))
            
            # æå–MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(
                y=audio, 
                sr=self.sample_rate, 
                n_mfcc=n_mfcc,
                n_fft=n_fft,
                hop_length=hop_length
            )
            
            # å½’ä¸€åŒ–
            mfcc = (mfcc - np.mean(mfcc, axis=1, keepdims=True)) / (np.std(mfcc, axis=1, keepdims=True) + 1e-8)
            
            return mfcc
            
        except Exception as e:
            print(f"MFCCæå–å¤±è´¥: {e}")
            return np.zeros((n_mfcc, 1))
    
    def calculate_speaker_similarity(self, mfcc1, mfcc2):
        """
        è®¡ç®—ä¸¤ä¸ªMFCCç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        Args:
            mfcc1: ç¬¬ä¸€ä¸ªMFCCç‰¹å¾
            mfcc2: ç¬¬äºŒä¸ªMFCCç‰¹å¾
            
        Returns:
            similarity: ç›¸ä¼¼åº¦å€¼ (0-1)
        """        try:
            # å±•å¹³ç‰¹å¾å‘é‡
            features1 = mfcc1.flatten()
            features2 = mfcc2.flatten()
            
            # ç¡®ä¿ç‰¹å¾å‘é‡é•¿åº¦ä¸€è‡´
            min_length = min(len(features1), len(features2))
            features1 = features1[:min_length]
            features2 = features2[:min_length]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = 1 - cosine(features1, features2)
            
            # ç¡®ä¿ç»“æœåœ¨åˆç†èŒƒå›´å†…
            similarity = max(0, min(1, similarity))
            
            return similarity
            
        except Exception as e:
            print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def enhance_speaker_separation(self, vocals_audio, reference_audio, separated_vocals_list):
        """
        å¢å¼ºè¯´è¯äººåˆ†ç¦»æ•ˆæœ (ä½¿ç”¨æ–°ç®—æ³•)
        
        Args:
            vocals_audio: åˆ†ç¦»çš„äººå£°éŸ³é¢‘
            reference_audio: å‚è€ƒéŸ³é¢‘
            separated_vocals_list: åˆ†ç¦»çš„äººå£°åˆ—è¡¨ (å…¼å®¹æ€§å‚æ•°)
            
        Returns:
            enhanced_audio: å¢å¼ºåçš„éŸ³é¢‘
            confidence: ç½®ä¿¡åº¦
        """
        try:
            print("ğŸš€ ä½¿ç”¨å¢å¼ºçš„è¯´è¯äººåˆ†ç¦»ç®—æ³•...")            # ä½¿ç”¨æ–°çš„å¢å¼ºæå–å™¨
            segments, confidences = self.enhanced_extractor.extract_target_speaker_segments(
                vocals_audio, reference_audio, min_confidence=0.05
            )
            
            if not segments:
                print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ®µï¼Œè¿”å›åŸéŸ³é¢‘")
                return vocals_audio, 0.1
            
            # é‡æ„éŸ³é¢‘
            enhanced_audio, avg_confidence = self.enhanced_extractor.reconstruct_target_speaker_audio(
                segments, confidences
            )
            
            if len(enhanced_audio) == 0:
                print("âš ï¸ é‡æ„å¤±è´¥ï¼Œè¿”å›åŸéŸ³é¢‘")
                return vocals_audio, 0.1
            
            return enhanced_audio, avg_confidence
            
        except Exception as e:
            print(f"å¢å¼ºåˆ†ç¦»å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
            # å›é€€åˆ°åŸæœ‰ç®—æ³•
            return self._legacy_enhance_speaker_separation(vocals_audio, reference_audio)
    
    def _legacy_enhance_speaker_separation(self, vocals_audio, reference_audio):
        """ä¼ ç»Ÿçš„è¯´è¯äººå¢å¼ºæ–¹æ³• (ä½œä¸ºå¤‡ç”¨)"""
        try:
            # æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
            ref_mfcc = self.extract_mfcc_features(reference_audio)
            
            # æå–äººå£°ç‰¹å¾
            vocals_mfcc = self.extract_mfcc_features(vocals_audio)
            
            # è®¡ç®—ç›¸ä¼¼åº¦ä½œä¸ºç½®ä¿¡åº¦
            confidence = self.calculate_speaker_similarity(ref_mfcc, vocals_mfcc)
            
            # ç®€å•çš„å¢å¼ºå¤„ç†ï¼šæ ¹æ®ç›¸ä¼¼åº¦è°ƒæ•´éŸ³é¢‘
            enhanced_audio = vocals_audio.copy()
            
            # å¦‚æœç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œä¿æŒåŸéŸ³é¢‘
            if confidence > 0.6:
                enhanced_audio = vocals_audio
            elif confidence > 0.3:
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œè¿›è¡Œè½»å¾®å¢å¼º
                enhanced_audio = self.apply_spectral_enhancement(vocals_audio, reference_audio)
            else:
                # ä½ç›¸ä¼¼åº¦ï¼Œå°è¯•æ›´å¤šå¤„ç†
                enhanced_audio = self.apply_adaptive_enhancement(vocals_audio, reference_audio)
            
            return enhanced_audio, confidence
            
        except Exception as e:
            print(f"ä¼ ç»Ÿè¯´è¯äººå¢å¼ºå¤±è´¥: {e}")
            return vocals_audio, 0.3
    
    def process_main_speaker_extraction(self, vocals_path, reference_path, output_path=None):
        """
        ä¸»è¦çš„è¯´è¯äººæå–æµç¨‹ (æ–°å¢æ–¹æ³•)
        
        Args:
            vocals_path: åˆ†ç¦»åçš„äººå£°æ–‡ä»¶è·¯å¾„
            reference_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ (å¯é€‰)
            
        Returns:
            success: å¤„ç†æ˜¯å¦æˆåŠŸ
            result_info: ç»“æœä¿¡æ¯å­—å…¸
        """
        return self.enhanced_extractor.process_main_speaker_extraction(
            vocals_path, reference_path, output_path
        )
    
    def apply_spectral_enhancement(self, audio, reference):
        """
        åº”ç”¨é¢‘è°±å¢å¼º
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            reference: å‚è€ƒéŸ³é¢‘
            
        Returns:
            enhanced_audio: å¢å¼ºåçš„éŸ³é¢‘
        """
        try:
            # ç®€å•çš„é¢‘è°±åŒ¹é…å¢å¼º
            if len(audio) == 0:
                return audio
            
            # åº”ç”¨è½»å¾®çš„æ»¤æ³¢
            enhanced = audio * 1.1  # è½»å¾®æ”¾å¤§
            enhanced = np.clip(enhanced, -1.0, 1.0)  # é™å¹…
            
            return enhanced
            
        except Exception as e:
            print(f"é¢‘è°±å¢å¼ºå¤±è´¥: {e}")
            return audio
    
    def apply_adaptive_enhancement(self, audio, reference):
        """
        åº”ç”¨è‡ªé€‚åº”å¢å¼º
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            reference: å‚è€ƒéŸ³é¢‘
            
        Returns:
            enhanced_audio: å¢å¼ºåçš„éŸ³é¢‘
        """
        try:
            # è‡ªé€‚åº”å¢å¼ºå¤„ç†
            if len(audio) == 0:
                return audio
            
            # èƒ½é‡å½’ä¸€åŒ–
            if np.std(audio) > 0:
                enhanced = (audio - np.mean(audio)) / np.std(audio)
                enhanced = enhanced * 0.1  # ç¼©æ”¾åˆ°åˆé€‚èŒƒå›´
            else:
                enhanced = audio
            
            enhanced = np.clip(enhanced, -1.0, 1.0)
            
            return enhanced
            
        except Exception as e:
            print(f"è‡ªé€‚åº”å¢å¼ºå¤±è´¥: {e}")
            return audio