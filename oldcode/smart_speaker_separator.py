#!/usr/bin/env python3
"""
æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ
Smart Voice Segmentation and Speaker Recognition System

é›†æˆå¤šç§åˆ†æ®µç­–ç•¥å’Œè¯†åˆ«æ–¹æ³•ï¼š
1. åŸºäºèƒ½é‡çš„åˆ†æ®µ
2. åŸºäºéŸ³é‡å˜åŒ–çš„åˆ†æ®µ  
3. åŸºäºé¢‘è°±å˜åŒ–çš„åˆ†æ®µ
4. ç›¸é‚»æ®µç›¸å…³æ€§åˆ†æ
5. å¤šç‰¹å¾èåˆè¯†åˆ«
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from pathlib import Path
import matplotlib.pyplot as plt

class SmartSpeakerSeparator:
    """
    æ™ºèƒ½è¯´è¯äººåˆ†ç¦»å™¨
    """
    
    def __i            print("-" * 80)
            print(f"ğŸ“Š æ™ºèƒ½åˆ†æå®Œæˆ:")
            print(f"   æ€»æ®µæ•°: {len(segments)}")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(segments)*100:.1f}%")
            print(f"   ä¿ç•™æ—¶é•¿: {total_kept_duration:.2f}s / {len(mixed_audio)/self.sample_rate:.2f}s")
            
            # æ€§åˆ«åˆ†æç»Ÿè®¡
            female_segments = sum(1 for _, _, _ in segments if self.analyze_gender_characteristics(
                mixed_audio[int(_*self.sample_rate):int(__*self.sample_rate)]) 
                and self.analyze_gender_characteristics(mixed_audio[int(_*self.sample_rate):int(__*self.sample_rate)])['is_likely_female'])
            print(f"   æ£€æµ‹å¥³å£°æ®µ: {female_segments}/{len(segments)}")
            print(f"   ç”·å£°æŠ‘åˆ¶: {'å¯ç”¨' if self.male_voice_suppression else 'ç¦ç”¨'}")
            print(f"   å¥³å£°å¢å¼º: {'å¯ç”¨' if self.female_voice_boost else 'ç¦ç”¨'}")self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13        # å¤šå±‚åˆ†æ®µå‚æ•°
        self.energy_threshold_ratio = 0.05   # è¿›ä¸€æ­¥é™ä½èƒ½é‡é˜ˆå€¼ï¼Œæ•è·æ›´å¤šä½éŸ³é‡æ®µ
        self.volume_change_threshold = 0.3   # é™ä½éŸ³é‡å˜åŒ–é˜ˆå€¼
        self.spectral_change_threshold = 0.25 # é™ä½é¢‘è°±å˜åŒ–é˜ˆå€¼
        self.min_segment_duration = 0.25     # è¿›ä¸€æ­¥é™ä½æœ€å°æ®µé•¿åº¦
        self.merge_threshold = 0.55          # é™ä½æ®µåˆå¹¶é˜ˆå€¼
        
        # è¯´è¯äººè¯†åˆ«å‚æ•°
        self.similarity_threshold = 0.30     # è¿›ä¸€æ­¥é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # ç”·å¥³å£°åˆ†ç¦»å‚æ•°ï¼ˆæ–°å¢ï¼‰
        self.male_voice_suppression = True   # å¯ç”¨ç”·å£°æŠ‘åˆ¶
        self.female_voice_boost = True       # å¯ç”¨å¥³å£°å¢å¼º
        self.low_freq_cutoff = 300          # ç”·å£°ä½é¢‘æˆªæ­¢é¢‘ç‡
        self.energy_dominance_ratio = 0.6   # èƒ½é‡å ä¼˜æ¯”ä¾‹
        self.pitch_gender_threshold = 165   # æ€§åˆ«åŸºé¢‘é˜ˆå€¼ï¼ˆHzï¼‰
        
    def multi_level_segmentation(self, audio):
        """
        å¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            segments: [(start_time, end_time, confidence), ...] 
        """
        print("ğŸ” æ‰§è¡Œå¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ...")
        
        # ç¬¬ä¸€å±‚ï¼šåŸºäºèƒ½é‡çš„ç²—åˆ†æ®µ
        energy_segments = self.energy_based_segmentation(audio)
        print(f"   èƒ½é‡åˆ†æ®µ: {len(energy_segments)} ä¸ªåˆå§‹æ®µ")
        
        # ç¬¬äºŒå±‚ï¼šåŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†æ®µ
        volume_segments = self.volume_change_segmentation(audio, energy_segments)
        print(f"   éŸ³é‡ç»†åˆ†: {len(volume_segments)} ä¸ªæ®µ")
          # ç¬¬ä¸‰å±‚ï¼šåŸºäºé¢‘è°±å˜åŒ–çš„ç²¾ç»†åˆ†æ®µ
        spectral_segments = self.spectral_change_segmentation(audio, volume_segments)
        print(f"   é¢‘è°±ç»†åˆ†: {len(spectral_segments)} ä¸ªæ®µ")
        
        # ç¬¬å››å±‚ï¼šç›¸é‚»æ®µç›¸å…³æ€§åˆ†æå’Œåˆå¹¶
        merged_segments = self.merge_similar_segments(audio, spectral_segments)
        print(f"   ç›¸å…³æ€§åˆå¹¶: {len(merged_segments)} ä¸ªæœ€ç»ˆæ®µ")
        
        # ç¬¬äº”å±‚ï¼šéªŒè¯å’Œè¡¥å……é—æ¼çš„è¯­éŸ³æ®µ
        final_segments = self.validate_and_supplement_segments(audio, merged_segments)
        print(f"   éªŒè¯è¡¥å……: {len(final_segments)} ä¸ªéªŒè¯æ®µ")
        
        return final_segments
    
    def energy_based_segmentation(self, audio):
        """æ”¹è¿›çš„åŸºäºèƒ½é‡çš„åˆ†æ®µ"""
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_size = int(0.02 * self.sample_rate)   # ç¼©çŸ­åˆ°20ms
        hop_size = int(0.005 * self.sample_rate)    # ç¼©çŸ­åˆ°5msï¼Œæ›´ç²¾ç»†
        
        energy = []
        for i in range(0, len(audio) - frame_size + 1, hop_size):
            frame = audio[i:i + frame_size]
            energy.append(np.sum(frame ** 2))
        
        energy = np.array(energy)
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„é˜ˆå€¼è®¡ç®—
        energy_sorted = np.sort(energy)
        # ä½¿ç”¨è¾ƒä½çš„ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼ï¼Œé¿å…æ¼æ‰ä½éŸ³é‡çš„è¯­éŸ³
        threshold = np.percentile(energy_sorted, 15)  # ä½¿ç”¨15%åˆ†ä½æ•°
        
        # å¦‚æœé˜ˆå€¼è¿‡ä½ï¼Œä½¿ç”¨å‡å€¼çš„ä¸€å®šæ¯”ä¾‹
        energy_mean = np.mean(energy)
        adaptive_threshold = energy_mean * self.energy_threshold_ratio
        threshold = max(threshold, adaptive_threshold)
        
        print(f"   èƒ½é‡åˆ†æ®µé˜ˆå€¼: {threshold:.8f} (å‡å€¼: {energy_mean:.8f})")
        
        # æ‰¾åˆ°è¯­éŸ³æ®µ
        speech_mask = energy > threshold
        
        # æ›´å®½æ¾çš„å¹³æ»‘å¤„ç†
        speech_mask = signal.medfilt(speech_mask.astype(int), kernel_size=3).astype(bool)
        
        # å½¢æ€å­¦æ“ä½œï¼šå¡«å……å°çš„é—´éš™
        from scipy.ndimage import binary_closing, binary_opening
        speech_mask = binary_closing(speech_mask, structure=np.ones(5))
        speech_mask = binary_opening(speech_mask, structure=np.ones(3))
        
        # è½¬æ¢ä¸ºæ—¶é—´æ®µ
        segments = []
        frame_times = np.arange(len(speech_mask)) * hop_size / self.sample_rate
        
        in_speech = False
        start_time = 0
        
        for i, is_speech in enumerate(speech_mask):
            current_time = frame_times[i]
            
            if is_speech and not in_speech:
                start_time = current_time
                in_speech = True
            elif not is_speech and in_speech:
                end_time = current_time
                # æ›´å®½æ¾çš„æœ€å°æ—¶é•¿é™åˆ¶
                if end_time - start_time >= self.min_segment_duration:
                    segments.append((start_time, end_time, 1.0))
                    print(f"     èƒ½é‡æ®µ: {start_time:.2f}s-{end_time:.2f}s (æ—¶é•¿: {end_time-start_time:.2f}s)")
                in_speech = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µ
        if in_speech:
            end_time = len(audio) / self.sample_rate
            if end_time - start_time >= self.min_segment_duration:
                segments.append((start_time, end_time, 1.0))
                print(f"     èƒ½é‡æ®µ: {start_time:.2f}s-{end_time:.2f}s (æ—¶é•¿: {end_time-start_time:.2f}s)")
        
        return segments
    
    def volume_change_segmentation(self, audio, initial_segments):
        """åŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†æ®µ"""
        refined_segments = []
        
        for start_time, end_time, confidence in initial_segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            segment_audio = audio[start_sample:end_sample]
            
            # è®¡ç®—æ»‘åŠ¨çª—å£RMS
            window_size = int(0.2 * self.sample_rate)  # 200msçª—å£
            hop_size = int(0.05 * self.sample_rate)    # 50msè·³è·ƒ
            
            rms_values = []
            rms_times = []
            
            for i in range(0, len(segment_audio) - window_size + 1, hop_size):
                window = segment_audio[i:i + window_size]
                rms = np.sqrt(np.mean(window ** 2))
                rms_values.append(rms)
                rms_times.append(start_time + i / self.sample_rate)
            
            if len(rms_values) < 3:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            rms_values = np.array(rms_values)
            
            # æ£€æµ‹éŸ³é‡å˜åŒ–ç‚¹
            rms_diff = np.abs(np.diff(rms_values))
            rms_mean_diff = np.mean(rms_diff)
            change_points = np.where(rms_diff > rms_mean_diff * 2)[0]
            
            # åˆ†å‰²æ®µ
            if len(change_points) > 0:
                prev_time = start_time
                for cp in change_points:
                    split_time = rms_times[cp]
                    if split_time - prev_time >= self.min_segment_duration:
                        refined_segments.append((prev_time, split_time, confidence))
                        prev_time = split_time
                
                # æœ€åä¸€æ®µ
                if end_time - prev_time >= self.min_segment_duration:
                    refined_segments.append((prev_time, end_time, confidence))
            else:
                refined_segments.append((start_time, end_time, confidence))
        
        return refined_segments
    
    def spectral_change_segmentation(self, audio, segments):
        """åŸºäºé¢‘è°±å˜åŒ–çš„ç²¾ç»†åˆ†æ®µ"""
        refined_segments = []
        
        for start_time, end_time, confidence in segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            segment_audio = audio[start_sample:end_sample]
            
            # è®¡ç®—é¢‘è°±ç‰¹å¾
            window_size = int(0.5 * self.sample_rate)  # 500msçª—å£
            hop_size = int(0.1 * self.sample_rate)     # 100msè·³è·ƒ
            
            spectral_features = []
            feature_times = []
            
            for i in range(0, len(segment_audio) - window_size + 1, hop_size):
                window = segment_audio[i:i + window_size]
                
                # æå–MFCC
                mfcc = librosa.feature.mfcc(y=window, sr=self.sample_rate, n_mfcc=5)
                mfcc_mean = np.mean(mfcc, axis=1)
                
                spectral_features.append(mfcc_mean)
                feature_times.append(start_time + i / self.sample_rate)
            
            if len(spectral_features) < 3:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            spectral_features = np.array(spectral_features)
            
            # æ£€æµ‹é¢‘è°±å˜åŒ–ç‚¹
            change_points = []
            for i in range(1, len(spectral_features)):
                # è®¡ç®—ç›¸é‚»ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦
                sim = 1 - cosine(spectral_features[i-1], spectral_features[i])
                if sim < (1 - self.spectral_change_threshold):
                    change_points.append(i)
            
            # åˆ†å‰²æ®µ
            if len(change_points) > 0:
                prev_time = start_time
                for cp in change_points:
                    split_time = feature_times[cp]
                    if split_time - prev_time >= self.min_segment_duration:
                        refined_segments.append((prev_time, split_time, confidence))
                        prev_time = split_time
                
                # æœ€åä¸€æ®µ
                if end_time - prev_time >= self.min_segment_duration:
                    refined_segments.append((prev_time, end_time, confidence))
            else:
                refined_segments.append((start_time, end_time, confidence))
        
        return refined_segments
    
    def merge_similar_segments(self, audio, segments):
        """åˆå¹¶ç›¸ä¼¼çš„ç›¸é‚»æ®µ"""
        if len(segments) <= 1:
            return segments
        
        print("ğŸ”„ åˆ†æç›¸é‚»æ®µç›¸å…³æ€§...")
        
        merged_segments = []
        current_start, current_end, current_conf = segments[0]
        
        for start_time, end_time, confidence in segments[1:]:
            # æå–ä¸¤ä¸ªæ®µçš„ç‰¹å¾
            curr_start_sample = int(current_start * self.sample_rate)
            curr_end_sample = int(current_end * self.sample_rate)
            curr_audio = audio[curr_start_sample:curr_end_sample]
            
            next_start_sample = int(start_time * self.sample_rate)
            next_end_sample = int(end_time * self.sample_rate)
            next_audio = audio[next_start_sample:next_end_sample]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_segment_similarity(curr_audio, next_audio)
            
            print(f"   æ®µ {current_start:.2f}-{current_end:.2f}s ä¸ {start_time:.2f}-{end_time:.2f}s ç›¸ä¼¼åº¦: {similarity:.3f}")
            
            if similarity > self.merge_threshold:
                # åˆå¹¶æ®µ
                current_end = end_time
                current_conf = max(current_conf, confidence)
                print(f"     -> åˆå¹¶æ®µ: {current_start:.2f}-{current_end:.2f}s")
            else:
                # ä¿å­˜å½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
                merged_segments.append((current_start, current_end, current_conf))
                current_start, current_end, current_conf = start_time, end_time, confidence
        
        # æ·»åŠ æœ€åä¸€æ®µ
        merged_segments.append((current_start, current_end, current_conf))
        
        return merged_segments
    
    def calculate_segment_similarity(self, audio1, audio2):
        """è®¡ç®—ä¸¤ä¸ªéŸ³é¢‘æ®µçš„ç›¸ä¼¼åº¦"""
        try:
            # é•¿åº¦å½’ä¸€åŒ–
            min_len = min(len(audio1), len(audio2))
            if min_len < self.sample_rate * 0.2:  # å¤ªçŸ­çš„æ®µ
                return 0.0
            
            audio1 = audio1[:min_len]
            audio2 = audio2[:min_len]
            
            similarities = []
            
            # 1. æ³¢å½¢ç›¸å…³æ€§
            correlation, _ = pearsonr(audio1, audio2)
            if not np.isnan(correlation):
                similarities.append(abs(correlation))
            
            # 2. MFCCç›¸ä¼¼åº¦
            try:
                mfcc1 = librosa.feature.mfcc(y=audio1, sr=self.sample_rate, n_mfcc=13)
                mfcc2 = librosa.feature.mfcc(y=audio2, sr=self.sample_rate, n_mfcc=13)
                
                mfcc1_mean = np.mean(mfcc1, axis=1)
                mfcc2_mean = np.mean(mfcc2, axis=1)
                
                mfcc_sim = 1 - cosine(mfcc1_mean, mfcc2_mean)
                if not np.isnan(mfcc_sim):
                    similarities.append(max(0, mfcc_sim))
            except:
                pass
            
            # 3. åŸºé¢‘ç›¸ä¼¼åº¦
            try:
                f0_1 = librosa.yin(audio1, fmin=60, fmax=500, sr=self.sample_rate)
                f0_2 = librosa.yin(audio2, fmin=60, fmax=500, sr=self.sample_rate)
                
                f0_1_clean = f0_1[f0_1 > 0]
                f0_2_clean = f0_2[f0_2 > 0]
                
                if len(f0_1_clean) > 5 and len(f0_2_clean) > 5:
                    f0_1_mean = np.mean(f0_1_clean)
                    f0_2_mean = np.mean(f0_2_clean)
                    f0_diff = abs(f0_1_mean - f0_2_mean)
                    f0_sim = 1 / (1 + f0_diff / 20)
                    similarities.append(f0_sim)
            except:
                pass
            
            # 4. èƒ½é‡ç›¸ä¼¼åº¦
            rms1 = np.sqrt(np.mean(audio1 ** 2))
            rms2 = np.sqrt(np.mean(audio2 ** 2))
            if rms1 > 0 and rms2 > 0:
                energy_sim = min(rms1, rms2) / max(rms1, rms2)
                similarities.append(energy_sim)
            
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            print(f"     ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def extract_comprehensive_features(self, audio):
        """æå–ç»¼åˆç‰¹å¾"""
        if len(audio) < self.sample_rate * 0.3:  # å¤ªçŸ­
            return None
        
        try:
            features = {}
            
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            
            # åŸºé¢‘ç‰¹å¾
            try:
                f0 = librosa.yin(audio, fmin=60, fmax=500, sr=self.sample_rate)
                f0_clean = f0[f0 > 0]
                if len(f0_clean) > 10:
                    features['f0_mean'] = np.mean(f0_clean)
                    features['f0_std'] = np.std(f0_clean)
                    features['f0_median'] = np.median(f0_clean)
                    features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)
                else:
                    features['f0_mean'] = features['f0_std'] = features['f0_median'] = features['f0_range'] = 0
            except:
                features['f0_mean'] = features['f0_std'] = features['f0_median'] = features['f0_range'] = 0
            
            # é¢‘è°±ç‰¹å¾
            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate))
            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate))
            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate))
            
            # è¿‡é›¶ç‡
            features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(audio))
            
            # èƒ½é‡ç‰¹å¾
            features['rms'] = np.sqrt(np.mean(audio ** 2))
            
            # è‰²åº¦ç‰¹å¾
            chroma = librosa.feature.chroma_stft(y=audio, sr=self.sample_rate)
            features['chroma_mean'] = np.mean(chroma, axis=1)
            
            return features
            
        except Exception as e:
            print(f"   ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def advanced_speaker_similarity(self, features1, features2):
        """é«˜çº§è¯´è¯äººç›¸ä¼¼åº¦è®¡ç®—"""
        if features1 is None or features2 is None:
            return 0.0
        
        try:
            similarities = []
            weights = []
            
            # MFCCç›¸ä¼¼åº¦ (æœ€é‡è¦)
            mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
            if not np.isnan(mfcc_sim):
                similarities.append(max(0, mfcc_sim))
                weights.append(0.4)
            
            # åŸºé¢‘ç›¸ä¼¼åº¦
            if features1['f0_mean'] > 0 and features2['f0_mean'] > 0:
                f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
                f0_sim = 1 / (1 + f0_diff / 25)
                similarities.append(f0_sim)
                weights.append(0.25)
                
                # åŸºé¢‘èŒƒå›´ç›¸ä¼¼åº¦
                f0_range_diff = abs(features1['f0_range'] - features2['f0_range'])
                f0_range_sim = 1 / (1 + f0_range_diff / 50)
                similarities.append(f0_range_sim)
                weights.append(0.1)
            
            # é¢‘è°±ç‰¹å¾ç›¸ä¼¼åº¦
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 400)
            similarities.append(centroid_sim)
            weights.append(0.15)
            
            # è‰²åº¦ç›¸ä¼¼åº¦
            chroma_sim = 1 - cosine(features1['chroma_mean'], features2['chroma_mean'])
            if not np.isnan(chroma_sim):
                similarities.append(max(0, chroma_sim))
                weights.append(0.1)
            
            # åŠ æƒå¹³å‡
            if similarities and weights:
                weights = np.array(weights[:len(similarities)])
                weights = weights / np.sum(weights)
                final_similarity = np.average(similarities, weights=weights)
                return max(0, min(1, final_similarity))
            else:
                return 0.0
                
        except Exception as e:
            print(f"   é«˜çº§ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def extract_target_speaker(self, mixed_audio_path, reference_audio_path, output_path):
        """æå–ç›®æ ‡è¯´è¯äºº"""
        try:
            print("ğŸ¯ æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ")
            print("=" * 70)
            
            # åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            mixed_audio, _ = librosa.load(mixed_audio_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_audio_path, sr=self.sample_rate)
            
            print(f"   æ··åˆéŸ³é¢‘: {len(mixed_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("\nğŸ” åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            ref_features = self.extract_comprehensive_features(reference_audio)
            if ref_features is None:
                print("âŒ æ— æ³•æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾")
                return False, {"error": "Reference feature extraction failed"}
            
            print(f"   å‚è€ƒåŸºé¢‘: {ref_features['f0_mean']:.1f} Hz")
            print(f"   å‚è€ƒé¢‘è°±è´¨å¿ƒ: {ref_features['spectral_centroid']:.1f} Hz")
            print(f"   å‚è€ƒåŸºé¢‘èŒƒå›´: {ref_features['f0_range']:.1f} Hz")
            
            # æ™ºèƒ½åˆ†æ®µ
            segments = self.multi_level_segmentation(mixed_audio)
            
            if not segments:
                print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ®µ")
                return False, {"error": "No valid segments detected"}
            
            # åˆå§‹åŒ–è¾“å‡º
            output_audio = np.zeros_like(mixed_audio)
              # é€æ®µåˆ†æ
            print(f"\nğŸ”„ æ™ºèƒ½è¯´è¯äººè¯†åˆ« - åˆ†æ {len(segments)} ä¸ªè¯­éŸ³æ®µ...")
            print("-" * 80)
            print("æ®µå· æ€§åˆ« æ—¶é—´èŒƒå›´        æ—¶é•¿   åŸºé¢‘   ç»¼åˆåˆ†   å†³ç­–   è¯´æ˜")
            print("-" * 80)
            
            kept_segments = 0
            total_kept_duration = 0
            
            for i, (start_time, end_time, confidence) in enumerate(segments):
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                
                start_sample = max(0, start_sample)
                end_sample = min(len(mixed_audio), end_sample)
                
                if end_sample <= start_sample:
                    continue
                
                segment_audio = mixed_audio[start_sample:end_sample]
                segment_duration = end_time - start_time
                  # æå–æ®µç‰¹å¾
                segment_features = self.extract_comprehensive_features(segment_audio)
                
                if segment_features is None:
                    print(f"{i+1:2d}    {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s   -    -      âŒ     ç‰¹å¾æå–å¤±è´¥")
                    continue
                
                # æ€§åˆ«ç‰¹å¾åˆ†æï¼ˆæ–°å¢ï¼‰
                gender_info = self.analyze_gender_characteristics(segment_audio)
                
                # èƒ½é‡å ä¼˜åˆ†æï¼ˆæ–°å¢ï¼‰
                energy_analysis = self.energy_dominance_analysis(segment_audio, reference_audio)
                
                # è®¡ç®—é«˜çº§ç›¸ä¼¼åº¦
                similarity = self.advanced_speaker_similarity(ref_features, segment_features)
                
                # å¢å¼ºå†³ç­–é€»è¾‘ï¼ˆè€ƒè™‘æ€§åˆ«ç‰¹å¾å’Œèƒ½é‡å ä¼˜ï¼‰
                decision_score = similarity
                
                # æ€§åˆ«ç‰¹å¾åŠ æƒ
                if gender_info:
                    if gender_info['is_likely_female']:
                        decision_score += 0.1  # å¥³å£°ç‰¹å¾åŠ åˆ†
                    if gender_info['f0_mean'] > self.pitch_gender_threshold:
                        decision_score += 0.05  # é«˜åŸºé¢‘åŠ åˆ†
                    if gender_info['low_freq_ratio'] < 0.3:  # ä½é¢‘å æ¯”ä½ï¼ˆéç”·å£°ç‰¹å¾ï¼‰
                        decision_score += 0.05
                
                # èƒ½é‡å ä¼˜åŠ æƒ
                if energy_analysis and energy_analysis['is_dominant']:
                    decision_score += 0.15 * energy_analysis['dominance_score']
                
                # åº”ç”¨é¢‘åŸŸå¤„ç†
                processed_segment = segment_audio.copy()
                if gender_info and gender_info['is_likely_female']:
                    # å¯¹å¯èƒ½çš„å¥³å£°è¿›è¡Œå¢å¼º
                    processed_segment = self.enhance_female_voice_frequency(processed_segment)
                else:
                    # å¯¹å¯èƒ½çš„ç”·å£°è¿›è¡ŒæŠ‘åˆ¶
                    processed_segment = self.suppress_male_voice_frequency(processed_segment)
                
                # å†³ç­–é€»è¾‘
                if decision_score >= self.similarity_threshold:
                    # é«˜ç›¸ä¼¼åº¦ï¼šä¿ç•™
                    output_audio[start_sample:end_sample] = processed_segment
                    kept_segments += 1
                    total_kept_duration += segment_duration
                    decision = "âœ… ä¿ç•™"
                    reason = "ç›®æ ‡è¯´è¯äºº"
                    
                elif decision_score >= self.similarity_threshold * 0.7:
                    # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šéƒ¨åˆ†ä¿ç•™
                    output_audio[start_sample:end_sample] = processed_segment * 0.6
                    kept_segments += 0.6
                    total_kept_duration += segment_duration * 0.6
                    decision = "ğŸ”¶ éƒ¨åˆ†"
                    reason = "å¯èƒ½ç›®æ ‡"
                    
                else:
                    # ä½ç›¸ä¼¼åº¦ï¼šä¸¢å¼ƒ
                    decision = "âŒ ä¸¢å¼ƒ"
                    reason = "éç›®æ ‡è¯´è¯äºº"
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                gender_mark = "â™€" if gender_info and gender_info['is_likely_female'] else "â™‚"
                energy_mark = "âš¡" if energy_analysis and energy_analysis['is_dominant'] else "  "
                
                print(f"{i+1:2d}  {gender_mark}{energy_mark} {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s  {segment_features['f0_mean']:5.1f}  {decision_score:6.3f}  {decision}  {reason}")
            
            print("-" * 70)
            print(f"ğŸ“Š æ™ºèƒ½åˆ†æå®Œæˆ:")
            print(f"   æ€»æ®µæ•°: {len(segments)}")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments:.1f}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(segments)*100:.1f}%")
            print(f"   ä¿ç•™æ—¶é•¿: {total_kept_duration:.2f}s / {len(mixed_audio)/self.sample_rate:.2f}s")
            
            # åå¤„ç†
            output_audio = self.post_process_audio(output_audio)
            
            # ä¿å­˜ç»“æœ
            if output_path:
                sf.write(output_path, output_audio, self.sample_rate)
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
            
            result_info = {
                'success': True,
                'total_segments': len(segments),
                'kept_segments': kept_segments,
                'keep_ratio': kept_segments / len(segments),
                'original_duration': len(mixed_audio) / self.sample_rate,
                'output_duration': len(output_audio) / self.sample_rate,
                'kept_speech_duration': total_kept_duration,
                'similarity_threshold': self.similarity_threshold
            }
            
            return True, result_info
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½åˆ†ç¦»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}
    
    def post_process_audio(self, audio):
        """æ™ºèƒ½éŸ³é¢‘åå¤„ç†"""
        if len(audio) == 0:
            return audio
        
        # å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # æ™ºèƒ½å™ªå£°é—¨é™
        rms = np.sqrt(np.mean(audio ** 2))
        threshold = rms * 0.05
        mask = np.abs(audio) < threshold
        audio[mask] = 0
        
        # è½»å¾®çš„å¹³æ»‘
        audio = signal.savgol_filter(audio, window_length=5, polyorder=2)
        
        # å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio

    def validate_and_supplement_segments(self, audio, segments):
        """éªŒè¯å¹¶è¡¥å……é—æ¼çš„è¯­éŸ³æ®µ"""
        print("ğŸ” éªŒè¯è¯­éŸ³æ®µå®Œæ•´æ€§...")
        
        # åˆ›å»ºå·²è¦†ç›–çš„æ—¶é—´æ©ç 
        total_duration = len(audio) / self.sample_rate
        covered_mask = np.zeros(int(total_duration * 10))  # 0.1sç²¾åº¦
        
        for start_time, end_time, confidence in segments:
            start_idx = int(start_time * 10)
            end_idx = int(end_time * 10)
            if end_idx < len(covered_mask):
                covered_mask[start_idx:end_idx] = 1
        
        # å¯»æ‰¾æœªè¦†ç›–çš„åŒºåŸŸ
        uncovered_regions = []
        in_uncovered = False
        start_uncovered = 0
        
        for i, is_covered in enumerate(covered_mask):
            current_time = i / 10
            
            if not is_covered and not in_uncovered:
                start_uncovered = current_time
                in_uncovered = True
            elif is_covered and in_uncovered:
                end_uncovered = current_time
                if end_uncovered - start_uncovered >= 0.5:  # è‡³å°‘0.5ç§’çš„é—´éš™
                    uncovered_regions.append((start_uncovered, end_uncovered))
                in_uncovered = False
        
        # å¤„ç†æœ€åä¸€ä¸ªåŒºåŸŸ
        if in_uncovered:
            end_uncovered = total_duration
            if end_uncovered - start_uncovered >= 0.5:
                uncovered_regions.append((start_uncovered, end_uncovered))
        
        print(f"   å‘ç° {len(uncovered_regions)} ä¸ªæœªè¦†ç›–åŒºåŸŸ")
        
        # æ£€æŸ¥æœªè¦†ç›–åŒºåŸŸæ˜¯å¦åŒ…å«è¯­éŸ³
        supplemented_segments = list(segments)
        
        for start_time, end_time in uncovered_regions:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            end_sample = min(end_sample, len(audio))
            
            if end_sample <= start_sample:
                continue
            
            region_audio = audio[start_sample:end_sample]
            
            # æ£€æŸ¥è¯¥åŒºåŸŸæ˜¯å¦åŒ…å«è¯­éŸ³
            rms = np.sqrt(np.mean(region_audio ** 2))
            energy_threshold = np.sqrt(np.mean(audio ** 2)) * 0.1  # æ›´ä½çš„é˜ˆå€¼
            
            if rms > energy_threshold:
                print(f"     è¡¥å……è¯­éŸ³æ®µ: {start_time:.2f}s-{end_time:.2f}s (RMS: {rms:.6f})")
                supplemented_segments.append((start_time, end_time, 0.8))  # è¾ƒä½çš„ç½®ä¿¡åº¦
        
        # æŒ‰æ—¶é—´æ’åº
        supplemented_segments.sort(key=lambda x: x[0])
        
        return supplemented_segments

    def analyze_gender_characteristics(self, audio_segment):
        """
        åˆ†æéŸ³é¢‘æ®µçš„æ€§åˆ«ç‰¹å¾
        
        Args:
            audio_segment: éŸ³é¢‘æ®µ
            
        Returns:
            dict: æ€§åˆ«ç‰¹å¾ä¿¡æ¯
        """
        try:
            if len(audio_segment) < self.frame_length:
                return None
                
            # åŸºé¢‘åˆ†æ
            f0 = librosa.yin(audio_segment, fmin=50, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) < 5:
                return None
                
            f0_mean = np.mean(f0_clean)
            f0_std = np.std(f0_clean)
            
            # æ€§åˆ«åˆ¤æ–­ï¼ˆç®€å•åŸºäºåŸºé¢‘ï¼‰
            is_likely_female = f0_mean > self.pitch_gender_threshold
            
            # é¢‘è°±ç‰¹å¾
            stft = librosa.stft(audio_segment, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            
            # ä½é¢‘èƒ½é‡å æ¯”ï¼ˆç”·å£°ç‰¹å¾ï¼‰
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            low_freq_mask = freq_bins < self.low_freq_cutoff
            low_freq_energy = np.sum(magnitude[low_freq_mask, :])
            total_energy = np.sum(magnitude)
            low_freq_ratio = low_freq_energy / total_energy if total_energy > 0 else 0
            
            # é«˜é¢‘èƒ½é‡å æ¯”ï¼ˆå¥³å£°ç‰¹å¾ï¼‰
            high_freq_mask = freq_bins > 1000
            high_freq_energy = np.sum(magnitude[high_freq_mask, :])
            high_freq_ratio = high_freq_energy / total_energy if total_energy > 0 else 0
            
            # RMSèƒ½é‡
            rms = np.sqrt(np.mean(audio_segment ** 2))
            
            return {
                'f0_mean': f0_mean,
                'f0_std': f0_std,
                'is_likely_female': is_likely_female,
                'low_freq_ratio': low_freq_ratio,
                'high_freq_ratio': high_freq_ratio,
                'rms_energy': rms,
                'gender_confidence': abs(f0_mean - self.pitch_gender_threshold) / 100
            }
            
        except Exception as e:
            print(f"æ€§åˆ«ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return None
    
    def suppress_male_voice_frequency(self, audio_segment):
        """
        åŸºäºé¢‘åŸŸçš„ç”·å£°æŠ‘åˆ¶
        
        Args:
            audio_segment: éŸ³é¢‘æ®µ
            
        Returns:
            numpy.ndarray: å¤„ç†åçš„éŸ³é¢‘
        """
        if not self.male_voice_suppression:
            return audio_segment
            
        try:
            # STFTå˜æ¢
            stft = librosa.stft(audio_segment, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # ç”·å£°ä½é¢‘æŠ‘åˆ¶ï¼ˆ50-300HzåŒºåŸŸï¼‰
            low_freq_mask = (freq_bins >= 50) & (freq_bins <= self.low_freq_cutoff)
            
            # è®¡ç®—æŠ‘åˆ¶å¼ºåº¦
            suppression_factor = 0.3  # æŠ‘åˆ¶åˆ°30%
            
            # åº”ç”¨æŠ‘åˆ¶
            magnitude[low_freq_mask, :] *= suppression_factor
            
            # é€†å˜æ¢
            stft_processed = magnitude * np.exp(1j * phase)
            audio_processed = librosa.istft(stft_processed, hop_length=self.hop_length)
            
            return audio_processed
            
        except Exception as e:
            print(f"ç”·å£°é¢‘åŸŸæŠ‘åˆ¶å¤±è´¥: {e}")
            return audio_segment
    
    def enhance_female_voice_frequency(self, audio_segment):
        """
        åŸºäºé¢‘åŸŸçš„å¥³å£°å¢å¼º
        
        Args:
            audio_segment: éŸ³é¢‘æ®µ
            
        Returns:
            numpy.ndarray: å¤„ç†åçš„éŸ³é¢‘
        """
        if not self.female_voice_boost:
            return audio_segment
            
        try:
            # STFTå˜æ¢
            stft = librosa.stft(audio_segment, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å¥³å£°å…³é”®é¢‘ç‡å¢å¼ºï¼ˆ150-300HzåŸºé¢‘åŒºï¼Œ1000-3000Hzå…±æŒ¯å³°åŒºï¼‰
            female_fundamental_mask = (freq_bins >= 150) & (freq_bins <= 300)
            female_formant_mask = (freq_bins >= 1000) & (freq_bins <= 3000)
            
            # åº”ç”¨å¢å¼º
            enhancement_factor = 1.2  # å¢å¼ºåˆ°120%
            magnitude[female_fundamental_mask, :] *= enhancement_factor
            magnitude[female_formant_mask, :] *= enhancement_factor
            
            # é€†å˜æ¢
            stft_processed = magnitude * np.exp(1j * phase)
            audio_processed = librosa.istft(stft_processed, hop_length=self.hop_length)
            
            return audio_processed
            
        except Exception as e:
            print(f"å¥³å£°é¢‘åŸŸå¢å¼ºå¤±è´¥: {e}")
            return audio_segment

    def energy_dominance_analysis(self, mixed_segment, ref_segment):
        """
        èƒ½é‡å ä¼˜åˆ†æ - åˆ©ç”¨å¥³å£°èƒ½é‡å¼ºçš„ç‰¹ç‚¹
        
        Args:
            mixed_segment: æ··åˆéŸ³é¢‘æ®µ
            ref_segment: å‚è€ƒéŸ³é¢‘æ®µ
            
        Returns:
            dict: èƒ½é‡åˆ†æç»“æœ
        """
        try:
            # RMSèƒ½é‡è®¡ç®—
            mixed_rms = np.sqrt(np.mean(mixed_segment ** 2))
            ref_rms = np.sqrt(np.mean(ref_segment ** 2))
            
            # èƒ½é‡æ¯”å€¼
            energy_ratio = mixed_rms / ref_rms if ref_rms > 0 else 0
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºèƒ½é‡å ä¼˜æ®µï¼ˆå¥³å£°ä¸»å¯¼ï¼‰
            is_dominant = energy_ratio > self.energy_dominance_ratio
            
            # é¢‘è°±èƒ½é‡åˆ†å¸ƒåˆ†æ
            mixed_stft = librosa.stft(mixed_segment, hop_length=self.hop_length)
            mixed_magnitude = np.abs(mixed_stft)
            
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å¥³å£°é¢‘ç‡èŒƒå›´èƒ½é‡å æ¯”
            female_freq_mask = (freq_bins >= 150) & (freq_bins <= 3000)
            female_energy = np.sum(mixed_magnitude[female_freq_mask, :])
            total_energy = np.sum(mixed_magnitude)
            female_energy_ratio = female_energy / total_energy if total_energy > 0 else 0
            
            return {
                'mixed_rms': mixed_rms,
                'ref_rms': ref_rms,
                'energy_ratio': energy_ratio,
                'is_dominant': is_dominant,
                'female_energy_ratio': female_energy_ratio,
                'dominance_score': min(energy_ratio * female_energy_ratio, 2.0)
            }
            
        except Exception as e:
            print(f"èƒ½é‡å ä¼˜åˆ†æå¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    separator = SmartSpeakerSeparator(sample_rate=22050)
    
    # æ–‡ä»¶è·¯å¾„
    mixed_path = "temp/demucs_output/htdemucs/answer/vocals.wav"
    reference_path = "reference/refne2.wav"
    output_path = "output/smart_separated_speaker.wav"
    
    # æ£€æŸ¥æ–‡ä»¶
    if not Path(mixed_path).exists():
        print(f"âŒ æ··åˆéŸ³é¢‘ä¸å­˜åœ¨: {mixed_path}")
        return
    
    if not Path(reference_path).exists():
        print(f"âŒ å‚è€ƒéŸ³é¢‘ä¸å­˜åœ¨: {reference_path}")
        return
    
    # æ‰§è¡Œæ™ºèƒ½åˆ†ç¦»
    success, result_info = separator.extract_target_speaker(
        mixed_path, reference_path, output_path
    )
    
    if success:
        print(f"\nğŸ‰ æ™ºèƒ½è¯´è¯äººåˆ†ç¦»æˆåŠŸ!")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   ä¿ç•™æ®µæ•°: {result_info['kept_segments']:.1f}/{result_info['total_segments']}")
        print(f"   ä¿ç•™ç‡: {result_info['keep_ratio']*100:.1f}%")
        print(f"   æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {result_info['kept_speech_duration']:.2f}s")
        
        # æ™ºèƒ½è¯„ä¼°
        if result_info['keep_ratio'] > 0.7:
            print("ğŸ† åˆ†ç¦»è´¨é‡: ä¼˜ç§€ - è¯†åˆ«å‡†ç¡®ç‡å¾ˆé«˜")
        elif result_info['keep_ratio'] > 0.5:
            print("ğŸ‘ åˆ†ç¦»è´¨é‡: è‰¯å¥½ - è¯†åˆ«å‡†ç¡®ç‡è¾ƒé«˜")
        elif result_info['keep_ratio'] > 0.3:
            print("âš ï¸ åˆ†ç¦»è´¨é‡: ä¸€èˆ¬ - å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
        else:
            print("âŒ åˆ†ç¦»è´¨é‡: è¾ƒå·® - å»ºè®®æ£€æŸ¥å‚è€ƒéŸ³é¢‘")
            
        print(f"\nğŸ’¡ å¦‚æœæ•ˆæœä¸ä½³ï¼Œå¯ä»¥å°è¯•:")
        print(f"   1. è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼ (å½“å‰: {result_info['similarity_threshold']})")
        print(f"   2. æ£€æŸ¥å‚è€ƒéŸ³é¢‘æ˜¯å¦æ¸…æ™°")
        print(f"   3. æ£€æŸ¥æ··åˆéŸ³é¢‘è´¨é‡")
    else:
        print(f"\nâŒ æ™ºèƒ½åˆ†ç¦»å¤±è´¥: {result_info.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()
