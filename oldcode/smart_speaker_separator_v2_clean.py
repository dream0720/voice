#!/usr/bin/env python3
"""
æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ V2 - ç®€åŒ–ç‰ˆ
é’ˆå¯¹ä¸»äººå£°èƒ½é‡å¼ºã€èƒŒæ™¯ç”·å£°ä½æ²‰çš„åœºæ™¯ä¼˜åŒ–
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from pathlib import Path

class SmartSpeakerSeparatorV2:
    """æ™ºèƒ½è¯´è¯äººåˆ†ç¦»å™¨ V2"""
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13
        
        # ä¼˜åŒ–å‚æ•°ï¼ˆé’ˆå¯¹å¼ºå¥³å£°å¼±ç”·å£°ï¼‰
        self.energy_threshold_ratio = 0.04   # æ›´ä½çš„èƒ½é‡é˜ˆå€¼
        self.volume_change_threshold = 0.25  # æ›´æ•æ„Ÿçš„éŸ³é‡å˜åŒ–
        self.spectral_change_threshold = 0.2 # æ›´æ•æ„Ÿçš„é¢‘è°±å˜åŒ–
        self.min_segment_duration = 0.2      # æ›´çŸ­çš„æœ€å°æ®µé•¿
        self.merge_threshold = 0.5           # æ›´å®¹æ˜“åˆå¹¶
        self.similarity_threshold = 0.25     # æ›´ä½çš„ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # æ€§åˆ«åˆ¤æ–­å‚æ•°
        self.pitch_gender_threshold = 160    # æ€§åˆ«åŸºé¢‘é˜ˆå€¼
        self.male_suppression_factor = 0.1   # ç”·å£°æŠ‘åˆ¶å¼ºåº¦
        self.female_boost_factor =  2      # å¥³å£°å¢å¼ºå¼ºåº¦
        
    def multi_level_segmentation(self, audio):
        """å¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ"""
        print("ğŸ” æ‰§è¡Œå¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ...")
        
        # åŸºäºèƒ½é‡çš„åˆ†æ®µ
        energy_segments = self.energy_based_segmentation(audio)
        print(f"   èƒ½é‡åˆ†æ®µ: {len(energy_segments)} ä¸ªæ®µ")
        
        # åŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†
        volume_segments = self.volume_change_segmentation(audio, energy_segments)
        print(f"   éŸ³é‡ç»†åˆ†: {len(volume_segments)} ä¸ªæ®µ")
        
        # éªŒè¯å’Œè¡¥å……
        final_segments = self.validate_and_supplement_segments(audio, volume_segments)
        print(f"   éªŒè¯è¡¥å……: {len(final_segments)} ä¸ªæ®µ")
        
        return final_segments
    
    def energy_based_segmentation(self, audio):
        """åŸºäºèƒ½é‡çš„åˆ†æ®µ"""
        frame_size = int(0.025 * self.sample_rate)  # 25ms
        hop_size = int(0.01 * self.sample_rate)     # 10ms
        
        energy = []
        for i in range(0, len(audio) - frame_size + 1, hop_size):
            frame = audio[i:i + frame_size]
            energy.append(np.sum(frame ** 2))
        
        energy = np.array(energy)
        
        # è‡ªé€‚åº”é˜ˆå€¼
        energy_mean = np.mean(energy)
        threshold = energy_mean * self.energy_threshold_ratio
        
        # æ‰¾åˆ°è¶…è¿‡é˜ˆå€¼çš„åŒºåŸŸ
        above_threshold = energy > threshold
        segments = []
        in_segment = False
        start_idx = 0
        
        for i, is_above in enumerate(above_threshold):
            if is_above and not in_segment:
                start_idx = i
                in_segment = True
            elif not is_above and in_segment:
                start_time = start_idx * hop_size / self.sample_rate
                end_time = i * hop_size / self.sample_rate
                
                if end_time - start_time >= self.min_segment_duration:
                    segments.append((start_time, end_time, np.mean(energy[start_idx:i])))
                
                in_segment = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µ
        if in_segment:
            start_time = start_idx * hop_size / self.sample_rate
            end_time = len(above_threshold) * hop_size / self.sample_rate
            if end_time - start_time >= self.min_segment_duration:
                segments.append((start_time, end_time, np.mean(energy[start_idx:])))
        
        return segments
    
    def volume_change_segmentation(self, audio, initial_segments):
        """åŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†"""
        refined_segments = []
        
        for start_time, end_time, confidence in initial_segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            end_sample = min(end_sample, len(audio))
            
            if end_sample <= start_sample:
                continue
                
            segment_audio = audio[start_sample:end_sample]
            
            # å¦‚æœæ®µå¤ªçŸ­ï¼Œç›´æ¥æ·»åŠ 
            if len(segment_audio) < self.frame_length * 2:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            # è®¡ç®—RMSå˜åŒ–
            frame_size = int(0.02 * self.sample_rate)
            hop_size = int(0.01 * self.sample_rate)
            
            rms_values = []
            for i in range(0, len(segment_audio) - frame_size + 1, hop_size):
                frame = segment_audio[i:i + frame_size]
                rms = np.sqrt(np.mean(frame ** 2))
                rms_values.append(rms)
            
            if len(rms_values) < 3:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            # æ£€æµ‹éŸ³é‡çªå˜
            rms_values = np.array(rms_values)
            diff = np.abs(np.diff(rms_values))
            threshold = np.std(diff) * self.volume_change_threshold
            
            split_points = []
            for i, d in enumerate(diff):
                if d > threshold:
                    split_time = start_time + (i + 1) * hop_size / self.sample_rate
                    split_points.append(split_time)
            
            # åˆ†å‰²æ®µ
            current_start = start_time
            for split_time in split_points:
                if split_time - current_start >= self.min_segment_duration:
                    refined_segments.append((current_start, split_time, confidence))
                    current_start = split_time
            
            # æ·»åŠ æœ€åä¸€æ®µ
            if end_time - current_start >= self.min_segment_duration:
                refined_segments.append((current_start, end_time, confidence))
        
        return refined_segments
    
    def validate_and_supplement_segments(self, audio, segments):
        """éªŒè¯å’Œè¡¥å……é—æ¼çš„è¯­éŸ³æ®µ"""
        # åˆ›å»ºè¦†ç›–æ©ç 
        total_samples = len(audio)
        coverage_mask = np.zeros(total_samples, dtype=bool)
        
        for start_time, end_time, _ in segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            end_sample = min(end_sample, total_samples)
            coverage_mask[start_sample:end_sample] = True
        
        # æ‰¾åˆ°æœªè¦†ç›–çš„åŒºåŸŸ
        uncovered_regions = []
        in_uncovered = False
        start_idx = 0
        
        for i, covered in enumerate(coverage_mask):
            if not covered and not in_uncovered:
                start_idx = i
                in_uncovered = True
            elif covered and in_uncovered:
                start_time = start_idx / self.sample_rate
                end_time = i / self.sample_rate
                
                if end_time - start_time >= 0.1:
                    uncovered_regions.append((start_time, end_time))
                
                in_uncovered = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæœªè¦†ç›–åŒºåŸŸ
        if in_uncovered:
            start_time = start_idx / self.sample_rate
            end_time = total_samples / self.sample_rate
            if end_time - start_time >= 0.1:
                uncovered_regions.append((start_time, end_time))
        
        # æ£€æŸ¥æœªè¦†ç›–åŒºåŸŸå¹¶è¡¥å……
        supplemented_segments = list(segments)
        
        for start_time, end_time in uncovered_regions:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            region_audio = audio[start_sample:end_sample]
            
            rms = np.sqrt(np.mean(region_audio ** 2))
            if rms > 0.01:
                supplemented_segments.append((start_time, end_time, rms))
        
        # æŒ‰æ—¶é—´æ’åº
        supplemented_segments.sort(key=lambda x: x[0])
        return supplemented_segments
    
    def extract_comprehensive_features(self, audio):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            if len(audio) < self.frame_length:
                return None
            
            features = {}
            
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=self.n_mfcc)
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            
            # åŸºé¢‘ç‰¹å¾
            f0 = librosa.yin(audio, fmin=60, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) > 0:
                features['f0_mean'] = np.mean(f0_clean)
                features['f0_std'] = np.std(f0_clean)
                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)
            else:
                features['f0_mean'] = 0
                features['f0_std'] = 0
                features['f0_range'] = 0
            
            # é¢‘è°±ç‰¹å¾
            stft = librosa.stft(audio, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            
            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(S=magnitude))
            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(S=magnitude))
            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(S=magnitude))
            
            # è‰²åº¦ç‰¹å¾
            chroma = librosa.feature.chroma_stft(S=magnitude, sr=self.sample_rate)
            features['chroma_mean'] = np.mean(chroma, axis=1)
            
            # å…¶ä»–ç‰¹å¾
            features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(audio))
            features['rms'] = np.sqrt(np.mean(audio ** 2))
            
            return features
            
        except Exception as e:
            return None
    
    def analyze_gender_and_energy(self, segment_audio, reference_audio):
        """åˆ†ææ€§åˆ«ç‰¹å¾å’Œèƒ½é‡å ä¼˜"""
        try:
            # åŸºé¢‘åˆ†æ
            f0 = librosa.yin(segment_audio, fmin=50, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) < 3:
                return {'is_female': False, 'energy_boost': 1.0, 'confidence': 0.0}
            
            f0_mean = np.mean(f0_clean)
            is_female = f0_mean > self.pitch_gender_threshold
            
            # èƒ½é‡åˆ†æ
            segment_rms = np.sqrt(np.mean(segment_audio ** 2))
            ref_rms = np.sqrt(np.mean(reference_audio ** 2))
            energy_ratio = segment_rms / ref_rms if ref_rms > 0 else 0
            
            # é¢‘è°±åˆ†æ
            stft = librosa.stft(segment_audio, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # ä½é¢‘å æ¯”ï¼ˆç”·å£°ç‰¹å¾ï¼‰
            low_freq_mask = freq_bins < 250
            low_freq_energy = np.sum(magnitude[low_freq_mask, :])
            total_energy = np.sum(magnitude)
            low_freq_ratio = low_freq_energy / total_energy if total_energy > 0 else 0
            
            # å†³ç­–é€»è¾‘
            confidence = 0.0
            energy_boost = 1.0
            
            if is_female:
                confidence += 0.3
                energy_boost = self.female_boost_factor
            
            if low_freq_ratio < 0.25:  # ä½é¢‘å æ¯”ä½
                confidence += 0.2
            
            if energy_ratio > 0.5:  # èƒ½é‡å ä¼˜
                confidence += 0.3
            
            # å¦‚æœåˆ¤æ–­ä¸ºç”·å£°ï¼Œåº”ç”¨æŠ‘åˆ¶
            if not is_female and low_freq_ratio > 0.35:
                energy_boost = self.male_suppression_factor
                confidence = max(0, confidence - 0.4)
            
            return {
                'is_female': is_female,
                'energy_boost': energy_boost,
                'confidence': confidence,
                'f0_mean': f0_mean,
                'low_freq_ratio': low_freq_ratio,
                'energy_ratio': energy_ratio
            }
            
        except Exception as e:
            return {'is_female': False, 'energy_boost': 1.0, 'confidence': 0.0}
    
    def advanced_speaker_similarity(self, features1, features2):
        """é«˜çº§è¯´è¯äººç›¸ä¼¼åº¦è®¡ç®—"""
        if features1 is None or features2 is None:
            return 0.0
        
        try:
            similarities = []
            weights = []
            
            # MFCCç›¸ä¼¼åº¦
            mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
            if not np.isnan(mfcc_sim):
                similarities.append(max(0, mfcc_sim))
                weights.append(0.4)
            
            # åŸºé¢‘ç›¸ä¼¼åº¦
            if features1['f0_mean'] > 0 and features2['f0_mean'] > 0:
                f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
                f0_sim = 1 / (1 + f0_diff / 30)
                similarities.append(f0_sim)
                weights.append(0.3)
            
            # é¢‘è°±ç‰¹å¾ç›¸ä¼¼åº¦
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 500)
            similarities.append(centroid_sim)
            weights.append(0.2)
            
            # è‰²åº¦ç›¸ä¼¼åº¦
            chroma_sim = 1 - cosine(features1['chroma_mean'], features2['chroma_mean'])
            if not np.isnan(chroma_sim):
                similarities.append(max(0, chroma_sim))
                weights.append(0.1)
            
            # åŠ æƒå¹³å‡
            if similarities and weights:
                weights = np.array(weights[:len(similarities)])
                weights = weights / np.sum(weights)
                return np.average(similarities, weights=weights)
            else:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def intelligent_separate(self, mixed_audio_path, reference_audio_path, output_path=None):
        """æ™ºèƒ½è¯´è¯äººåˆ†ç¦»ä¸»å‡½æ•°"""
        try:
            print("ğŸ¯ æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ V2")
            print("=" * 80)
            
            # åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            mixed_audio, _ = librosa.load(mixed_audio_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_audio_path, sr=self.sample_rate)
            
            print(f"   æ··åˆéŸ³é¢‘: {len(mixed_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("ğŸ” åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            ref_features = self.extract_comprehensive_features(reference_audio)
            
            if ref_features is None:
                print("âŒ å‚è€ƒéŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return False, {"error": "Reference feature extraction failed"}
            
            print(f"   å‚è€ƒåŸºé¢‘: {ref_features['f0_mean']:.1f} Hz")
            print(f"   å‚è€ƒé¢‘è°±è´¨å¿ƒ: {ref_features['spectral_centroid']:.1f} Hz")
            
            # æ™ºèƒ½åˆ†æ®µ
            segments = self.multi_level_segmentation(mixed_audio)
            
            if not segments:
                print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ®µ")
                return False, {"error": "No valid segments detected"}
            
            # åˆå§‹åŒ–è¾“å‡º
            output_audio = np.zeros_like(mixed_audio)
            
            # é€æ®µåˆ†æ
            print(f"\nğŸ”„ æ™ºèƒ½è¯´è¯äººè¯†åˆ« - åˆ†æ {len(segments)} ä¸ªè¯­éŸ³æ®µ...")
            print("-" * 85)
            print("æ®µå· æ€§åˆ« æ—¶é—´èŒƒå›´        æ—¶é•¿   åŸºé¢‘   ç›¸ä¼¼åº¦ èƒ½é‡å¢ç›Š ç»¼åˆåˆ†   å†³ç­–   è¯´æ˜")
            print("-" * 85)
            
            kept_segments = 0
            total_kept_duration = 0
            female_segments = 0
            male_segments = 0
            
            for i, (start_time, end_time, confidence) in enumerate(segments):
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                
                start_sample = max(0, start_sample)
                end_sample = min(len(mixed_audio), end_sample)
                
                if end_sample <= start_sample:
                    continue
                
                segment_audio = mixed_audio[start_sample:end_sample]
                segment_duration = end_time - start_time
                
                # æå–æ®µç‰¹å¾
                segment_features = self.extract_comprehensive_features(segment_audio)
                
                if segment_features is None:
                    print(f"{i+1:2d}  â“  {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s   -    -      -      -      âŒ     ç‰¹å¾æå–å¤±è´¥")
                    continue
                
                # æ€§åˆ«å’Œèƒ½é‡åˆ†æ
                gender_energy_info = self.analyze_gender_and_energy(segment_audio, reference_audio)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.advanced_speaker_similarity(ref_features, segment_features)
                
                # ç»¼åˆå†³ç­–åˆ†æ•°
                decision_score = similarity + gender_energy_info['confidence']
                
                # ç»Ÿè®¡æ€§åˆ«
                if gender_energy_info['is_female']:
                    female_segments += 1
                    gender_mark = "â™€"
                else:
                    male_segments += 1
                    gender_mark = "â™‚"
                
                # å†³ç­–
                if decision_score >= self.similarity_threshold:
                    # åº”ç”¨èƒ½é‡è°ƒæ•´
                    processed_segment = segment_audio * gender_energy_info['energy_boost']
                    output_audio[start_sample:end_sample] = processed_segment
                    kept_segments += 1
                    total_kept_duration += segment_duration
                    decision = "âœ… ä¿ç•™"
                    reason = "ç›®æ ‡è¯´è¯äºº"
                    
                elif decision_score >= self.similarity_threshold * 0.6:
                    # éƒ¨åˆ†ä¿ç•™
                    processed_segment = segment_audio * gender_energy_info['energy_boost'] * 0.5
                    output_audio[start_sample:end_sample] = processed_segment
                    kept_segments += 0.5
                    total_kept_duration += segment_duration * 0.5
                    decision = "ğŸ”¶ éƒ¨åˆ†"
                    reason = "å¯èƒ½ç›®æ ‡"
                    
                else:
                    # ä¸¢å¼ƒ
                    decision = "âŒ ä¸¢å¼ƒ"
                    reason = "éç›®æ ‡è¯´è¯äºº"
                
                # æ˜¾ç¤ºç»“æœ
                print(f"{i+1:2d}  {gender_mark}  {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s  {segment_features['f0_mean']:5.1f}  {similarity:6.3f}   {gender_energy_info['energy_boost']:4.1f}   {decision_score:6.3f}  {decision}  {reason}")
            
            print("-" * 85)
            print(f"ğŸ“Š æ™ºèƒ½åˆ†æå®Œæˆ:")
            print(f"   æ€»æ®µæ•°: {len(segments)}")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(segments)*100:.1f}%")
            print(f"   ä¿ç•™æ—¶é•¿: {total_kept_duration:.2f}s / {len(mixed_audio)/self.sample_rate:.2f}s")
            print(f"   æ£€æµ‹å¥³å£°æ®µ: {female_segments}")
            print(f"   æ£€æµ‹ç”·å£°æ®µ: {male_segments}")
            
            # åå¤„ç†
            output_audio = self.post_process_audio(output_audio)
            
            # ä¿å­˜ç»“æœ
            if output_path:
                sf.write(output_path, output_audio, self.sample_rate)
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
            
            # è¯„ä¼°è´¨é‡
            if kept_segments / len(segments) > 0.8:
                quality = "ä¼˜ç§€"
            elif kept_segments / len(segments) > 0.6:
                quality = "è‰¯å¥½"
            elif kept_segments / len(segments) > 0.4:
                quality = "ä¸€èˆ¬"
            else:
                quality = "éœ€è¦è°ƒä¼˜"
            
            print(f"\nğŸ‰ æ™ºèƒ½è¯´è¯äººåˆ†ç¦»V2æˆåŠŸ!")
            print(f"ğŸ† åˆ†ç¦»è´¨é‡: {quality}")
            print(f"ğŸ’¡ ä¼˜åŒ–ç‰¹ç‚¹:")
            print(f"   - é’ˆå¯¹å¼ºå¥³å£°å¼±ç”·å£°åœºæ™¯ä¼˜åŒ–")
            print(f"   - åŸºäºæ€§åˆ«ç‰¹å¾çš„æ™ºèƒ½å¢å¼º/æŠ‘åˆ¶")
            print(f"   - æ›´æ•æ„Ÿçš„åˆ†æ®µç®—æ³•")
            print(f"   - ç»¼åˆèƒ½é‡å’Œé¢‘è°±ç‰¹å¾åˆ¤æ–­")
            
            result_info = {
                'success': True,
                'total_segments': len(segments),
                'kept_segments': kept_segments,
                'keep_ratio': kept_segments / len(segments),
                'female_segments': female_segments,
                'male_segments': male_segments,
                'quality': quality
            }
            
            return True, result_info
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½åˆ†ç¦»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}
    
    def post_process_audio(self, audio):
        """éŸ³é¢‘åå¤„ç†"""
        if len(audio) == 0:
            return audio
        
        # å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # å™ªå£°é—¨é™
        rms = np.sqrt(np.mean(audio ** 2))
        threshold = rms * 0.03
        mask = np.abs(audio) < threshold
        audio[mask] = 0
        
        # è½»å¾®å¹³æ»‘
        if len(audio) > 10:
            audio = signal.savgol_filter(audio, window_length=min(5, len(audio)//2*2-1), polyorder=2)
        
        # å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåˆ†ç¦»å™¨
        separator = SmartSpeakerSeparatorV2()
        
        # è®¾ç½®è·¯å¾„
        mixed_audio_path = "temp/demucs_output/htdemucs/lttgd/vocals.wav"
        reference_audio_path = "reference/lttgd_ref.wav"
        output_path = "output/lltgd.wav"
        
        # æ‰§è¡Œåˆ†ç¦»
        success, result = separator.intelligent_separate(
            mixed_audio_path, 
            reference_audio_path, 
            output_path
        )
        
        if success:
            print("\nâœ… åˆ†ç¦»æˆåŠŸå®Œæˆ!")
        else:
            print(f"\nâŒ åˆ†ç¦»å¤±è´¥: {result.get('error', 'Unknown error')}")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
