#!/usr/bin/env python3
"""
é‡æ–°ç¼–å†™çš„ä¸»äººå£°æå–ç®—æ³•
åŸºäºå‚è€ƒéŸ³é¢‘è¿›è¡Œç”·å¥³å£°åˆ†ç¦»
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class GenderBasedSpeakerExtractor:
    """
    åŸºäºæ€§åˆ«ç‰¹å¾çš„è¯´è¯äººæå–å™¨
    """
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13
        
        # åˆ†æå‚æ•°
        self.window_duration = 2.0  # åˆ†æçª—å£é•¿åº¦(ç§’)
        self.hop_duration = 1.0     # çª—å£ç§»åŠ¨æ­¥é•¿(ç§’)
        
    def analyze_gender_features(self, audio):
        """
        åˆ†æéŸ³é¢‘çš„æ€§åˆ«ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            features: æ€§åˆ«ç‰¹å¾å­—å…¸
        """
        if len(audio) < self.sample_rate * 0.5:  # éŸ³é¢‘å¤ªçŸ­
            return None
        
        try:
            # åŸºé¢‘åˆ†æ
            f0 = librosa.yin(audio, fmin=50, fmax=400, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) == 0:
                return None
            
            f0_mean = np.mean(f0_clean)
            f0_median = np.median(f0_clean)
            f0_std = np.std(f0_clean)
            
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(
                y=audio, sr=self.sample_rate, n_mfcc=self.n_mfcc,
                n_fft=self.frame_length, hop_length=self.hop_length
            )
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate))
            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate))
            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate))
            
            # èƒ½é‡åˆ†å¸ƒåˆ†æ
            stft = librosa.stft(audio, n_fft=self.frame_length, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            freqs = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å®šä¹‰é¢‘ç‡å¸¦ï¼ˆæ ¹æ®æ‚¨çš„æè¿°è°ƒæ•´ï¼‰
            low_freq_mask = (freqs >= 80) & (freqs <= 150)    # ä½é¢‘å¸¦
            mid_freq_mask = (freqs >= 150) & (freqs <= 250)   # ä¸­é¢‘å¸¦  
            high_freq_mask = (freqs >= 250) & (freqs <= 400)  # é«˜é¢‘å¸¦
            
            low_energy = np.mean(magnitude[low_freq_mask, :]) if np.any(low_freq_mask) else 0
            mid_energy = np.mean(magnitude[mid_freq_mask, :]) if np.any(mid_freq_mask) else 0
            high_energy = np.mean(magnitude[high_freq_mask, :]) if np.any(high_freq_mask) else 0
            
            total_energy = low_energy + mid_energy + high_energy
            if total_energy > 0:
                low_ratio = low_energy / total_energy
                mid_ratio = mid_energy / total_energy
                high_ratio = high_energy / total_energy
            else:
                low_ratio = mid_ratio = high_ratio = 0
            
            return {
                'f0_mean': f0_mean,
                'f0_median': f0_median,
                'f0_std': f0_std,
                'mfcc_mean': mfcc_mean,
                'mfcc_std': mfcc_std,
                'spectral_centroid': spectral_centroid,
                'spectral_bandwidth': spectral_bandwidth,
                'spectral_rolloff': spectral_rolloff,
                'low_energy_ratio': low_ratio,
                'mid_energy_ratio': mid_ratio,
                'high_energy_ratio': high_ratio,
                'rms': np.sqrt(np.mean(audio**2))
            }
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def calculate_similarity(self, features1, features2):
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦
        """
        if features1 is None or features2 is None:
            return 0.0
        
        try:
            # MFCCç›¸ä¼¼åº¦
            mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
            mfcc_sim = max(0, mfcc_sim)
            
            # åŸºé¢‘ç›¸ä¼¼åº¦
            f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
            f0_sim = 1 / (1 + f0_diff / 20)  # åŸºé¢‘ç›¸ä¼¼åº¦
            
            # é¢‘è°±è´¨å¿ƒç›¸ä¼¼åº¦
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 500)
            
            # èƒ½é‡åˆ†å¸ƒç›¸ä¼¼åº¦
            energy_diff = abs(features1['mid_energy_ratio'] - features2['mid_energy_ratio'])
            energy_sim = 1 / (1 + energy_diff * 10)
            
            # åŠ æƒç»„åˆ
            similarity = (
                0.4 * mfcc_sim + 
                0.3 * f0_sim + 
                0.2 * centroid_sim + 
                0.1 * energy_sim
            )
            
            return max(0, min(1, similarity))
            
        except Exception as e:
            print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def apply_gender_filter(self, audio, target_features, strength=0.8):
        """
        åº”ç”¨åŸºäºæ€§åˆ«ç‰¹å¾çš„æ»¤æ³¢å™¨
        """
        if target_features is None:
            return audio
        
        try:
            # åŸºäºç›®æ ‡ç‰¹å¾ç¡®å®šæ»¤æ³¢å‚æ•°
            target_f0 = target_features['f0_mean']
            
            # è®¾è®¡å¸¦é€šæ»¤æ³¢å™¨
            if target_f0 > 200:  # å¥³å£°ï¼ˆæ ¹æ®æ‚¨çš„æè¿°ï¼Œè¿™é‡Œæ˜¯è¾ƒé«˜çš„å¥³å£°ï¼‰
                # ä¿ç•™å¥³å£°é¢‘ç‡èŒƒå›´ï¼ŒæŠ‘åˆ¶ç”·å£°
                low_cutoff = 150
                high_cutoff = 350
                suppress_low = 80
                suppress_high = 180
            else:  # ç”·å£°ï¼ˆæ ¹æ®æ‚¨çš„æè¿°ï¼Œè¿™é‡Œæ˜¯è¾ƒé«˜çš„ç”·å£°ï¼‰
                # ä¿ç•™ç”·å£°é¢‘ç‡èŒƒå›´ï¼ŒæŠ‘åˆ¶å¥³å£°
                low_cutoff = 100
                high_cutoff = 200
                suppress_low = 200
                suppress_high = 300
            
            # é¢‘åŸŸæ»¤æ³¢
            stft = librosa.stft(audio, n_fft=self.frame_length, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            freqs = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # åˆ›å»ºæ»¤æ³¢å™¨æ©ç 
            filter_mask = np.ones_like(magnitude)
            
            # æŠ‘åˆ¶ä¸éœ€è¦çš„é¢‘ç‡èŒƒå›´
            suppress_mask = (freqs >= suppress_low) & (freqs <= suppress_high)
            filter_mask[suppress_mask, :] *= (1 - strength)
            
            # å¢å¼ºç›®æ ‡é¢‘ç‡èŒƒå›´
            enhance_mask = (freqs >= low_cutoff) & (freqs <= high_cutoff)
            filter_mask[enhance_mask, :] *= (1 + strength * 0.3)
            
            # åº”ç”¨æ»¤æ³¢å™¨
            filtered_magnitude = magnitude * filter_mask
            
            # é‡æ„éŸ³é¢‘
            filtered_stft = filtered_magnitude * np.exp(1j * phase)
            filtered_audio = librosa.istft(filtered_stft, hop_length=self.hop_length)
            
            return filtered_audio
            
        except Exception as e:
            print(f"æ€§åˆ«æ»¤æ³¢å¤±è´¥: {e}")
            return audio
    
    def segment_audio_by_similarity(self, target_audio, reference_features, threshold=0.3):
        """
        æ ¹æ®ç›¸ä¼¼åº¦å¯¹éŸ³é¢‘è¿›è¡Œåˆ†æ®µ
        """
        print(f"ğŸ”„ æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ®µéŸ³é¢‘...")
        
        # è®¡ç®—åˆ†æ®µå‚æ•°
        window_samples = int(self.window_duration * self.sample_rate)
        hop_samples = int(self.hop_duration * self.sample_rate)
        
        segments = []
        confidences = []
        
        # æ»‘åŠ¨çª—å£åˆ†æ
        for start_sample in range(0, len(target_audio) - window_samples + 1, hop_samples):
            end_sample = start_sample + window_samples
            window_audio = target_audio[start_sample:end_sample]
            
            # æå–çª—å£ç‰¹å¾
            window_features = self.analyze_gender_features(window_audio)
            
            if window_features is not None:
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(reference_features, window_features)
                
                # è®°å½•æ®µä¿¡æ¯
                start_time = start_sample / self.sample_rate
                end_time = end_sample / self.sample_rate
                
                segments.append({
                    'start_time': start_time,
                    'end_time': end_time,
                    'start_sample': start_sample,
                    'end_sample': end_sample,
                    'similarity': similarity,
                    'audio': window_audio,
                    'features': window_features
                })
                confidences.append(similarity)
        
        print(f"   åˆ†æäº† {len(segments)} ä¸ªéŸ³é¢‘æ®µ")
        print(f"   å¹³å‡ç›¸ä¼¼åº¦: {np.mean(confidences):.3f}")
        
        # ç­›é€‰é«˜ç›¸ä¼¼åº¦æ®µ
        target_segments = [seg for seg in segments if seg['similarity'] >= threshold]
        print(f"   æ‰¾åˆ° {len(target_segments)} ä¸ªç›®æ ‡æ®µ (é˜ˆå€¼: {threshold})")
        
        return segments, target_segments
    
    def reconstruct_timeline_audio(self, target_audio, segments, target_segments):
        """
        é‡æ„æ—¶é—´è½´éŸ³é¢‘ï¼Œä¿æŒåŸå§‹æ—¶é•¿
        """
        print(f"ğŸ”„ é‡æ„æ—¶é—´è½´éŸ³é¢‘...")
        
        # åˆå§‹åŒ–è¾“å‡ºéŸ³é¢‘
        output_audio = np.zeros_like(target_audio)
        
        # ä¸ºæ¯ä¸ªæ—¶é—´æ®µåˆ†é…æƒé‡
        for segment in segments:
            start_idx = segment['start_sample']
            end_idx = min(segment['end_sample'], len(target_audio))
            similarity = segment['similarity']
            
            # è·å–åŸå§‹éŸ³é¢‘æ®µ
            original_segment = target_audio[start_idx:end_idx]
            
            if similarity > 0.5:
                # é«˜ç›¸ä¼¼åº¦ï¼šä¿ç•™å¹¶å¢å¼º
                processed_segment = self.apply_gender_filter(
                    original_segment, segment['features'], strength=0.8
                )
                weight = 1.0
            elif similarity > 0.3:
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šä¿ç•™ä½†è¡°å‡
                processed_segment = original_segment * 0.7
                weight = 0.7
            elif similarity > 0.1:
                # ä½ç›¸ä¼¼åº¦ï¼šå¤§å¹…è¡°å‡
                processed_segment = original_segment * 0.2
                weight = 0.2
            else:
                # æä½ç›¸ä¼¼åº¦ï¼šåŸºæœ¬é™éŸ³
                processed_segment = original_segment * 0.05
                weight = 0.05
              # åº”ç”¨åˆ°è¾“å‡ºéŸ³é¢‘
            if end_idx > start_idx:
                # ç¡®ä¿é•¿åº¦åŒ¹é…
                target_len = end_idx - start_idx
                segment_len = len(processed_segment)
                
                if segment_len >= target_len:
                    # æ®µæ¯”ç›®æ ‡é•¿ï¼Œæˆªå–
                    output_audio[start_idx:end_idx] = processed_segment[:target_len] * weight
                else:
                    # æ®µæ¯”ç›®æ ‡çŸ­ï¼Œå¡«å……
                    output_audio[start_idx:start_idx + segment_len] = processed_segment * weight
        
        # åå¤„ç†
        output_audio = self.post_process_audio(output_audio)
        
        print(f"âœ… é‡æ„å®Œæˆï¼Œæ—¶é•¿: {len(output_audio)/self.sample_rate:.2f}ç§’")
        
        return output_audio
    
    def post_process_audio(self, audio):
        """éŸ³é¢‘åå¤„ç†"""
        if len(audio) == 0:
            return audio
        
        # å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # è½»å¾®çš„åŠ¨æ€èŒƒå›´å‹ç¼©
        threshold = 0.7
        ratio = 3.0
        abs_audio = np.abs(audio)
        mask = abs_audio > threshold
        
        if np.any(mask):
            compression_factor = threshold + (abs_audio[mask] - threshold) / ratio
            audio[mask] = np.sign(audio[mask]) * compression_factor
        
        # å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio
    
    def extract_target_speaker(self, vocals_path, reference_path, output_path):
        """
        ä¸»è¦çš„è¯´è¯äººæå–æµç¨‹
        """
        print("ğŸ¯ å¼€å§‹åŸºäºæ€§åˆ«ç‰¹å¾çš„è¯´è¯äººæå–")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            target_audio, _ = librosa.load(vocals_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_path, sr=self.sample_rate)
            
            print(f"   ç›®æ ‡éŸ³é¢‘: {len(target_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # 2. æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("ğŸ” åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            reference_features = self.analyze_gender_features(reference_audio)
            
            if reference_features is None:
                print("âŒ æ— æ³•æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾")
                return False
            
            print(f"   å‚è€ƒåŸºé¢‘: {reference_features['f0_mean']:.1f} Hz")
            print(f"   é¢‘è°±è´¨å¿ƒ: {reference_features['spectral_centroid']:.1f} Hz")
            
            # åˆ¤æ–­æ€§åˆ«ç±»å‹
            if reference_features['f0_mean'] > 180:
                gender_type = "å¥³å£°"
                print(f"   è¯†åˆ«ä¸º: {gender_type} (åŸºé¢‘è¾ƒé«˜)")
            else:
                gender_type = "ç”·å£°"
                print(f"   è¯†åˆ«ä¸º: {gender_type} (åŸºé¢‘è¾ƒä½)")
            
            # 3. éŸ³é¢‘åˆ†æ®µåˆ†æ
            all_segments, target_segments = self.segment_audio_by_similarity(
                target_audio, reference_features, threshold=0.3
            )
            
            if not target_segments:
                print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„éŸ³é¢‘æ®µ")
                return False
            
            # 4. é‡æ„éŸ³é¢‘
            result_audio = self.reconstruct_timeline_audio(
                target_audio, all_segments, target_segments
            )
            
            # 5. ä¿å­˜ç»“æœ
            sf.write(output_path, result_audio, self.sample_rate)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            print(f"\nğŸ“Š æå–ç»“æœ:")
            print(f"   åŸå§‹æ—¶é•¿: {len(target_audio)/self.sample_rate:.2f}ç§’")
            print(f"   è¾“å‡ºæ—¶é•¿: {len(result_audio)/self.sample_rate:.2f}ç§’")
            print(f"   æ—¶é•¿ä¿æŒ: {'âœ…' if abs(len(result_audio)/len(target_audio) - 1) < 0.01 else 'âš ï¸'}")
            print(f"   åŒ¹é…æ®µæ•°: {len(target_segments)}")
            print(f"   å¹³å‡ç›¸ä¼¼åº¦: {np.mean([seg['similarity'] for seg in target_segments]):.3f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    extractor = GenderBasedSpeakerExtractor(sample_rate=22050)
    
    # æ–‡ä»¶è·¯å¾„
    vocals_path = "temp/demucs_output/htdemucs/answer/vocals.wav"
    reference_path = "reference/refne2.wav"
    output_path = "output/gender_separated_speaker.wav"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(vocals_path).exists():
        print(f"âŒ äººå£°æ–‡ä»¶ä¸å­˜åœ¨: {vocals_path}")
        return
    
    if not Path(reference_path).exists():
        print(f"âŒ å‚è€ƒæ–‡ä»¶ä¸å­˜åœ¨: {reference_path}")
        return
    
    # æ‰§è¡Œæå–
    success = extractor.extract_target_speaker(vocals_path, reference_path, output_path)
    
    if success:
        print("\nğŸ‰ è¯´è¯äººæå–æˆåŠŸ!")
    else:
        print("\nâŒ è¯´è¯äººæå–å¤±è´¥!")

if __name__ == "__main__":
    main()
