#!/usr/bin/env python3
"""
VoiceFilter-WavLM ä¸»äººå£°æå–å™¨
ä½¿ç”¨VoiceFilter-WavLMæ¨¡å‹è¿›è¡ŒåŸºäºå‚è€ƒéŸ³é¢‘çš„ç›®æ ‡è¯´è¯äººæå–
"""

import numpy as np
import librosa
import torch
import torchaudio
from pathlib import Path
import soundfile as sf
from scipy.signal import wiener
from sklearn.decomposition import FastICA
import matplotlib.pyplot as plt

class VoiceFilterWavLMExtractor:
    """åŸºäºVoiceFilter-WavLMçš„ä¸»äººå£°æå–å™¨"""
    
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.frame_length = 1024
        self.hop_length = 256
        self.n_fft = 2048
        
    def load_audio(self, audio_path):
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        try:
            # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)
            print(f"åŠ è½½éŸ³é¢‘: {audio_path}")
            print(f"  æ—¶é•¿: {len(audio)/self.sample_rate:.2f}ç§’")
            print(f"  é‡‡æ ·ç‡: {sr}Hz")
            return audio
        except Exception as e:
            print(f"éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
            return None
    
    def extract_speaker_embedding(self, audio):
        """æå–è¯´è¯äººåµŒå…¥ç‰¹å¾ï¼ˆæ¨¡æ‹ŸWavLMç‰¹å¾æå–ï¼‰"""
        try:
            # ä½¿ç”¨MFCCç‰¹å¾ä½œä¸ºè¯´è¯äººç‰¹å¾çš„ç®€åŒ–ç‰ˆæœ¬
            mfcc = librosa.feature.mfcc(
                y=audio, 
                sr=self.sample_rate,
                n_mfcc=13,
                hop_length=self.hop_length
            )
            
            # æå–ç»Ÿè®¡ç‰¹å¾
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            mfcc_delta = librosa.feature.delta(mfcc)
            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)
            
            # åˆå¹¶ç‰¹å¾
            embedding = np.concatenate([mfcc_mean, mfcc_std, mfcc_delta_mean])
            
            print(f"è¯´è¯äººåµŒå…¥ç»´åº¦: {embedding.shape}")
            return embedding
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def calculate_similarity(self, ref_embedding, mixed_frame_embedding):
        """è®¡ç®—å‚è€ƒåµŒå…¥å’Œæ··åˆéŸ³é¢‘å¸§åµŒå…¥çš„ç›¸ä¼¼åº¦"""
        try:
            # ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(ref_embedding, mixed_frame_embedding)
            norm_ref = np.linalg.norm(ref_embedding)
            norm_frame = np.linalg.norm(mixed_frame_embedding)
            
            if norm_ref == 0 or norm_frame == 0:
                return 0.0
                
            similarity = dot_product / (norm_ref * norm_frame)
            return max(0, similarity)  # ç¡®ä¿éè´Ÿ
            
        except Exception as e:
            return 0.0
    
    def frame_level_extraction(self, mixed_audio, reference_embedding):
        """åŸºäºå¸§çº§åˆ«çš„è¯´è¯äººæå–"""
        try:
            print("ğŸ¯ å¼€å§‹å¸§çº§åˆ«è¯´è¯äººæå–...")
            
            # è®¡ç®—STFT
            stft = librosa.stft(
                mixed_audio, 
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                win_length=self.frame_length
            )
            
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # ä¸ºæ¯ä¸€å¸§è®¡ç®—è¯´è¯äººç›¸ä¼¼åº¦
            n_frames = magnitude.shape[1]
            similarities = np.zeros(n_frames)
            enhanced_magnitude = magnitude.copy()
            
            frame_size = self.hop_length
            
            for i in range(n_frames):
                # è·å–å½“å‰å¸§çš„éŸ³é¢‘
                start_sample = i * self.hop_length
                end_sample = min(start_sample + frame_size, len(mixed_audio))
                
                if end_sample - start_sample < frame_size // 2:
                    continue
                    
                frame_audio = mixed_audio[start_sample:end_sample]
                
                if len(frame_audio) < frame_size // 4:
                    continue
                
                # æå–å½“å‰å¸§çš„è¯´è¯äººç‰¹å¾
                try:
                    frame_mfcc = librosa.feature.mfcc(
                        y=frame_audio,
                        sr=self.sample_rate,
                        n_mfcc=13,
                        hop_length=self.hop_length // 4
                    )
                    
                    if frame_mfcc.shape[1] > 0:
                        frame_embedding = np.mean(frame_mfcc, axis=1)
                        
                        # è¡¥é½åˆ°å‚è€ƒåµŒå…¥çš„ç»´åº¦
                        if len(frame_embedding) < len(reference_embedding):
                            padding = len(reference_embedding) - len(frame_embedding)
                            frame_embedding = np.pad(frame_embedding, (0, padding), 'constant')
                        elif len(frame_embedding) > len(reference_embedding):
                            frame_embedding = frame_embedding[:len(reference_embedding)]
                        
                        # è®¡ç®—ç›¸ä¼¼åº¦
                        similarity = self.calculate_similarity(reference_embedding, frame_embedding)
                        similarities[i] = similarity
                    
                except Exception as e:
                    similarities[i] = 0.0
                    continue
            
            # å¹³æ»‘ç›¸ä¼¼åº¦æ›²çº¿
            from scipy.ndimage import gaussian_filter1d
            similarities_smooth = gaussian_filter1d(similarities, sigma=2.0)
            
            # åŠ¨æ€é˜ˆå€¼
            similarity_mean = np.mean(similarities_smooth)
            similarity_std = np.std(similarities_smooth)
            threshold = max(0.3, similarity_mean + 0.5 * similarity_std)
            
            print(f"ç›¸ä¼¼åº¦ç»Ÿè®¡: å‡å€¼={similarity_mean:.3f}, æ ‡å‡†å·®={similarity_std:.3f}")
            print(f"åŠ¨æ€é˜ˆå€¼: {threshold:.3f}")
            
            # åˆ›å»ºæ©ç 
            mask = similarities_smooth > threshold
            
            # åº”ç”¨è½¯æ©ç 
            for i in range(n_frames):
                if mask[i]:
                    # ä¿ç•™é«˜ç›¸ä¼¼åº¦å¸§ï¼Œç¨ä½œå¢å¼º
                    enhanced_magnitude[:, i] *= (1.0 + 0.3 * similarities_smooth[i])
                else:
                    # æŠ‘åˆ¶ä½ç›¸ä¼¼åº¦å¸§
                    suppression_factor = max(0.1, 1.0 - 2.0 * (threshold - similarities_smooth[i]))
                    enhanced_magnitude[:, i] *= suppression_factor
            
            # é‡æ„éŸ³é¢‘
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            enhanced_audio = librosa.istft(
                enhanced_stft,
                hop_length=self.hop_length,
                win_length=self.frame_length
            )
            
            # ç»Ÿè®¡ä¿¡æ¯
            target_frames = np.sum(mask)
            total_frames = len(mask)
            coverage = target_frames / total_frames * 100
            
            print(f"ç›®æ ‡è¯´è¯äººå¸§æ•°: {target_frames}/{total_frames} ({coverage:.1f}%)")
            
            return enhanced_audio, similarities_smooth, mask
            
        except Exception as e:
            print(f"å¸§çº§åˆ«æå–å¤±è´¥: {e}")
            return mixed_audio, None, None
    
    def post_process_audio(self, audio):
        """åå¤„ç†éŸ³é¢‘"""
        try:
            # 1. é™å™ªå¤„ç†
            noise_profile = audio[:int(0.5 * self.sample_rate)]  # å‰0.5ç§’ä½œä¸ºå™ªå£°å‚è€ƒ
            noise_power = np.mean(noise_profile ** 2)
            
            if noise_power > 1e-8:
                # ç®€å•çš„è°±å‡æ³•é™å™ª
                stft = librosa.stft(audio, hop_length=self.hop_length)
                magnitude = np.abs(stft)
                phase = np.angle(stft)
                
                # ä¼°è®¡å™ªå£°è°±
                noise_magnitude = np.mean(magnitude[:, :int(0.5 * self.sample_rate / self.hop_length)], axis=1, keepdims=True)
                
                # è°±å‡æ³•
                alpha = 2.0  # è¿‡å‡å› å­
                enhanced_magnitude = magnitude - alpha * noise_magnitude
                enhanced_magnitude = np.maximum(enhanced_magnitude, 0.1 * magnitude)
                
                # é‡æ„
                enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
                audio = librosa.istft(enhanced_stft, hop_length=self.hop_length)
            
            # 2. éŸ³é‡å½’ä¸€åŒ–
            max_val = np.max(np.abs(audio))
            if max_val > 0:
                audio = audio / max_val * 0.95
            
            # 3. å¹³æ»‘å¤„ç†
            from scipy.signal import savgol_filter
            if len(audio) > 51:
                audio = savgol_filter(audio, 51, 3)
            
            return audio
            
        except Exception as e:
            print(f"åå¤„ç†å¤±è´¥: {e}")
            return audio
    
    def extract_target_speaker(self, mixed_audio_path, reference_audio_path, output_path="output/voicefilter_result.wav"):
        """ä¸»è¦çš„ç›®æ ‡è¯´è¯äººæå–å‡½æ•°"""
        try:
            print("ğŸ¤ VoiceFilter-WavLM ä¸»äººå£°æå–å™¨")
            print("=" * 60)
            
            # 1. åŠ è½½éŸ³é¢‘
            print("\nğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            mixed_audio = self.load_audio(mixed_audio_path)
            reference_audio = self.load_audio(reference_audio_path)
            
            if mixed_audio is None or reference_audio is None:
                print("âŒ éŸ³é¢‘åŠ è½½å¤±è´¥")
                return False
            
            # 2. æå–å‚è€ƒè¯´è¯äººåµŒå…¥
            print("\nğŸ§  æå–å‚è€ƒè¯´è¯äººç‰¹å¾...")
            reference_embedding = self.extract_speaker_embedding(reference_audio)
            
            if reference_embedding is None:
                print("âŒ å‚è€ƒç‰¹å¾æå–å¤±è´¥")
                return False
            
            # 3. åŸºäºå¸§çº§åˆ«çš„è¯´è¯äººæå–
            print("\nğŸ¯ æ‰§è¡Œç›®æ ‡è¯´è¯äººæå–...")
            enhanced_audio, similarities, mask = self.frame_level_extraction(mixed_audio, reference_embedding)
            
            # 4. åå¤„ç†
            print("\nğŸ”§ éŸ³é¢‘åå¤„ç†...")
            final_audio = self.post_process_audio(enhanced_audio)
            
            # 5. ä¿å­˜ç»“æœ
            print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
            Path(output_path).parent.mkdir(exist_ok=True)
            sf.write(output_path, final_audio, self.sample_rate)
            
            # 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            if similarities is not None and mask is not None:
                self.generate_analysis_report(similarities, mask, output_path)
            
            print("âœ… VoiceFilter-WavLM æå–å®Œæˆ!")
            return True
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            return False
    
    def generate_analysis_report(self, similarities, mask, output_path):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–"""
        try:
            print("\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            
            # åˆ›å»ºå¯è§†åŒ–
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
            
            # æ—¶é—´è½´
            time_axis = np.arange(len(similarities)) * self.hop_length / self.sample_rate
            
            # ç›¸ä¼¼åº¦æ›²çº¿
            ax1.plot(time_axis, similarities, 'b-', linewidth=1, label='è¯´è¯äººç›¸ä¼¼åº¦')
            ax1.fill_between(time_axis, 0, similarities, alpha=0.3)
            ax1.set_ylabel('ç›¸ä¼¼åº¦')
            ax1.set_title('ç›®æ ‡è¯´è¯äººç›¸ä¼¼åº¦åˆ†æ')
            ax1.grid(True, alpha=0.3)
            ax1.legend()
            
            # æå–å†³ç­–
            ax2.fill_between(time_axis, 0, mask.astype(float), alpha=0.6, color='green', label='ä¿ç•™ç‰‡æ®µ')
            ax2.fill_between(time_axis, 0, (~mask).astype(float), alpha=0.6, color='red', label='æŠ‘åˆ¶ç‰‡æ®µ')
            ax2.set_xlabel('æ—¶é—´ (ç§’)')
            ax2.set_ylabel('æå–å†³ç­–')
            ax2.set_title('è¯´è¯äººæå–å†³ç­–')
            ax2.set_ylim(-0.1, 1.1)
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            
            plt.tight_layout()
            
            # ä¿å­˜åˆ†æå›¾
            analysis_path = str(Path(output_path).parent / "voicefilter_analysis.png")
            plt.savefig(analysis_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"  ğŸ“ˆ åˆ†æå›¾ä¿å­˜åˆ°: {analysis_path}")
            
            # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
            report_path = str(Path(output_path).parent / "voicefilter_report.txt")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("VoiceFilter-WavLM æå–æŠ¥å‘Š\n")
                f.write("=" * 40 + "\n\n")
                
                target_ratio = np.mean(mask) * 100
                avg_similarity = np.mean(similarities)
                max_similarity = np.max(similarities)
                
                f.write(f"ç›®æ ‡è¯´è¯äººè¦†ç›–ç‡: {target_ratio:.1f}%\n")
                f.write(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}\n")
                f.write(f"æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}\n")
                f.write(f"æ€»å¤„ç†æ—¶é•¿: {len(similarities) * self.hop_length / self.sample_rate:.2f}ç§’\n")
                
            print(f"  ğŸ“„ æ–‡æœ¬æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
            
        except Exception as e:
            print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    mixed_audio_path = "input/lttgd.wav"  # æ··åˆéŸ³é¢‘
    reference_audio_path = "reference/lttgd_ref.wav"  # å‚è€ƒéŸ³é¢‘
    output_path = "output/voicefilter_extracted.wav"
    
    # åˆ›å»ºæå–å™¨
    extractor = VoiceFilterWavLMExtractor(sample_rate=16000)
    
    # æ‰§è¡Œæå–
    success = extractor.extract_target_speaker(
        mixed_audio_path=mixed_audio_path,
        reference_audio_path=reference_audio_path,
        output_path=output_path
    )
    
    if success:
        print(f"\nğŸ‰ æˆåŠŸ! æå–ç»“æœä¿å­˜åˆ°: {output_path}")
    else:
        print("\nâŒ æå–å¤±è´¥")

if __name__ == "__main__":
    main()
