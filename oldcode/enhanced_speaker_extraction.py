#!/usr/bin/env python3
"""
å¢å¼ºçš„ä¸»äººå£°æå–æ¨¡å—
Enhanced Speaker Extraction Module

åŸºäºå¤šç‰¹å¾èåˆå’Œæ—¶åºåŒ¹é…çš„è¯´è¯äººè¯†åˆ«ä¸æå–ç®—æ³•
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine, euclidean
from scipy.stats import pearsonr
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class EnhancedSpeakerExtractor:
    """
    å¢å¼ºçš„è¯´è¯äººæå–å™¨
    
    åŠŸèƒ½ï¼š
    1. å¤šç‰¹å¾æå–ï¼ˆMFCCã€é¢‘è°±ç‰¹å¾ã€éŸ³é«˜ç­‰ï¼‰
    2. æ»‘åŠ¨çª—å£æ—¶åºåŒ¹é…
    3. åŸºäºç›¸ä¼¼åº¦çš„éŸ³é¢‘åˆ†æ®µ
    4. æ™ºèƒ½éŸ³é¢‘å¢å¼ºå’Œé‡æ„
    """
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13
          # è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°
        self.vad_threshold = 0.005  # é™ä½VADé˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
        self.min_speech_duration = 0.2  # æœ€å°è¯­éŸ³æ®µé•¿åº¦(ç§’) - é™ä½åˆ°0.2ç§’
        
        # åŒ¹é…å‚æ•°
        self.window_duration = 1.5  # åŒ¹é…çª—å£é•¿åº¦(ç§’) - é™ä½åˆ°1.5ç§’
        self.overlap_ratio = 0.8    # çª—å£é‡å æ¯”ä¾‹ - å¢åŠ åˆ°80%
        self.similarity_threshold = 0.05  # ç›¸ä¼¼åº¦é˜ˆå€¼ - è¿›ä¸€æ­¥é™ä½
        
    def extract_comprehensive_features(self, audio):
        """
        æå–ç»¼åˆéŸ³é¢‘ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            features: ç‰¹å¾å­—å…¸
        """
        if len(audio) == 0:
            return None
            
        try:
            features = {}
            
            # 1. MFCCç‰¹å¾ (æ ¸å¿ƒç‰¹å¾)
            mfcc = librosa.feature.mfcc(
                y=audio, 
                sr=self.sample_rate,
                n_mfcc=self.n_mfcc,
                n_fft=self.frame_length,
                hop_length=self.hop_length
            )
            
            # MFCCç»Ÿè®¡ç‰¹å¾
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            features['mfcc_delta'] = np.mean(librosa.feature.delta(mfcc), axis=1)
            features['mfcc_delta2'] = np.mean(librosa.feature.delta(mfcc, order=2), axis=1)
            
            # 2. é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate)[0]
            features['spectral_centroid'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)
            
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate)[0]
            features['spectral_bandwidth'] = np.mean(spectral_bandwidth)
            
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate)[0]
            features['spectral_rolloff'] = np.mean(spectral_rolloff)
            
            # 3. è¿‡é›¶ç‡
            zcr = librosa.feature.zero_crossing_rate(audio)[0]
            features['zcr'] = np.mean(zcr)
            features['zcr_std'] = np.std(zcr)
            
            # 4. èƒ½é‡ç‰¹å¾
            rms = librosa.feature.rms(y=audio)[0]
            features['rms'] = np.mean(rms)
            features['rms_std'] = np.std(rms)
              # 5. åŸºé¢‘ç‰¹å¾ (éŸ³é«˜) - ä¼˜åŒ–å‚æ•°
            try:
                f0 = librosa.yin(audio, fmin=60, fmax=500, sr=self.sample_rate, frame_length=self.frame_length)
                f0_clean = f0[f0 > 0]  # ç§»é™¤æ— å£°æ®µ
                if len(f0_clean) > 0:
                    features['f0_mean'] = np.mean(f0_clean)
                    features['f0_std'] = np.std(f0_clean)
                    features['f0_range'] = np.ptp(f0_clean)
                    features['f0_median'] = np.median(f0_clean)  # æ–°å¢ä¸­ä½æ•°
                else:
                    features['f0_mean'] = 0
                    features['f0_std'] = 0
                    features['f0_range'] = 0
                    features['f0_median'] = 0
            except:
                features['f0_mean'] = 0
                features['f0_std'] = 0
                features['f0_range'] = 0
                features['f0_median'] = 0
            
            # 6. æ–°å¢ï¼šå£°è°±å¯¹æ¯”åº¦ç‰¹å¾
            try:
                spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=self.sample_rate)
                features['spectral_contrast'] = np.mean(spectral_contrast, axis=1)
            except:
                features['spectral_contrast'] = np.zeros(7)  # é»˜è®¤7ä¸ªå­å¸¦
            
            # 7. æ–°å¢ï¼šè‰²åº¦ç‰¹å¾
            try:
                chroma = librosa.feature.chroma_stft(y=audio, sr=self.sample_rate)
                features['chroma'] = np.mean(chroma, axis=1)
            except:
                features['chroma'] = np.zeros(12)  # é»˜è®¤12ä¸ªè‰²åº¦
            
            return features
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def detect_voice_activity(self, audio):
        """
        è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (VAD)
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            voice_segments: è¯­éŸ³æ®µåˆ—è¡¨ [(start_time, end_time), ...]
        """
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_length = int(0.025 * self.sample_rate)  # 25mså¸§
        hop_length = int(0.01 * self.sample_rate)     # 10msè·³è·ƒ
        
        # è®¡ç®—RMSèƒ½é‡
        rms = librosa.feature.rms(
            y=audio, 
            frame_length=frame_length, 
            hop_length=hop_length
        )[0]
        
        # åŠ¨æ€é˜ˆå€¼ (åŸºäºèƒ½é‡åˆ†å¸ƒ)
        energy_threshold = np.percentile(rms, 30)  # ä½¿ç”¨30%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        energy_threshold = max(energy_threshold, self.vad_threshold)
        
        # æ£€æµ‹è¯­éŸ³æ®µ
        voice_frames = rms > energy_threshold
        
        # è½¬æ¢ä¸ºæ—¶é—´æ®µ
        frame_times = librosa.frames_to_time(
            np.arange(len(voice_frames)), 
            sr=self.sample_rate, 
            hop_length=hop_length
        )
        
        # æŸ¥æ‰¾è¿ç»­çš„è¯­éŸ³æ®µ
        voice_segments = []
        start_time = None
        
        for i, is_voice in enumerate(voice_frames):
            if is_voice and start_time is None:
                start_time = frame_times[i]
            elif not is_voice and start_time is not None:
                end_time = frame_times[i]
                if end_time - start_time >= self.min_speech_duration:
                    voice_segments.append((start_time, end_time))
                start_time = None
        
        # å¤„ç†æœ€åä¸€æ®µ
        if start_time is not None:
            end_time = frame_times[-1]
            if end_time - start_time >= self.min_speech_duration:
                voice_segments.append((start_time, end_time))
        
        return voice_segments
    
    def calculate_segment_similarity(self, features1, features2):
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦
        
        Args:
            features1, features2: ç‰¹å¾å­—å…¸
              Returns:
            similarity: ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if features1 is None or features2 is None:
            return 0.0
        
        similarities = []
        weights = []
        
        # 1. MFCCç›¸ä¼¼åº¦ (æœ€é‡è¦çš„ç‰¹å¾)
        try:
            mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
            similarities.append(max(0, mfcc_sim))
            weights.append(0.3)  # é™ä½æƒé‡ï¼Œä¸ºå…¶ä»–ç‰¹å¾ç•™ç©ºé—´
        except:
            pass
        
        # 2. MFCCå·®åˆ†ç‰¹å¾ç›¸ä¼¼åº¦
        try:
            delta_sim = 1 - cosine(features1['mfcc_delta'], features2['mfcc_delta'])
            similarities.append(max(0, delta_sim))
            weights.append(0.15)
        except:
            pass
        
        # 3. åŸºé¢‘ç›¸ä¼¼åº¦ (è¯´è¯äººèº«ä»½çš„é‡è¦æŒ‡æ ‡)
        try:
            f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
            f0_sim = 1 / (1 + f0_diff / 30)  # æ›´æ•æ„Ÿçš„å½’ä¸€åŒ–
            similarities.append(f0_sim)
            weights.append(0.2)
        except:
            pass
        
        # 4. é¢‘è°±è´¨å¿ƒç›¸ä¼¼åº¦
        try:
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 800)  # æ›´æ•æ„Ÿ
            similarities.append(centroid_sim)
            weights.append(0.15)
        except:
            pass
        
        # 5. æ–°å¢ï¼šé¢‘è°±å¯¹æ¯”åº¦ç›¸ä¼¼åº¦
        try:
            contrast_sim = 1 - cosine(features1['spectral_contrast'], features2['spectral_contrast'])
            similarities.append(max(0, contrast_sim))
            weights.append(0.1)
        except:
            pass
        
        # 6. æ–°å¢ï¼šè‰²åº¦ç‰¹å¾ç›¸ä¼¼åº¦
        try:
            chroma_sim = 1 - cosine(features1['chroma'], features2['chroma'])
            similarities.append(max(0, chroma_sim))
            weights.append(0.05)
        except:
            pass
        
        # 7. è¿‡é›¶ç‡ç›¸ä¼¼åº¦
        try:
            zcr_diff = abs(features1['zcr'] - features2['zcr'])
            zcr_sim = 1 / (1 + zcr_diff * 100)
            similarities.append(zcr_sim)
            weights.append(0.05)
        except:
            pass
        
        if len(similarities) == 0:
            return 0.0
        
        # åŠ æƒå¹³å‡
        weights = np.array(weights[:len(similarities)])
        weights = weights / np.sum(weights)  # å½’ä¸€åŒ–æƒé‡
        
        final_similarity = np.average(similarities, weights=weights)
        return final_similarity
    
    def sliding_window_matching(self, target_audio, reference_audio):
        """
        æ»‘åŠ¨çª—å£åŒ¹é…ç®—æ³•
        
        Args:
            target_audio: ç›®æ ‡éŸ³é¢‘ (é•¿éŸ³é¢‘)
            reference_audio: å‚è€ƒéŸ³é¢‘ (çŸ­éŸ³é¢‘)
            
        Returns:
            match_results: åŒ¹é…ç»“æœåˆ—è¡¨ [(start_time, end_time, similarity), ...]
        """
        # çª—å£å‚æ•°
        window_samples = int(self.window_duration * self.sample_rate)
        hop_samples = int(window_samples * (1 - self.overlap_ratio))
        
        # æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
        print("ğŸ”„ æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾...")
        ref_features = self.extract_comprehensive_features(reference_audio)
        if ref_features is None:
            return []
        
        # æ»‘åŠ¨çª—å£åŒ¹é…
        print("ğŸ”„ æ‰§è¡Œæ»‘åŠ¨çª—å£åŒ¹é…...")
        match_results = []
        
        for start_sample in range(0, len(target_audio) - window_samples + 1, hop_samples):
            end_sample = start_sample + window_samples
            window_audio = target_audio[start_sample:end_sample]
            
            # æå–çª—å£ç‰¹å¾
            window_features = self.extract_comprehensive_features(window_audio)
            if window_features is None:
                continue
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_segment_similarity(ref_features, window_features)
            
            # è®°å½•ç»“æœ
            start_time = start_sample / self.sample_rate
            end_time = end_sample / self.sample_rate
            match_results.append((start_time, end_time, similarity))
        
        return match_results
    
    def extract_target_speaker_segments(self, target_audio, reference_audio, min_confidence=0.05):
        """
        æå–ç›®æ ‡è¯´è¯äººçš„éŸ³é¢‘æ®µ
        
        Args:
            target_audio: ç›®æ ‡éŸ³é¢‘
            reference_audio: å‚è€ƒéŸ³é¢‘
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            extracted_segments: æå–çš„éŸ³é¢‘æ®µ
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•°
        """
        print("ğŸ¯ å¼€å§‹æå–ç›®æ ‡è¯´è¯äººéŸ³é¢‘æ®µ...")
        
        # 1. æ»‘åŠ¨çª—å£åŒ¹é…
        match_results = self.sliding_window_matching(target_audio, reference_audio)
        
        if not match_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„éŸ³é¢‘æ®µ")
            return [], []
        
        # 2. ç­›é€‰é«˜ç½®ä¿¡åº¦æ®µ
        high_confidence_segments = [
            (start, end, sim) for start, end, sim in match_results 
            if sim >= min_confidence
        ]
          print(f"ğŸ“Š æ‰¾åˆ° {len(high_confidence_segments)} ä¸ªé«˜ç½®ä¿¡åº¦æ®µ (é˜ˆå€¼: {min_confidence})")
        
        if not high_confidence_segments:
            # å¦‚æœæ²¡æœ‰é«˜ç½®ä¿¡åº¦æ®µï¼Œä½¿ç”¨æœ€é«˜çš„å‡ ä¸ª
            sorted_results = sorted(match_results, key=lambda x: x[2], reverse=True)
            top_count = min(10, len(sorted_results))
            high_confidence_segments = sorted_results[:top_count]
            print(f"ğŸ“Š ä½¿ç”¨å‰ {top_count} ä¸ªæœ€é«˜ç›¸ä¼¼åº¦æ®µ")
        
        # 3. æå–éŸ³é¢‘æ®µ
        extracted_segments = []
        confidence_scores = []
        
        for start_time, end_time, similarity in high_confidence_segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            segment = target_audio[start_sample:end_sample]
            extracted_segments.append(segment)
            confidence_scores.append(similarity)
            
            print(f"   âœ… æ®µ {len(extracted_segments)}: {start_time:.1f}s-{end_time:.1f}s, ç›¸ä¼¼åº¦: {similarity:.3f}")
        
        return extracted_segments, confidence_scores
    
    def reconstruct_target_speaker_audio_timeline(self, target_audio, match_results, min_confidence=0.05):
        """
        åŸºäºæ—¶åºé‡æ„ç›®æ ‡è¯´è¯äººéŸ³é¢‘ - ä¿æŒåŸå§‹æ—¶é•¿ï¼Œé›†æˆæ€§åˆ«åˆ†ç¦»
        
        Args:
            target_audio: åŸå§‹ç›®æ ‡éŸ³é¢‘
            match_results: æ»‘åŠ¨çª—å£åŒ¹é…ç»“æœ [(start_time, end_time, similarity), ...]
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            reconstructed_audio: é‡æ„çš„éŸ³é¢‘ (ä¸åŸå§‹æ—¶é•¿ä¸€è‡´)
            avg_confidence: å¹³å‡ç½®ä¿¡åº¦
        """
        if not match_results:
            return np.zeros_like(target_audio), 0.0
        
        print("ğŸ”„ åŸºäºæ—¶åºé‡æ„ç›®æ ‡è¯´è¯äººéŸ³é¢‘...")
        
        # 1. åˆ†æç›®æ ‡éŸ³é¢‘çš„æ€§åˆ«ç‰¹å¾
        print("ğŸ” åˆ†æç›®æ ‡è¯´è¯äººæ€§åˆ«ç‰¹å¾...")
        # ä½¿ç”¨å‰30ç§’çš„éŸ³é¢‘è¿›è¡Œæ€§åˆ«åˆ†æ
        analysis_length = min(len(target_audio), int(30 * self.sample_rate))
        gender_info = self.analyze_voice_gender(target_audio[:analysis_length])
        
        print(f"   æ£€æµ‹åˆ°æ€§åˆ«: {gender_info['gender']}")
        print(f"   ç½®ä¿¡åº¦: {gender_info['confidence']:.3f}")
        print(f"   å¹³å‡åŸºé¢‘: {gender_info['f0_mean']:.1f} Hz")
        
        # 2. åº”ç”¨å…¨å±€æ€§åˆ«æ»¤æ³¢
        print("ğŸ”§ åº”ç”¨æ€§åˆ«ç‰¹å®šæ»¤æ³¢...")
        if gender_info['gender'] in ['male', 'female'] and gender_info['confidence'] > 0.4:
            target_audio_filtered = self.apply_gender_specific_filter(
                target_audio, gender_info['gender'], strength=0.7
            )
            print(f"   âœ… åº”ç”¨äº†{gender_info['gender']}å£°æ»¤æ³¢")
        else:
            target_audio_filtered = target_audio
            print("   âš ï¸ æ€§åˆ«è¯†åˆ«ä¸ç¡®å®šï¼Œè·³è¿‡æ€§åˆ«æ»¤æ³¢")
        
        # åˆå§‹åŒ–è¾“å‡ºéŸ³é¢‘ï¼ˆä¸åŸå§‹éŸ³é¢‘ç­‰é•¿ï¼‰
        reconstructed_audio = np.zeros_like(target_audio)
        confidence_map = np.zeros(len(target_audio))  # æ¯ä¸ªé‡‡æ ·ç‚¹çš„ç½®ä¿¡åº¦
        
        # çª—å£å‚æ•°
        window_samples = int(self.window_duration * self.sample_rate)
        hop_samples = int(window_samples * (1 - self.overlap_ratio))
        overlap_samples = window_samples - hop_samples
          valid_segments = 0
        total_confidence = 0.0
        
        # æŒ‰æ—¶é—´é¡ºåºå¤„ç†æ¯ä¸ªåŒ¹é…ç»“æœ
        for start_time, end_time, similarity in match_results:
            if similarity < min_confidence:
                continue
                
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            start_sample = max(0, start_sample)
            end_sample = min(len(target_audio), end_sample)
            
            if end_sample <= start_sample:
                continue
            
            # æå–å½“å‰çª—å£çš„æ»¤æ³¢åéŸ³é¢‘
            window_audio = target_audio_filtered[start_sample:end_sample]
            
            # æ ¹æ®ç½®ä¿¡åº¦å’Œæ€§åˆ«ä¿¡æ¯å†³å®šå¤„ç†æ–¹å¼
            if similarity > 0.5:  # é«˜ç½®ä¿¡åº¦ï¼šå¢å¼ºå¤„ç†
                # åº”ç”¨è‡ªé€‚åº”è°±å‡æ³•
                if gender_info['gender'] in ['male', 'female']:
                    processed_audio = self.apply_adaptive_spectral_subtraction(
                        window_audio, None, gender_info['gender']
                    )
                else:
                    processed_audio = self.enhance_audio_segment(window_audio, similarity)
                
                # é¢å¤–çš„å¢ç›Š
                processed_audio = processed_audio * (1.0 + similarity * 0.3)
                weight = similarity
                
            elif similarity > 0.2:  # ä¸­ç­‰ç½®ä¿¡åº¦ï¼šä¿ç•™ä½†å¤„ç†
                # è½»åº¦è°±å‡æ³•å¤„ç†
                if gender_info['gender'] in ['male', 'female']:
                    processed_audio = self.apply_adaptive_spectral_subtraction(
                        window_audio, None, gender_info['gender']
                    )
                    processed_audio = processed_audio * 0.8
                else:
                    processed_audio = window_audio * (0.6 + similarity * 0.4)
                weight = similarity * 0.8
                
            else:  # ä½ç½®ä¿¡åº¦ï¼šå¤§å¹…è¡°å‡
                processed_audio = window_audio * similarity * 0.3
                weight = similarity * 0.2
            
            # å¤„ç†é‡å åŒºåŸŸçš„åŠ æƒæ··åˆ
            current_len = end_sample - start_sample
            for i in range(current_len):
                global_idx = start_sample + i
                if global_idx >= len(reconstructed_audio):
                    break
                
                # è®¡ç®—å½“å‰ä½ç½®çš„æƒé‡ï¼ˆè€ƒè™‘çª—å£å†…çš„ä½ç½®ï¼‰
                if i < overlap_samples and confidence_map[global_idx] > 0:
                    # é‡å åŒºåŸŸï¼šåŠ æƒå¹³å‡
                    existing_weight = confidence_map[global_idx]
                    total_weight = existing_weight + weight
                    if total_weight > 0:
                        # åŠ æƒæ··åˆ
                        reconstructed_audio[global_idx] = (
                            reconstructed_audio[global_idx] * existing_weight + 
                            processed_audio[i] * weight
                        ) / total_weight
                        confidence_map[global_idx] = min(1.0, total_weight)
                else:
                    # éé‡å åŒºåŸŸï¼šç›´æ¥èµ‹å€¼
                    if confidence_map[global_idx] == 0:
                        reconstructed_audio[global_idx] = processed_audio[i]
                        confidence_map[global_idx] = weight
            
            valid_segments += 1
            total_confidence += similarity
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = total_confidence / valid_segments if valid_segments > 0 else 0.0
          # å¯¹æœªåŒ¹é…çš„åŒºåŸŸè¿›è¡Œå¤„ç†
        unmatched_mask = confidence_map < 0.01
        if np.any(unmatched_mask):
            # å¯¹æœªåŒ¹é…åŒºåŸŸåº”ç”¨æ€§åˆ«æ»¤æ³¢å’Œå™ªå£°æŠ‘åˆ¶
            unmatched_audio = target_audio_filtered[unmatched_mask]
            if gender_info['gender'] in ['male', 'female']:
                # å¯¹æœªåŒ¹é…åŒºåŸŸåº”ç”¨æ›´å¼ºçš„æŠ‘åˆ¶
                suppressed_audio = unmatched_audio * 0.05  # å¼ºçƒˆæŠ‘åˆ¶
            else:
                suppressed_audio = unmatched_audio * 0.1
            reconstructed_audio[unmatched_mask] = suppressed_audio
        
        # åå¤„ç†ï¼šå†æ¬¡åº”ç”¨æ€§åˆ«æ»¤æ³¢å¢å¼ºæ•ˆæœ
        if gender_info['gender'] in ['male', 'female'] and gender_info['confidence'] > 0.5:
            print("ğŸ”§ åº”ç”¨æœ€ç»ˆæ€§åˆ«å¢å¼ºæ»¤æ³¢...")
            reconstructed_audio = self.apply_gender_specific_filter(
                reconstructed_audio, gender_info['gender'], strength=0.5
            )
        
        # å¸¸è§„åå¤„ç†
        reconstructed_audio = self.post_process_audio(reconstructed_audio)
        
        print(f"âœ… æ—¶åºé‡æ„å®Œæˆï¼")
        print(f"   åŸå§‹æ—¶é•¿: {len(target_audio)/self.sample_rate:.2f}ç§’")
        print(f"   é‡æ„æ—¶é•¿: {len(reconstructed_audio)/self.sample_rate:.2f}ç§’")
        print(f"   æœ‰æ•ˆæ®µæ•°: {valid_segments}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"   æ€§åˆ«æ»¤æ³¢: {'âœ…' if gender_info['gender'] in ['male', 'female'] else 'âŒ'}")
        
        return reconstructed_audio, avg_confidence
    
    def reconstruct_target_speaker_audio(self, segments, confidence_scores):
        """
        é‡æ„ç›®æ ‡è¯´è¯äººéŸ³é¢‘ (æ—§æ–¹æ³•ï¼Œä¿ç•™å…¼å®¹æ€§)
        
        Args:
            segments: éŸ³é¢‘æ®µåˆ—è¡¨
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•°åˆ—è¡¨
            
        Returns:
            reconstructed_audio: é‡æ„çš„éŸ³é¢‘
            avg_confidence: å¹³å‡ç½®ä¿¡åº¦
        """
        if not segments:
            return np.array([]), 0.0
        
        print("ğŸ”„ é‡æ„ç›®æ ‡è¯´è¯äººéŸ³é¢‘...")
        
        # 1. æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_pairs = sorted(zip(segments, confidence_scores), key=lambda x: x[1], reverse=True)
        
        # 2. æ‹¼æ¥éŸ³é¢‘æ®µ
        reconstructed_audio = []
        used_confidences = []
        
        for i, (segment, confidence) in enumerate(sorted_pairs):
            # æ·»åŠ çŸ­æš‚çš„é™éŸ³é—´éš”
            if i > 0:
                silence_duration = 0.1  # 100msé™éŸ³
                silence_samples = int(silence_duration * self.sample_rate)
                silence = np.zeros(silence_samples)
                reconstructed_audio.append(silence)
            
            # éŸ³é¢‘å¢å¼ºå¤„ç†
            enhanced_segment = self.enhance_audio_segment(segment, confidence)
            reconstructed_audio.append(enhanced_segment)
            used_confidences.append(confidence)
        
        # 3. åˆå¹¶æ‰€æœ‰æ®µ
        final_audio = np.concatenate(reconstructed_audio)
        avg_confidence = np.mean(used_confidences)
        
        # 4. åå¤„ç†
        final_audio = self.post_process_audio(final_audio)
        
        print(f"âœ… é‡æ„å®Œæˆï¼Œæ€»æ—¶é•¿: {len(final_audio)/self.sample_rate:.2f}ç§’")
        print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        
        return final_audio, avg_confidence
    
    def enhance_audio_segment(self, audio_segment, confidence):
        """
        å¢å¼ºéŸ³é¢‘æ®µ
        
        Args:
            audio_segment: éŸ³é¢‘æ®µ
            confidence: ç½®ä¿¡åº¦
            
        Returns:
            enhanced_segment: å¢å¼ºåçš„éŸ³é¢‘æ®µ
        """
        enhanced = audio_segment.copy()
        
        # 1. åŸºäºç½®ä¿¡åº¦çš„å¢ç›Šè°ƒæ•´
        gain_factor = 0.8 + (confidence * 0.4)  # 0.8-1.2èŒƒå›´
        enhanced = enhanced * gain_factor
        
        # 2. åŠ¨æ€èŒƒå›´å‹ç¼©
        if confidence > 0.5:
            # é«˜ç½®ä¿¡åº¦ï¼šè½»å¾®å‹ç¼©
            threshold = 0.7
            ratio = 3.0
        else:
            # ä½ç½®ä¿¡åº¦ï¼šæ›´å¼ºå‹ç¼©
            threshold = 0.5
            ratio = 5.0
        
        enhanced = self.apply_compression(enhanced, threshold, ratio)
        
        # 3. å™ªå£°æŠ‘åˆ¶
        if confidence < 0.4:
            enhanced = self.noise_suppression(enhanced)
        
        # 4. é™å¹…
        enhanced = np.clip(enhanced, -0.95, 0.95)
        
        return enhanced
    
    def apply_compression(self, audio, threshold=0.7, ratio=3.0):
        """åº”ç”¨åŠ¨æ€èŒƒå›´å‹ç¼©"""
        # ç®€å•çš„è½¯å‹ç¼©å™¨å®ç°
        abs_audio = np.abs(audio)
        
        # è®¡ç®—å¢ç›Šå‡å°‘
        gain_reduction = np.ones_like(abs_audio)
        over_threshold = abs_audio > threshold
        
        if np.any(over_threshold):
            excess = abs_audio[over_threshold] - threshold
            gain_reduction[over_threshold] = 1 - (excess / ratio) / abs_audio[over_threshold]
        
        # åº”ç”¨å‹ç¼©
        compressed = audio * gain_reduction
        return compressed
    
    def noise_suppression(self, audio):
        """ç®€å•çš„å™ªå£°æŠ‘åˆ¶"""
        # ä½¿ç”¨è½¯é˜ˆå€¼è¿›è¡Œå™ªå£°æŠ‘åˆ¶
        threshold = np.percentile(np.abs(audio), 10)  # 10%åˆ†ä½æ•°ä½œä¸ºå™ªå£°é˜ˆå€¼
        
        # è½¯é˜ˆå€¼å‡½æ•°
        sign = np.sign(audio)
        magnitude = np.abs(audio)
        suppressed_magnitude = np.maximum(magnitude - threshold, 0.1 * magnitude)
        
        return sign * suppressed_magnitude
    
    def post_process_audio(self, audio):
        """éŸ³é¢‘åå¤„ç†"""
        # 1. å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # 2. é«˜é€šæ»¤æ³¢ (å»é™¤ä½é¢‘å™ªå£°)
        sos = signal.butter(3, 80, btype='high', fs=self.sample_rate, output='sos')
        audio = signal.sosfilt(sos, audio)
        
        # 3. ä½é€šæ»¤æ³¢ (å»é™¤é«˜é¢‘å™ªå£°)
        sos = signal.butter(3, 8000, btype='low', fs=self.sample_rate, output='sos')
        audio = signal.sosfilt(sos, audio)
        
        # 4. å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio
    
    def process_main_speaker_extraction(self, vocals_path, reference_path, output_path=None):
        """
        ä¸»è¦çš„è¯´è¯äººæå–æµç¨‹
        
        Args:
            vocals_path: åˆ†ç¦»åçš„äººå£°æ–‡ä»¶è·¯å¾„
            reference_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            success: å¤„ç†æ˜¯å¦æˆåŠŸ
            result_info: ç»“æœä¿¡æ¯å­—å…¸
        """
        try:
            print("ğŸ¯ å¼€å§‹ä¸»äººå£°æå–æµç¨‹")
            print("=" * 50)
            
            # 1. åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            vocals_audio, _ = librosa.load(vocals_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_path, sr=self.sample_rate)
            
            print(f"   äººå£°éŸ³é¢‘: {len(vocals_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # 2. è¯­éŸ³æ´»åŠ¨æ£€æµ‹
            print("ğŸ”„ æ‰§è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹...")
            voice_segments = self.detect_voice_activity(vocals_audio)
            print(f"   æ£€æµ‹åˆ° {len(voice_segments)} ä¸ªè¯­éŸ³æ®µ")            # 3. æ»‘åŠ¨çª—å£åŒ¹é… (è·å–åŒ¹é…ç»“æœè€Œä¸æ˜¯ç›´æ¥æå–æ®µ)
            print("ğŸ”„ æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            match_results = self.sliding_window_matching(vocals_audio, reference_audio)
            
            if not match_results:
                print("âŒ æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„éŸ³é¢‘æ®µ")
                return False, {"error": "No matching segments found"}
            
            # 4. ä½¿ç”¨æ–°çš„æ—¶åºé‡æ„æ–¹æ³•
            final_audio, avg_confidence = self.reconstruct_target_speaker_audio_timeline(
                vocals_audio, match_results, min_confidence=0.05
            )
            
            # 5. ä¿å­˜ç»“æœ
            if output_path:
                sf.write(output_path, final_audio, self.sample_rate)
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
              # 6. ç”ŸæˆæŠ¥å‘Š
            num_valid_segments = sum(1 for _, _, sim in match_results if sim >= 0.05)
            result_info = {
                "original_duration": len(vocals_audio) / self.sample_rate,
                "extracted_duration": len(final_audio) / self.sample_rate,
                "num_segments": num_valid_segments,
                "avg_confidence": avg_confidence,
                "voice_segments_detected": len(voice_segments),
                "compression_ratio": len(final_audio) / len(vocals_audio),
                "success": True
            }
            
            print("\nğŸ“Š æå–ç»“æœ:")
            print(f"   åŸå§‹æ—¶é•¿: {result_info['original_duration']:.2f}ç§’")
            print(f"   æå–æ—¶é•¿: {result_info['extracted_duration']:.2f}ç§’")
            print(f"   å‹ç¼©æ¯”: {result_info['compression_ratio']:.2f}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
            print(f"   ä½¿ç”¨æ®µæ•°: {num_valid_segments}")
            
            return True, result_info
            
        except Exception as e:
            print(f"âŒ ä¸»äººå£°æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}
    
    def extract_target_speaker_segments(self, vocals_audio, reference_audio, min_confidence=0.2):
        """
        æå–ç›®æ ‡è¯´è¯äººéŸ³é¢‘æ®µ
        
        Args:
            vocals_audio: åˆ†ç¦»çš„äººå£°éŸ³é¢‘
            reference_audio: å‚è€ƒéŸ³é¢‘
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
              Returns:
            segments: åŒ¹é…çš„éŸ³é¢‘æ®µåˆ—è¡¨
            confidences: å¯¹åº”çš„ç½®ä¿¡åº¦åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨æ»‘åŠ¨çª—å£åŒ¹é…
            match_results = self.sliding_window_matching(vocals_audio, reference_audio)
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ®µ
            segments = []
            confidences = []
            
            for start_time, end_time, similarity in match_results:
                if similarity >= min_confidence:
                    start_sample = int(start_time * self.sample_rate)
                    end_sample = int(end_time * self.sample_rate)
                    segment = vocals_audio[start_sample:end_sample]
                    segments.append(segment)
                    confidences.append(similarity)
            
            return segments, confidences
            
        except Exception as e:
            print(f"éŸ³é¢‘æ®µæå–å¤±è´¥: {e}")
            return [], []
    
    def reconstruct_target_speaker_audio(self, segments, confidences):
        """
        é‡æ„ç›®æ ‡è¯´è¯äººéŸ³é¢‘
        
        Args:
            segments: éŸ³é¢‘æ®µåˆ—è¡¨
            confidences: ç½®ä¿¡åº¦åˆ—è¡¨
            
        Returns:
            reconstructed_audio: é‡æ„çš„éŸ³é¢‘
            avg_confidence: å¹³å‡ç½®ä¿¡åº¦
        """
        try:
            if not segments:
                return np.array([]), 0.0
            
            # è¿æ¥æ‰€æœ‰æ®µ
            reconstructed_audio = np.concatenate(segments)
            
            # åå¤„ç†
            reconstructed_audio = self.post_process_audio(reconstructed_audio)
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = np.mean(confidences) if confidences else 0.0
            
            return reconstructed_audio, avg_confidence
            
        except Exception as e:
            print(f"éŸ³é¢‘é‡æ„å¤±è´¥: {e}")
            return np.array([]), 0.0
    
    def analyze_voice_gender(self, audio):
        """
        åˆ†æéŸ³é¢‘çš„æ€§åˆ«ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            gender_info: æ€§åˆ«ä¿¡æ¯å­—å…¸
        """
        if len(audio) == 0:
            return {'gender': 'unknown', 'confidence': 0.0, 'f0_mean': 0}
        
        try:
            # åŸºé¢‘åˆ†æ
            f0 = librosa.yin(audio, fmin=60, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) == 0:
                return {'gender': 'unknown', 'confidence': 0.0, 'f0_mean': 0}
            
            f0_mean = np.mean(f0_clean)
            f0_median = np.median(f0_clean)
            
            # é¢‘åŸŸèƒ½é‡åˆ†æ
            stft = librosa.stft(audio, n_fft=self.frame_length, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            freqs = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å®šä¹‰æ€§åˆ«ç‰¹å¾é¢‘ç‡å¸¦
            male_freq_mask = (freqs >= 80) & (freqs <= 180)     # ç”·å£°åŸºé¢‘åŒº
            female_freq_mask = (freqs >= 150) & (freqs <= 350)  # å¥³å£°åŸºé¢‘åŒº
            
            male_energy = np.mean(magnitude[male_freq_mask, :]) if np.any(male_freq_mask) else 0
            female_energy = np.mean(magnitude[female_freq_mask, :]) if np.any(female_freq_mask) else 0
            
            # æ€§åˆ«åˆ¤æ–­
            if f0_mean < 150 and male_energy > female_energy * 1.2:
                gender = 'male'
                confidence = min(1.0, (150 - f0_mean) / 70 + 0.3)
            elif f0_mean > 170 and female_energy > male_energy * 1.1:
                gender = 'female'
                confidence = min(1.0, (f0_mean - 170) / 80 + 0.3)
            else:
                # åŸºäºèƒ½é‡æ¯”ä¾‹åˆ¤æ–­
                if male_energy > female_energy * 1.3:
                    gender = 'male'
                    confidence = 0.6
                elif female_energy > male_energy * 1.2:
                    gender = 'female'
                    confidence = 0.6
                else:
                    gender = 'unknown'
                    confidence = 0.2
            
            return {
                'gender': gender,
                'confidence': confidence,
                'f0_mean': f0_mean,
                'f0_median': f0_median,
                'male_energy': male_energy,
                'female_energy': female_energy
            }
            
        except Exception as e:
            print(f"æ€§åˆ«åˆ†æå¤±è´¥: {e}")
            return {'gender': 'unknown', 'confidence': 0.0, 'f0_mean': 0}
    
    def apply_gender_specific_filter(self, audio, target_gender, strength=0.8):
        """
        åº”ç”¨åŸºäºæ€§åˆ«çš„é¢‘åŸŸæ»¤æ³¢
        
        Args:
            audio: è¾“å…¥éŸ³é¢‘
            target_gender: ç›®æ ‡æ€§åˆ« ('male' æˆ– 'female')
            strength: æ»¤æ³¢å¼ºåº¦ (0-1)
            
        Returns:
            filtered_audio: æ»¤æ³¢åçš„éŸ³é¢‘
        """
        if len(audio) == 0:
            return audio
        
        try:
            # çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
            stft = librosa.stft(audio, n_fft=self.frame_length, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freqs = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # åˆ›å»ºæ»¤æ³¢å™¨æ©ç 
            filter_mask = np.ones_like(magnitude)
            
            if target_gender == 'female':
                # ä¿ç•™å¥³å£°ï¼ŒæŠ‘åˆ¶ç”·å£°
                # å¼ºçƒˆæŠ‘åˆ¶ç”·å£°åŸºé¢‘åŒºåŸŸ (80-180 Hz)
                male_mask = (freqs >= 80) & (freqs <= 180)
                filter_mask[male_mask, :] *= (1 - strength * 0.8)
                
                # è½»åº¦æŠ‘åˆ¶ä½é¢‘åŒºåŸŸ (60-120 Hz)
                low_male_mask = (freqs >= 60) & (freqs <= 120)
                filter_mask[low_male_mask, :] *= (1 - strength * 0.6)
                
                # ä¿ç•™å¥³å£°åŸºé¢‘åŒºåŸŸ (150-350 Hz)
                female_mask = (freqs >= 150) & (freqs <= 350)
                filter_mask[female_mask, :] *= (1 + strength * 0.3)
                
                # ä¿ç•™é«˜é¢‘å…±æŒ¯å³° (1000-3000 Hz)
                high_freq_mask = (freqs >= 1000) & (freqs <= 3000)
                filter_mask[high_freq_mask, :] *= (1 + strength * 0.2)
                
            elif target_gender == 'male':
                # ä¿ç•™ç”·å£°ï¼ŒæŠ‘åˆ¶å¥³å£°
                # å¼ºçƒˆæŠ‘åˆ¶å¥³å£°åŸºé¢‘åŒºåŸŸ (200-400 Hz)
                female_mask = (freqs >= 200) & (freqs <= 400)
                filter_mask[female_mask, :] *= (1 - strength * 0.8)
                
                # ä¿ç•™ç”·å£°åŸºé¢‘åŒºåŸŸ (80-180 Hz)
                male_mask = (freqs >= 80) & (freqs <= 180)
                filter_mask[male_mask, :] *= (1 + strength * 0.4)
                
                # è½»åº¦æŠ‘åˆ¶é«˜é¢‘åŒºåŸŸ (2000-4000 Hz)
                high_freq_mask = (freqs >= 2000) & (freqs <= 4000)
                filter_mask[high_freq_mask, :] *= (1 - strength * 0.3)
            
            # åº”ç”¨æ»¤æ³¢å™¨
            filtered_magnitude = magnitude * filter_mask
            
            # é‡æ„éŸ³é¢‘
            filtered_stft = filtered_magnitude * np.exp(1j * phase)
            filtered_audio = librosa.istft(filtered_stft, hop_length=self.hop_length)
            
            return filtered_audio
            
        except Exception as e:
            print(f"æ€§åˆ«æ»¤æ³¢å¤±è´¥: {e}")
            return audio
    
    def apply_adaptive_spectral_subtraction(self, mixed_audio, target_features, target_gender):
        """
        è‡ªé€‚åº”è°±å‡æ³•ï¼Œç»“åˆæ€§åˆ«ä¿¡æ¯
        
        Args:
            mixed_audio: æ··åˆéŸ³é¢‘
            target_features: ç›®æ ‡è¯´è¯äººç‰¹å¾
            target_gender: ç›®æ ‡æ€§åˆ«
            
        Returns:
            enhanced_audio: å¢å¼ºåçš„éŸ³é¢‘  
        """
        try:
            # çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
            stft = librosa.stft(mixed_audio, n_fft=self.frame_length, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freqs = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # ä¼°è®¡å™ªå£°è°±ï¼ˆéç›®æ ‡æ€§åˆ«çš„é¢‘è°±ï¼‰
            frame_energy = np.sum(magnitude**2, axis=0)
            
            # æ ¹æ®ç›®æ ‡æ€§åˆ«è°ƒæ•´å™ªå£°ä¼°è®¡ç­–ç•¥
            if target_gender == 'female':
                # å¯¹äºå¥³å£°ç›®æ ‡ï¼Œç”·å£°åŒºåŸŸä½œä¸ºå™ªå£°
                male_freq_mask = (freqs >= 80) & (freqs <= 180)
                noise_estimation_mask = male_freq_mask
            elif target_gender == 'male':
                # å¯¹äºç”·å£°ç›®æ ‡ï¼Œå¥³å£°åŒºåŸŸä½œä¸ºå™ªå£°
                female_freq_mask = (freqs >= 200) & (freqs <= 400)
                noise_estimation_mask = female_freq_mask
            else:
                # æœªçŸ¥æ€§åˆ«ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                noise_threshold = np.percentile(frame_energy, 20)
                noise_frames = magnitude[:, frame_energy < noise_threshold]
                if noise_frames.shape[1] > 0:
                    noise_spectrum = np.mean(noise_frames, axis=1, keepdims=True)
                else:
                    noise_spectrum = np.mean(magnitude, axis=1, keepdims=True) * 0.1
                
                # è°±å‡æ³•
                alpha = 2.0
                beta = 0.01
                enhanced_magnitude = magnitude - alpha * noise_spectrum
                enhanced_magnitude = np.maximum(enhanced_magnitude, beta * magnitude)
                
                # é‡æ„
                enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
                return librosa.istft(enhanced_stft, hop_length=self.hop_length)
            
            # åŸºäºæ€§åˆ«çš„å™ªå£°è°±ä¼°è®¡
            if np.any(noise_estimation_mask):
                noise_spectrum = np.mean(magnitude[noise_estimation_mask, :], axis=0, keepdims=True).T
                noise_spectrum = np.repeat(noise_spectrum, magnitude.shape[0], axis=1).T
            else:
                noise_spectrum = np.mean(magnitude, axis=1, keepdims=True) * 0.1
            
            # è‡ªé€‚åº”è°±å‡æ³•å‚æ•°
            alpha = 2.5  # æ›´å¼ºçš„å™ªå£°æŠ‘åˆ¶
            beta = 0.05  # é€‚ä¸­çš„æ®‹ç•™å™ªå£°
            
            # åº”ç”¨è°±å‡æ³•
            enhanced_magnitude = magnitude - alpha * noise_spectrum
            enhanced_magnitude = np.maximum(enhanced_magnitude, beta * magnitude)
            
            # é‡æ„éŸ³é¢‘
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            enhanced_audio = librosa.istft(enhanced_stft, hop_length=self.hop_length)
            
            return enhanced_audio
            
        except Exception as e:
            print(f"è‡ªé€‚åº”è°±å‡æ³•å¤±è´¥: {e}")
            return mixed_audio

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æå–å™¨
    extractor = EnhancedSpeakerExtractor(sample_rate=22050)
    
    # æ–‡ä»¶è·¯å¾„
    vocals_path = "temp/demucs_output/htdemucs/answer/vocals.wav"
    reference_path = "reference/refne2.wav"
    output_path = "output/enhanced_main_speaker.wav"
    
    # æ‰§è¡Œæå–
    success, result_info = extractor.process_main_speaker_extraction(
        vocals_path, reference_path, output_path
    )
    
    if success:
        print("\nğŸ‰ ä¸»äººå£°æå–æˆåŠŸ!")
    else:
        print(f"\nâŒ ä¸»äººå£°æå–å¤±è´¥: {result_info.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()
