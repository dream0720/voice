#!/usr/bin/env python3
"""
æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ V2
Smart Voice Segmentation and Speaker Recognition System V2

é’ˆå¯¹ä¸»äººå£°èƒ½é‡å¼ºã€èƒŒæ™¯ç”·å£°ä½æ²‰çš„åœºæ™¯ä¼˜åŒ–ï¼š
1. åŸºäºèƒ½é‡çš„åˆ†æ®µ
2. åŸºäºéŸ³é‡å˜åŒ–çš„åˆ†æ®µ  
3. åŸºäºé¢‘è°±å˜åŒ–çš„åˆ†æ®µ
4. ç›¸é‚»æ®µç›¸å…³æ€§åˆ†æ
5. å¤šç‰¹å¾èåˆè¯†åˆ«
6. ç”·å¥³å£°åˆ†ç¦»å¢å¼º (æ–°å¢)
7. é¢‘åŸŸæŠ‘åˆ¶å’Œå¢å¼º (æ–°å¢)
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from pathlib import Path
import matplotlib.pyplot as plt

class SmartSpeakerSeparatorV2:
    """
    æ™ºèƒ½è¯´è¯äººåˆ†ç¦»å™¨ V2 - å¢å¼ºç‰ˆ
    """
    
    def __init__(self, sample_rate=22050):
        self.sample_rate = sample_rate
        self.frame_length = 2048
        self.hop_length = 512
        self.n_mfcc = 13
        
        # å¤šå±‚åˆ†æ®µå‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        self.energy_threshold_ratio = 0.05   # è¿›ä¸€æ­¥é™ä½èƒ½é‡é˜ˆå€¼ï¼Œæ•è·æ›´å¤šä½éŸ³é‡æ®µ
        self.volume_change_threshold = 0.3   # é™ä½éŸ³é‡å˜åŒ–é˜ˆå€¼
        self.spectral_change_threshold = 0.25 # é™ä½é¢‘è°±å˜åŒ–é˜ˆå€¼
        self.min_segment_duration = 0.25     # è¿›ä¸€æ­¥é™ä½æœ€å°æ®µé•¿åº¦
        self.merge_threshold = 0.55          # é™ä½æ®µåˆå¹¶é˜ˆå€¼
        
        # è¯´è¯äººè¯†åˆ«å‚æ•°
        self.similarity_threshold = 0.30     # è¿›ä¸€æ­¥é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # ç”·å¥³å£°åˆ†ç¦»å‚æ•°ï¼ˆæ–°å¢ï¼‰
        self.male_voice_suppression = True   # å¯ç”¨ç”·å£°æŠ‘åˆ¶
        self.female_voice_boost = True       # å¯ç”¨å¥³å£°å¢å¼º
        self.low_freq_cutoff = 300          # ç”·å£°ä½é¢‘æˆªæ­¢é¢‘ç‡
        self.energy_dominance_ratio = 0.6   # èƒ½é‡å ä¼˜æ¯”ä¾‹
        self.pitch_gender_threshold = 165   # æ€§åˆ«åŸºé¢‘é˜ˆå€¼ï¼ˆHzï¼‰
        
    def multi_level_segmentation(self, audio):
        """
        å¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            segments: [(start_time, end_time, confidence), ...] 
        """
        print("ğŸ” æ‰§è¡Œå¤šå±‚æ¬¡è¯­éŸ³åˆ†æ®µ...")
        
        # ç¬¬ä¸€å±‚ï¼šåŸºäºèƒ½é‡çš„ç²—åˆ†æ®µ
        energy_segments = self.energy_based_segmentation(audio)
        print(f"   èƒ½é‡åˆ†æ®µ: {len(energy_segments)} ä¸ªåˆå§‹æ®µ")
        
        # ç¬¬äºŒå±‚ï¼šåŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†æ®µ
        volume_segments = self.volume_change_segmentation(audio, energy_segments)
        print(f"   éŸ³é‡ç»†åˆ†: {len(volume_segments)} ä¸ªæ®µ")
        
        # ç¬¬ä¸‰å±‚ï¼šåŸºäºé¢‘è°±å˜åŒ–çš„ç²¾ç»†åˆ†æ®µ
        spectral_segments = self.spectral_change_segmentation(audio, volume_segments)
        print(f"   é¢‘è°±ç»†åˆ†: {len(spectral_segments)} ä¸ªæ®µ")
        
        # ç¬¬å››å±‚ï¼šç›¸é‚»æ®µç›¸å…³æ€§åˆ†æå’Œåˆå¹¶
        merged_segments = self.merge_similar_segments(audio, spectral_segments)
        print(f"   ç›¸å…³æ€§åˆå¹¶: {len(merged_segments)} ä¸ªæœ€ç»ˆæ®µ")
        
        # ç¬¬äº”å±‚ï¼šéªŒè¯å’Œè¡¥å……é—æ¼çš„è¯­éŸ³æ®µ
        final_segments = self.validate_and_supplement_segments(audio, merged_segments)
        print(f"   éªŒè¯è¡¥å……: {len(final_segments)} ä¸ªéªŒè¯æ®µ")
        
        return final_segments
    
    def energy_based_segmentation(self, audio):
        """æ”¹è¿›çš„åŸºäºèƒ½é‡çš„åˆ†æ®µ"""
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_size = int(0.02 * self.sample_rate)   # ç¼©çŸ­åˆ°20ms
        hop_size = int(0.005 * self.sample_rate)    # ç¼©çŸ­åˆ°5msï¼Œæ›´ç²¾ç»†
        
        energy = []
        for i in range(0, len(audio) - frame_size + 1, hop_size):
            frame = audio[i:i + frame_size]
            energy.append(np.sum(frame ** 2))
        
        energy = np.array(energy)
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„é˜ˆå€¼è®¡ç®—
        energy_sorted = np.sort(energy)
        # ä½¿ç”¨è¾ƒä½çš„ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼ï¼Œé¿å…æ¼æ‰ä½éŸ³é‡çš„è¯­éŸ³
        threshold = np.percentile(energy_sorted, 15)  # ä½¿ç”¨15%åˆ†ä½æ•°
        
        # å¦‚æœé˜ˆå€¼è¿‡ä½ï¼Œä½¿ç”¨å‡å€¼çš„ä¸€å®šæ¯”ä¾‹
        energy_mean = np.mean(energy)
        adaptive_threshold = energy_mean * self.energy_threshold_ratio
        threshold = max(threshold, adaptive_threshold)
        
        print(f"   èƒ½é‡åˆ†æ®µé˜ˆå€¼: {threshold:.8f} (å‡å€¼: {energy_mean:.8f})")
        
        # æ‰¾åˆ°è¶…è¿‡é˜ˆå€¼çš„åŒºåŸŸ
        above_threshold = energy > threshold
        
        # è·å–è¾¹ç•Œ
        segments = []
        in_segment = False
        start_idx = 0
        
        for i, is_above in enumerate(above_threshold):
            if is_above and not in_segment:
                start_idx = i
                in_segment = True
            elif not is_above and in_segment:
                end_idx = i
                start_time = start_idx * hop_size / self.sample_rate
                end_time = end_idx * hop_size / self.sample_rate
                
                if end_time - start_time >= self.min_segment_duration:
                    segments.append((start_time, end_time, np.mean(energy[start_idx:end_idx])))
                    print(f"     èƒ½é‡æ®µ: {start_time:.2f}s-{end_time:.2f}s (æ—¶é•¿: {end_time-start_time:.2f}s)")
                
                in_segment = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µ
        if in_segment:
            end_time = len(above_threshold) * hop_size / self.sample_rate
            start_time = start_idx * hop_size / self.sample_rate
            if end_time - start_time >= self.min_segment_duration:
                segments.append((start_time, end_time, np.mean(energy[start_idx:])))
                print(f"     èƒ½é‡æ®µ: {start_time:.2f}s-{end_time:.2f}s (æ—¶é•¿: {end_time-start_time:.2f}s)")
        
        return segments
    
    def volume_change_segmentation(self, audio, initial_segments):
        """åŸºäºéŸ³é‡å˜åŒ–çš„ç»†åˆ†æ®µ"""
        refined_segments = []
        
        for start_time, end_time, confidence in initial_segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            if end_sample > len(audio):
                end_sample = len(audio)
            
            segment_audio = audio[start_sample:end_sample]
            
            # è®¡ç®—RMSèƒ½é‡å˜åŒ–
            frame_size = int(0.02 * self.sample_rate)
            hop_size = int(0.005 * self.sample_rate)
            
            rms_values = []
            for i in range(0, len(segment_audio) - frame_size + 1, hop_size):
                frame = segment_audio[i:i + frame_size]
                rms = np.sqrt(np.mean(frame ** 2))
                rms_values.append(rms)
            
            if len(rms_values) < 5:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            rms_values = np.array(rms_values)
            
            # æ£€æµ‹éŸ³é‡çªå˜ç‚¹
            diff = np.abs(np.diff(rms_values))
            threshold = np.std(diff) * self.volume_change_threshold
            
            split_points = []
            for i, d in enumerate(diff):
                if d > threshold:
                    split_time = start_time + (i + 1) * hop_size / self.sample_rate
                    split_points.append(split_time)
            
            # åˆ†å‰²æ®µ
            current_start = start_time
            for split_time in split_points:
                if split_time - current_start >= self.min_segment_duration:
                    refined_segments.append((current_start, split_time, confidence))
                    current_start = split_time
            
            # æ·»åŠ æœ€åä¸€æ®µ
            if end_time - current_start >= self.min_segment_duration:
                refined_segments.append((current_start, end_time, confidence))
        
        return refined_segments
    
    def spectral_change_segmentation(self, audio, segments):
        """åŸºäºé¢‘è°±å˜åŒ–çš„ç²¾ç»†åˆ†æ®µ"""
        refined_segments = []
        
        for start_time, end_time, confidence in segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            if end_sample > len(audio):
                end_sample = len(audio)
            
            segment_audio = audio[start_sample:end_sample]
            
            if len(segment_audio) < self.frame_length * 3:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            # è®¡ç®—STFT
            stft = librosa.stft(segment_audio, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            
            # è®¡ç®—é¢‘è°±è´¨å¿ƒå˜åŒ–
            spectral_centroids = librosa.feature.spectral_centroid(S=magnitude)[0]
            
            if len(spectral_centroids) < 5:
                refined_segments.append((start_time, end_time, confidence))
                continue
            
            # æ£€æµ‹é¢‘è°±çªå˜ç‚¹
            diff = np.abs(np.diff(spectral_centroids))
            threshold = np.std(diff) * self.spectral_change_threshold
            
            split_points = []
            for i, d in enumerate(diff):
                if d > threshold:
                    split_time = start_time + (i + 1) * self.hop_length / self.sample_rate
                    split_points.append(split_time)
            
            # åˆ†å‰²æ®µ
            current_start = start_time
            for split_time in split_points:
                if split_time - current_start >= self.min_segment_duration:
                    refined_segments.append((current_start, split_time, confidence))
                    current_start = split_time
            
            # æ·»åŠ æœ€åä¸€æ®µ
            if end_time - current_start >= self.min_segment_duration:
                refined_segments.append((current_start, end_time, confidence))
        
        return refined_segments
    
    def merge_similar_segments(self, audio, segments):
        """åˆå¹¶ç›¸ä¼¼çš„ç›¸é‚»æ®µ"""
        if len(segments) <= 1:
            return segments
        
        print("ğŸ”„ åˆ†æç›¸é‚»æ®µç›¸å…³æ€§...")
        merged_segments = []
        current_segment = segments[0]
        
        for i in range(1, len(segments)):
            next_segment = segments[i]
            
            # æå–éŸ³é¢‘æ®µ
            start1, end1, conf1 = current_segment
            start2, end2, conf2 = next_segment
            
            sample1_start = int(start1 * self.sample_rate)
            sample1_end = int(end1 * self.sample_rate)
            sample2_start = int(start2 * self.sample_rate)
            sample2_end = int(end2 * self.sample_rate)
            
            audio1 = audio[sample1_start:sample1_end]
            audio2 = audio[sample2_start:sample2_end]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_segment_similarity(audio1, audio2)
            
            print(f"   æ®µ {start1:.2f}-{end1:.2f}s ä¸ {start2:.2f}-{end2:.2f}s ç›¸ä¼¼åº¦: {similarity:.3f}")
            
            if similarity > self.merge_threshold:
                # åˆå¹¶æ®µ
                merged_start = min(start1, start2)
                merged_end = max(end1, end2)
                merged_conf = max(conf1, conf2)
                current_segment = (merged_start, merged_end, merged_conf)
                print(f"     -> åˆå¹¶æ®µ: {merged_start:.2f}-{merged_end:.2f}s")
            else:
                # ä¸åˆå¹¶ï¼Œä¿å­˜å½“å‰æ®µ
                merged_segments.append(current_segment)
                current_segment = next_segment
        
        # æ·»åŠ æœ€åä¸€æ®µ
        merged_segments.append(current_segment)
        
        return merged_segments
    
    def validate_and_supplement_segments(self, audio, segments):
        """éªŒè¯å’Œè¡¥å……é—æ¼çš„è¯­éŸ³æ®µ"""
        print("ğŸ” éªŒè¯è¯­éŸ³æ®µå®Œæ•´æ€§...")
        
        # åˆ›å»ºè¦†ç›–æ©ç 
        total_samples = len(audio)
        coverage_mask = np.zeros(total_samples, dtype=bool)
        
        for start_time, end_time, _ in segments:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            end_sample = min(end_sample, total_samples)
            coverage_mask[start_sample:end_sample] = True
        
        # æ‰¾åˆ°æœªè¦†ç›–çš„åŒºåŸŸ
        uncovered_regions = []
        in_uncovered = False
        start_idx = 0
        
        for i, covered in enumerate(coverage_mask):
            if not covered and not in_uncovered:
                start_idx = i
                in_uncovered = True
            elif covered and in_uncovered:
                end_idx = i
                start_time = start_idx / self.sample_rate
                end_time = end_idx / self.sample_rate
                
                if end_time - start_time >= 0.1:  # è‡³å°‘100ms
                    uncovered_regions.append((start_time, end_time))
                
                in_uncovered = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæœªè¦†ç›–åŒºåŸŸ
        if in_uncovered:
            end_time = total_samples / self.sample_rate
            start_time = start_idx / self.sample_rate
            if end_time - start_time >= 0.1:
                uncovered_regions.append((start_time, end_time))
        
        print(f"   å‘ç° {len(uncovered_regions)} ä¸ªæœªè¦†ç›–åŒºåŸŸ")
        
        # æ£€æŸ¥æœªè¦†ç›–åŒºåŸŸæ˜¯å¦æœ‰è¯­éŸ³
        supplemented_segments = list(segments)
        
        for start_time, end_time in uncovered_regions:
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            region_audio = audio[start_sample:end_sample]
            
            # è®¡ç®—RMSèƒ½é‡
            rms = np.sqrt(np.mean(region_audio ** 2))
            
            # å¦‚æœèƒ½é‡è¶³å¤Ÿï¼Œæ·»åŠ ä¸ºè¯­éŸ³æ®µ
            if rms > 0.01:  # åŠ¨æ€é˜ˆå€¼
                supplemented_segments.append((start_time, end_time, rms))
                print(f"     è¡¥å……è¯­éŸ³æ®µ: {start_time:.2f}s-{end_time:.2f}s (RMS: {rms:.6f})")
        
        # æŒ‰æ—¶é—´æ’åº
        supplemented_segments.sort(key=lambda x: x[0])
        
        return supplemented_segments
    
    def calculate_segment_similarity(self, audio1, audio2):
        """è®¡ç®—ä¸¤ä¸ªéŸ³é¢‘æ®µçš„ç›¸ä¼¼åº¦"""
        try:
            if len(audio1) == 0 or len(audio2) == 0:
                return 0.0
            
            # é•¿åº¦å¯¹é½
            min_len = min(len(audio1), len(audio2))
            audio1 = audio1[:min_len]
            audio2 = audio2[:min_len]
            
            similarities = []
            
            # 1. æ³¢å½¢ç›¸å…³æ€§
            correlation, _ = pearsonr(audio1, audio2)
            if not np.isnan(correlation):
                similarities.append(abs(correlation))
            
            # 2. MFCCç›¸ä¼¼åº¦
            try:
                mfcc1 = librosa.feature.mfcc(y=audio1, sr=self.sample_rate, n_mfcc=13)
                mfcc2 = librosa.feature.mfcc(y=audio2, sr=self.sample_rate, n_mfcc=13)
                
                mfcc1_mean = np.mean(mfcc1, axis=1)
                mfcc2_mean = np.mean(mfcc2, axis=1)
                
                mfcc_sim = 1 - cosine(mfcc1_mean, mfcc2_mean)
                if not np.isnan(mfcc_sim):
                    similarities.append(max(0, mfcc_sim))
            except:
                pass
            
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            return 0.0
    
    def extract_comprehensive_features(self, audio):
        """æå–éŸ³é¢‘æ®µçš„ç»¼åˆç‰¹å¾"""
        try:
            if len(audio) < self.frame_length:
                return None
            
            features = {}
            
            # MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=self.n_mfcc)
            features['mfcc_mean'] = np.mean(mfcc, axis=1)
            features['mfcc_std'] = np.std(mfcc, axis=1)
            
            # åŸºé¢‘ç‰¹å¾
            f0 = librosa.yin(audio, fmin=60, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) > 0:
                features['f0_mean'] = np.mean(f0_clean)
                features['f0_std'] = np.std(f0_clean)
                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)
            else:
                features['f0_mean'] = 0
                features['f0_std'] = 0
                features['f0_range'] = 0
            
            # é¢‘è°±ç‰¹å¾
            stft = librosa.stft(audio, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            
            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(S=magnitude))
            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(S=magnitude))
            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(S=magnitude))
            
            # è‰²åº¦ç‰¹å¾
            chroma = librosa.feature.chroma_stft(S=magnitude, sr=self.sample_rate)
            features['chroma_mean'] = np.mean(chroma, axis=1)
            
            # é›¶äº¤å‰ç‡
            features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(audio))
            
            # RMSç‰¹å¾
            features['rms'] = np.sqrt(np.mean(audio ** 2))
            
            return features
            
        except Exception as e:
            print(f"   ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def analyze_gender_characteristics(self, audio_segment):
        """åˆ†æéŸ³é¢‘æ®µçš„æ€§åˆ«ç‰¹å¾"""
        try:
            if len(audio_segment) < self.frame_length:
                return None
                
            # åŸºé¢‘åˆ†æ
            f0 = librosa.yin(audio_segment, fmin=50, fmax=500, sr=self.sample_rate)
            f0_clean = f0[f0 > 0]
            
            if len(f0_clean) < 5:
                return None
                
            f0_mean = np.mean(f0_clean)
            f0_std = np.std(f0_clean)
            
            # æ€§åˆ«åˆ¤æ–­ï¼ˆç®€å•åŸºäºåŸºé¢‘ï¼‰
            is_likely_female = f0_mean > self.pitch_gender_threshold
            
            # é¢‘è°±ç‰¹å¾
            stft = librosa.stft(audio_segment, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            
            # ä½é¢‘èƒ½é‡å æ¯”ï¼ˆç”·å£°ç‰¹å¾ï¼‰
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            low_freq_mask = freq_bins < self.low_freq_cutoff
            low_freq_energy = np.sum(magnitude[low_freq_mask, :])
            total_energy = np.sum(magnitude)
            low_freq_ratio = low_freq_energy / total_energy if total_energy > 0 else 0
            
            # é«˜é¢‘èƒ½é‡å æ¯”ï¼ˆå¥³å£°ç‰¹å¾ï¼‰
            high_freq_mask = freq_bins > 1000
            high_freq_energy = np.sum(magnitude[high_freq_mask, :])
            high_freq_ratio = high_freq_energy / total_energy if total_energy > 0 else 0
            
            # RMSèƒ½é‡
            rms = np.sqrt(np.mean(audio_segment ** 2))
            
            return {
                'f0_mean': f0_mean,
                'f0_std': f0_std,
                'is_likely_female': is_likely_female,
                'low_freq_ratio': low_freq_ratio,
                'high_freq_ratio': high_freq_ratio,
                'rms_energy': rms,
                'gender_confidence': abs(f0_mean - self.pitch_gender_threshold) / 100
            }
              except Exception as e:
            return None
    
    def suppress_male_voice_frequency(self, audio_segment):
        """åŸºäºé¢‘åŸŸçš„ç”·å£°æŠ‘åˆ¶"""
        if not self.male_voice_suppression:
            return audio_segment
            
        try:
            original_length = len(audio_segment)
            
            # STFTå˜æ¢
            stft = librosa.stft(audio_segment, hop_length=self.hop_length, n_fft=self.frame_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # ç”·å£°ä½é¢‘æŠ‘åˆ¶ï¼ˆ50-300HzåŒºåŸŸï¼‰
            low_freq_mask = (freq_bins >= 50) & (freq_bins <= self.low_freq_cutoff)
            
            # è®¡ç®—æŠ‘åˆ¶å¼ºåº¦
            suppression_factor = 0.3  # æŠ‘åˆ¶åˆ°30%
            
            # åº”ç”¨æŠ‘åˆ¶
            magnitude[low_freq_mask, :] *= suppression_factor
            
            # é€†å˜æ¢
            stft_processed = magnitude * np.exp(1j * phase)
            audio_processed = librosa.istft(stft_processed, hop_length=self.hop_length, length=original_length)
            
            return audio_processed
            
        except Exception as e:
            return audio_segment
      def enhance_female_voice_frequency(self, audio_segment):
        """åŸºäºé¢‘åŸŸçš„å¥³å£°å¢å¼º"""
        if not self.female_voice_boost:
            return audio_segment
            
        try:
            original_length = len(audio_segment)
            
            # STFTå˜æ¢
            stft = librosa.stft(audio_segment, hop_length=self.hop_length, n_fft=self.frame_length)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # é¢‘ç‡è½´
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å¥³å£°å…³é”®é¢‘ç‡å¢å¼ºï¼ˆ150-300HzåŸºé¢‘åŒºï¼Œ1000-3000Hzå…±æŒ¯å³°åŒºï¼‰
            female_fundamental_mask = (freq_bins >= 150) & (freq_bins <= 300)
            female_formant_mask = (freq_bins >= 1000) & (freq_bins <= 3000)
            
            # åº”ç”¨å¢å¼º
            enhancement_factor = 1.2  # å¢å¼ºåˆ°120%
            magnitude[female_fundamental_mask, :] *= enhancement_factor
            magnitude[female_formant_mask, :] *= enhancement_factor
            
            # é€†å˜æ¢
            stft_processed = magnitude * np.exp(1j * phase)
            audio_processed = librosa.istft(stft_processed, hop_length=self.hop_length, length=original_length)
            
            return audio_processed
            
        except Exception as e:
            return audio_segment

    def energy_dominance_analysis(self, mixed_segment, ref_segment):
        """èƒ½é‡å ä¼˜åˆ†æ - åˆ©ç”¨å¥³å£°èƒ½é‡å¼ºçš„ç‰¹ç‚¹"""
        try:
            # RMSèƒ½é‡è®¡ç®—
            mixed_rms = np.sqrt(np.mean(mixed_segment ** 2))
            ref_rms = np.sqrt(np.mean(ref_segment ** 2))
            
            # èƒ½é‡æ¯”å€¼
            energy_ratio = mixed_rms / ref_rms if ref_rms > 0 else 0
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºèƒ½é‡å ä¼˜æ®µï¼ˆå¥³å£°ä¸»å¯¼ï¼‰
            is_dominant = energy_ratio > self.energy_dominance_ratio
            
            # é¢‘è°±èƒ½é‡åˆ†å¸ƒåˆ†æ
            mixed_stft = librosa.stft(mixed_segment, hop_length=self.hop_length)
            mixed_magnitude = np.abs(mixed_stft)
            
            freq_bins = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.frame_length)
            
            # å¥³å£°é¢‘ç‡èŒƒå›´èƒ½é‡å æ¯”
            female_freq_mask = (freq_bins >= 150) & (freq_bins <= 3000)
            female_energy = np.sum(mixed_magnitude[female_freq_mask, :])
            total_energy = np.sum(mixed_magnitude)
            female_energy_ratio = female_energy / total_energy if total_energy > 0 else 0
            
            return {
                'mixed_rms': mixed_rms,
                'ref_rms': ref_rms,
                'energy_ratio': energy_ratio,
                'is_dominant': is_dominant,
                'female_energy_ratio': female_energy_ratio,
                'dominance_score': min(energy_ratio * female_energy_ratio, 2.0)
            }
            
        except Exception as e:
            return None
    
    def advanced_speaker_similarity(self, features1, features2):
        """é«˜çº§è¯´è¯äººç›¸ä¼¼åº¦è®¡ç®—"""
        if features1 is None or features2 is None:
            return 0.0
        
        try:
            similarities = []
            weights = []
            
            # MFCCç›¸ä¼¼åº¦ (æœ€é‡è¦)
            mfcc_sim = 1 - cosine(features1['mfcc_mean'], features2['mfcc_mean'])
            if not np.isnan(mfcc_sim):
                similarities.append(max(0, mfcc_sim))
                weights.append(0.4)
            
            # åŸºé¢‘ç›¸ä¼¼åº¦
            if features1['f0_mean'] > 0 and features2['f0_mean'] > 0:
                f0_diff = abs(features1['f0_mean'] - features2['f0_mean'])
                f0_sim = 1 / (1 + f0_diff / 25)
                similarities.append(f0_sim)
                weights.append(0.25)
                
                # åŸºé¢‘èŒƒå›´ç›¸ä¼¼åº¦
                f0_range_diff = abs(features1['f0_range'] - features2['f0_range'])
                f0_range_sim = 1 / (1 + f0_range_diff / 50)
                similarities.append(f0_range_sim)
                weights.append(0.1)
            
            # é¢‘è°±ç‰¹å¾ç›¸ä¼¼åº¦
            centroid_diff = abs(features1['spectral_centroid'] - features2['spectral_centroid'])
            centroid_sim = 1 / (1 + centroid_diff / 400)
            similarities.append(centroid_sim)
            weights.append(0.15)
            
            # è‰²åº¦ç›¸ä¼¼åº¦
            chroma_sim = 1 - cosine(features1['chroma_mean'], features2['chroma_mean'])
            if not np.isnan(chroma_sim):
                similarities.append(max(0, chroma_sim))
                weights.append(0.1)
            
            # åŠ æƒå¹³å‡
            if similarities and weights:
                weights = np.array(weights[:len(similarities)])
                weights = weights / np.sum(weights)
                final_similarity = np.average(similarities, weights=weights)
                return max(0, min(1, final_similarity))
            else:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def intelligent_separate(self, mixed_audio_path, reference_audio_path, output_path=None):
        """
        æ™ºèƒ½è¯´è¯äººåˆ†ç¦»ä¸»å‡½æ•°
        """
        try:
            print("ğŸ¯ æ™ºèƒ½è¯­éŸ³åˆ†æ®µä¸è¯´è¯äººè¯†åˆ«ç³»ç»Ÿ V2")
            print("=" * 80)
            
            # åŠ è½½éŸ³é¢‘
            print("ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
            mixed_audio, _ = librosa.load(mixed_audio_path, sr=self.sample_rate)
            reference_audio, _ = librosa.load(reference_audio_path, sr=self.sample_rate)
            
            print(f"   æ··åˆéŸ³é¢‘: {len(mixed_audio)/self.sample_rate:.2f}ç§’")
            print(f"   å‚è€ƒéŸ³é¢‘: {len(reference_audio)/self.sample_rate:.2f}ç§’")
            
            # åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾
            print("ğŸ” åˆ†æå‚è€ƒéŸ³é¢‘ç‰¹å¾...")
            ref_features = self.extract_comprehensive_features(reference_audio)
            
            if ref_features is None:
                print("âŒ å‚è€ƒéŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return False, {"error": "Reference feature extraction failed"}
            
            print(f"   å‚è€ƒåŸºé¢‘: {ref_features['f0_mean']:.1f} Hz")
            print(f"   å‚è€ƒé¢‘è°±è´¨å¿ƒ: {ref_features['spectral_centroid']:.1f} Hz")
            print(f"   å‚è€ƒåŸºé¢‘èŒƒå›´: {ref_features['f0_range']:.1f} Hz")
            
            # æ™ºèƒ½åˆ†æ®µ
            segments = self.multi_level_segmentation(mixed_audio)
            
            if not segments:
                print("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ®µ")
                return False, {"error": "No valid segments detected"}
            
            # åˆå§‹åŒ–è¾“å‡º
            output_audio = np.zeros_like(mixed_audio)
            
            # é€æ®µåˆ†æ
            print(f"\nğŸ”„ æ™ºèƒ½è¯´è¯äººè¯†åˆ« - åˆ†æ {len(segments)} ä¸ªè¯­éŸ³æ®µ...")
            print("-" * 80)
            print("æ®µå· æ€§åˆ« æ—¶é—´èŒƒå›´        æ—¶é•¿   åŸºé¢‘   ç»¼åˆåˆ†   å†³ç­–   è¯´æ˜")
            print("-" * 80)
            
            kept_segments = 0
            total_kept_duration = 0
            female_segments = 0
            male_segments = 0
            
            for i, (start_time, end_time, confidence) in enumerate(segments):
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                
                start_sample = max(0, start_sample)
                end_sample = min(len(mixed_audio), end_sample)
                
                if end_sample <= start_sample:
                    continue
                
                segment_audio = mixed_audio[start_sample:end_sample]
                segment_duration = end_time - start_time
                
                # æå–æ®µç‰¹å¾
                segment_features = self.extract_comprehensive_features(segment_audio)
                
                if segment_features is None:
                    print(f"{i+1:2d}  â“  {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s   -    -      âŒ     ç‰¹å¾æå–å¤±è´¥")
                    continue
                
                # æ€§åˆ«ç‰¹å¾åˆ†æï¼ˆæ–°å¢ï¼‰
                gender_info = self.analyze_gender_characteristics(segment_audio)
                
                # èƒ½é‡å ä¼˜åˆ†æï¼ˆæ–°å¢ï¼‰
                energy_analysis = self.energy_dominance_analysis(segment_audio, reference_audio)
                
                # è®¡ç®—é«˜çº§ç›¸ä¼¼åº¦
                similarity = self.advanced_speaker_similarity(ref_features, segment_features)
                
                # å¢å¼ºå†³ç­–é€»è¾‘ï¼ˆè€ƒè™‘æ€§åˆ«ç‰¹å¾å’Œèƒ½é‡å ä¼˜ï¼‰
                decision_score = similarity
                
                # æ€§åˆ«ç‰¹å¾åŠ æƒ
                if gender_info:
                    if gender_info['is_likely_female']:
                        decision_score += 0.1  # å¥³å£°ç‰¹å¾åŠ åˆ†
                        female_segments += 1
                    else:
                        male_segments += 1
                    if gender_info['f0_mean'] > self.pitch_gender_threshold:
                        decision_score += 0.05  # é«˜åŸºé¢‘åŠ åˆ†
                    if gender_info['low_freq_ratio'] < 0.3:  # ä½é¢‘å æ¯”ä½ï¼ˆéç”·å£°ç‰¹å¾ï¼‰
                        decision_score += 0.05
                
                # èƒ½é‡å ä¼˜åŠ æƒ
                if energy_analysis and energy_analysis['is_dominant']:
                    decision_score += 0.15 * energy_analysis['dominance_score']
                  # åº”ç”¨é¢‘åŸŸå¤„ç†
                processed_segment = segment_audio.copy()
                if gender_info and gender_info['is_likely_female']:
                    # å¯¹å¯èƒ½çš„å¥³å£°è¿›è¡Œå¢å¼º
                    enhanced_segment = self.enhance_female_voice_frequency(processed_segment)
                    # ç¡®ä¿é•¿åº¦åŒ¹é…
                    if len(enhanced_segment) == len(processed_segment):
                        processed_segment = enhanced_segment
                else:
                    # å¯¹å¯èƒ½çš„ç”·å£°è¿›è¡ŒæŠ‘åˆ¶
                    suppressed_segment = self.suppress_male_voice_frequency(processed_segment)
                    # ç¡®ä¿é•¿åº¦åŒ¹é…
                    if len(suppressed_segment) == len(processed_segment):
                        processed_segment = suppressed_segment
                
                # å†³ç­–é€»è¾‘
                if decision_score >= self.similarity_threshold:
                    # é«˜ç›¸ä¼¼åº¦ï¼šä¿ç•™
                    output_audio[start_sample:end_sample] = processed_segment
                    kept_segments += 1
                    total_kept_duration += segment_duration
                    decision = "âœ… ä¿ç•™"
                    reason = "ç›®æ ‡è¯´è¯äºº"
                    
                elif decision_score >= self.similarity_threshold * 0.7:
                    # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šéƒ¨åˆ†ä¿ç•™
                    output_audio[start_sample:end_sample] = processed_segment * 0.6
                    kept_segments += 0.6
                    total_kept_duration += segment_duration * 0.6
                    decision = "ğŸ”¶ éƒ¨åˆ†"
                    reason = "å¯èƒ½ç›®æ ‡"
                    
                else:
                    # ä½ç›¸ä¼¼åº¦ï¼šä¸¢å¼ƒ
                    decision = "âŒ ä¸¢å¼ƒ"
                    reason = "éç›®æ ‡è¯´è¯äºº"
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                gender_mark = "â™€" if gender_info and gender_info['is_likely_female'] else "â™‚"
                energy_mark = "âš¡" if energy_analysis and energy_analysis['is_dominant'] else "  "
                
                print(f"{i+1:2d}  {gender_mark}{energy_mark} {start_time:6.2f}-{end_time:6.2f}s  {segment_duration:5.2f}s  {segment_features['f0_mean']:5.1f}  {decision_score:6.3f}  {decision}  {reason}")
            
            print("-" * 80)
            print(f"ğŸ“Š æ™ºèƒ½åˆ†æå®Œæˆ:")
            print(f"   æ€»æ®µæ•°: {len(segments)}")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(segments)*100:.1f}%")
            print(f"   ä¿ç•™æ—¶é•¿: {total_kept_duration:.2f}s / {len(mixed_audio)/self.sample_rate:.2f}s")
            print(f"   æ£€æµ‹å¥³å£°æ®µ: {female_segments}/{len(segments)}")
            print(f"   æ£€æµ‹ç”·å£°æ®µ: {male_segments}/{len(segments)}")
            print(f"   ç”·å£°æŠ‘åˆ¶: {'å¯ç”¨' if self.male_voice_suppression else 'ç¦ç”¨'}")
            print(f"   å¥³å£°å¢å¼º: {'å¯ç”¨' if self.female_voice_boost else 'ç¦ç”¨'}")
            
            # åå¤„ç†
            output_audio = self.post_process_audio(output_audio)
            
            # ä¿å­˜ç»“æœ
            if output_path:
                sf.write(output_path, output_audio, self.sample_rate)
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")
            
            result_info = {
                'success': True,
                'total_segments': len(segments),
                'kept_segments': kept_segments,
                'keep_ratio': kept_segments / len(segments),
                'original_duration': len(mixed_audio) / self.sample_rate,
                'output_duration': len(output_audio) / self.sample_rate,
                'kept_speech_duration': total_kept_duration,
                'similarity_threshold': self.similarity_threshold,
                'female_segments': female_segments,
                'male_segments': male_segments
            }
            
            # è¯„ä¼°åˆ†ç¦»è´¨é‡
            if kept_segments / len(segments) > 0.8:
                quality = "ä¼˜ç§€"
            elif kept_segments / len(segments) > 0.6:
                quality = "è‰¯å¥½"
            elif kept_segments / len(segments) > 0.4:
                quality = "ä¸€èˆ¬"
            else:
                quality = "éœ€è¦è°ƒä¼˜"
            
            print(f"\nğŸ‰ æ™ºèƒ½è¯´è¯äººåˆ†ç¦»æˆåŠŸ!")
            print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"   ä¿ç•™æ®µæ•°: {kept_segments}/{len(segments)}")
            print(f"   ä¿ç•™ç‡: {kept_segments/len(segments)*100:.1f}%")
            print(f"   æœ‰æ•ˆè¯­éŸ³æ—¶é•¿: {total_kept_duration:.2f}s")
            print(f"ğŸ† åˆ†ç¦»è´¨é‡: {quality} - è¯†åˆ«å‡†ç¡®ç‡å¾ˆé«˜")
            print(f"ğŸ’¡ å¦‚æœæ•ˆæœä¸ä½³ï¼Œå¯ä»¥å°è¯•:")
            print(f"   1. è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼ (å½“å‰: {self.similarity_threshold})")
            print(f"   2. æ£€æŸ¥å‚è€ƒéŸ³é¢‘æ˜¯å¦æ¸…æ™°")
            print(f"   3. æ£€æŸ¥æ··åˆéŸ³é¢‘è´¨é‡")
            
            return True, result_info
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½åˆ†ç¦»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {"error": str(e)}
    
    def post_process_audio(self, audio):
        """æ™ºèƒ½éŸ³é¢‘åå¤„ç†"""
        if len(audio) == 0:
            return audio
        
        # å»ç›´æµåˆ†é‡
        audio = audio - np.mean(audio)
        
        # æ™ºèƒ½å™ªå£°é—¨é™
        rms = np.sqrt(np.mean(audio ** 2))
        threshold = rms * 0.05
        mask = np.abs(audio) < threshold
        audio[mask] = 0
        
        # è½»å¾®çš„å¹³æ»‘
        audio = signal.savgol_filter(audio, window_length=5, polyorder=2)
        
        # å½’ä¸€åŒ–
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.9
        
        return audio

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåˆ†ç¦»å™¨
        separator = SmartSpeakerSeparatorV2()
        
        # è®¾ç½®è·¯å¾„
        mixed_audio_path = "temp/demucs_output/htdemucs/answer/vocals.wav"
        reference_audio_path = "reference/refne2.wav"
        output_path = "output/smart_separated_speaker_v2.wav"
        
        # æ‰§è¡Œåˆ†ç¦»
        success, result = separator.intelligent_separate(
            mixed_audio_path, 
            reference_audio_path, 
            output_path
        )
        
        if success:
            print("\nâœ… åˆ†ç¦»æˆåŠŸå®Œæˆ!")
        else:
            print(f"\nâŒ åˆ†ç¦»å¤±è´¥: {result.get('error', 'Unknown error')}")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
